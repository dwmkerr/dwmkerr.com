<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AWS on dwmkerr.com</title><link>https://dwmkerr.com/tags/aws/</link><description>Recent content in AWS on dwmkerr.com</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><copyright>Copright &amp;copy; Dave Kerr</copyright><lastBuildDate>Fri, 23 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://dwmkerr.com/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework</title><link>https://dwmkerr.com/building-least-privilege-permissions-aws/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://dwmkerr.com/building-least-privilege-permissions-aws/</guid><description>&lt;p>In this article I&amp;rsquo;m going to give a brief overview of some techniques to build &amp;rsquo;least privilege&amp;rsquo; roles in AWS. This assumes a basic knowledge of AWS and Identity and Access Management. It uses the (at time of writing) &lt;a href="https://aws.amazon.com/about-aws/whats-new/2021/04/iam-access-analyzer-easier-implement-least-privilege-permissions-generating-iam-policies-access-activity/">newly announced features in the AWS IAM Access Analyser&lt;/a>&lt;/p>
&lt;p>I&amp;rsquo;ll be demoing the techniques using a project built on &lt;a href="https://www.serverless.com/">The Serverless Framework&lt;/a> but you don&amp;rsquo;t need to know anything about how this framework works to follow the article - it is just used to demonstrate the concepts. You should be able to apply these techniques to almost any process which accesses or manages AWS resources.&lt;/p>
&lt;p>Let&amp;rsquo;s get into it!&lt;/p>
&lt;hr>
&lt;h2 id="updates-to-the-aws-iam-access-analyser">Updates to the AWS IAM Access Analyser&lt;/h2>
&lt;p>AWS recently announced some new features to the IAM Access Analyser, which are designed to help build &amp;rsquo;least privilege&amp;rsquo; policies for your AWS solutions. As I have been deploying a number of solutions based on &lt;a href="https://www.serverless.com/">The Serverless Application Framework&lt;/a> I thought this would be a great time to try out these new features.&lt;/p>
&lt;p>The Serverless Application Framework is a useful framework if you want to rapidly create serverless applications. You can rapidly create lambda functions, deploy to AWS, test locally, debug and so on.&lt;/p>
&lt;h2 id="our-use-case---a-serverless-framework-deployment-process">Our Use Case - A Serverless Framework Deployment Process&lt;/h2>
&lt;p>When you use the Serverless Framework, a common pattern for deployment is to let the framework itself deploy resources.&lt;/p>
&lt;p>A deploy command would look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>serverless deploy --stack uat
&lt;/code>&lt;/pre>&lt;p>Under the hood, this will do a few things:&lt;/p>
&lt;ol>
&lt;li>Create a CloudFormation template which defines an application stack&lt;/li>
&lt;li>Upload the template to an S3 bucket on a specified AWS account&lt;/li>
&lt;li>Deploy the stack&lt;/li>
&lt;/ol>
&lt;p>Now what is deployed is very dependent on what you decide to use in your application, but common resources would be things like:&lt;/p>
&lt;ul>
&lt;li>CloudFormation Stacks&lt;/li>
&lt;li>Lambda Functions&lt;/li>
&lt;li>API Gateway resources&lt;/li>
&lt;li>DynamoDB tables&lt;/li>
&lt;li>SQS Queues&lt;/li>
&lt;li>&amp;hellip;and many more!&lt;/li>
&lt;/ul>
&lt;p>This raises some interesting questions - how should we secure this provisioning process?&lt;/p>
&lt;h2 id="fundamental-principles---isolation-and-least-privilege">Fundamental Principles - Isolation and Least-Privilege&lt;/h2>
&lt;p>There are two fundamental principles&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> which make sense to consider when thinking about how to integrate the Serverless Framework into your stack:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Isolation of Resources&lt;/strong>: Can we make sure that the stack resources are logically isolated from &lt;em>other&lt;/em> resources we are managing? This can reduce the impact of incidents - if the stack is compromised, it should only compromise specific resources, not all resources in your estate&lt;/li>
&lt;li>&lt;strong>Least Privilege&lt;/strong>: Can we make sure that when the &lt;code>serverless&lt;/code> binary deploys resources, it has the least permissions required to do its work, again reducing the impact of a potential incident&lt;/li>
&lt;/ol>
&lt;p>Isolation of resources can be handled in a number of ways - my preferred approach is to create separate AWS accounts for each application (and in fact, each environment, such as &amp;lsquo;dev&amp;rsquo;, &amp;rsquo;test&amp;rsquo; and so on). This is not something I will discuss in this article. What I would like to focus on is the second point - least privilege.&lt;/p>
&lt;h2 id="least-privilege-in-aws">Least Privilege in AWS&lt;/h2>
&lt;p>We don&amp;rsquo;t quite know what the Serverless Framework does when it provisions resources. I don&amp;rsquo;t mean this in a bad way. We &lt;em>could&lt;/em> investigate and read in detail exactly what happens, but part of the benefit of the framework is that it takes care of this for you&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. In the early stages of a project this is probably a great time saver - in the later stages it represents a potential vulnerability.&lt;/p>
&lt;p>We understand that it creates a CloudFormation stack, but some of the details are not necessarily readily discoverable from the documentation.&lt;/p>
&lt;p>If we wanted to create a policy which represents the permissions which the Serverless we could try a few approaches:&lt;/p>
&lt;ul>
&lt;li>Give the process full access to an environment, with wide permissions to create any resources&lt;/li>
&lt;li>Give the process limited access to an environment, run the provisioning process, see the permissions issues which arise, then iteratively add more permissions&lt;/li>
&lt;/ul>
&lt;p>You might think that the first instinct of a security team might be that a &amp;lsquo;full access&amp;rsquo; approach is fundamentally wrong. But great security experts balance risk, agility, cost all the time. They don&amp;rsquo;t want to stop experimentation - just make sure that it is done in a safe way.&lt;/p>
&lt;p>The first approach is perfectly fine in low sensitivity environments. If you want to move fast and try out the technology, you could lose valuable time trying to get the permissions just right. If you have a solid approach to &lt;em>isolation&lt;/em>, then you should be able to run your proof of concepts in an isolated sandbox environment, which has no access to sensitive resources.&lt;/p>
&lt;p>You can also add Billing Alerts, or even automate the &lt;em>destruction&lt;/em> of resources at a certain point, so that you automatically &amp;lsquo;clean up&amp;rsquo;. This is great security practice - provide teams with a way to experiment &lt;em>safely&lt;/em>. Give teams full access to their own self-serve sandbox environments, with automated guardrails to mitigate the risk of incidents&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="beyond-the-sandbox">Beyond the Sandbox&lt;/h2>
&lt;p>The &amp;lsquo;sandbox environment&amp;rsquo; approach &lt;em>can&lt;/em> be valid. But there will likely come a stage when this is no longer appropriate.&lt;/p>
&lt;p>When you want to deploy into an environment which has other resources, sensitive data, is accessible to the public, runs production workloads and so on. There will likely come a point where you cannot fully isolate your application - at this stage we should be looking at improving our security.&lt;/p>
&lt;p>Specifically, at this point we really should try to make sure that we limit the permissions of the process which runs the &lt;code>serverless deploy&lt;/code> command.&lt;/p>
&lt;p>Limiting the permissions has the benefit of reducing the &amp;lsquo;blast radius&amp;rsquo; of an attack. If the process is compromised it can do fewer things. It also has the benefit of &lt;em>increasing transparency&lt;/em> - we will explicitly document &lt;em>what we expect the process to do&lt;/em>. This is highly useful when performing security checks.&lt;/p>
&lt;h2 id="the-challenges-of-limiting-permissions">The Challenges of Limiting Permissions&lt;/h2>
&lt;p>The process of working out the specific permissions required for a process can be challenging. It might involve looking through lots of documentation, trying to build a policy, seeing if the process works, adding permissions, changing permissions and so on.&lt;/p>
&lt;p>Just this month AWS released some updates to the IAM Access Manager, adding some features to help build &amp;rsquo;least permission&amp;rsquo; policies:&lt;/p>
&lt;p>&lt;a href="https://aws.amazon.com/about-aws/whats-new/2021/04/iam-access-analyzer-easier-implement-least-privilege-permissions-generating-iam-policies-access-activity/">https://aws.amazon.com/about-aws/whats-new/2021/04/iam-access-analyzer-easier-implement-least-privilege-permissions-generating-iam-policies-access-activity/&lt;/a>&lt;/p>
&lt;p>These features immediately caught my eye as I&amp;rsquo;m very interesting in security practices. We&amp;rsquo;re going to take a look at these features in detail for the rest of the article and see how we can use them to improve the security of a process like our &amp;lsquo;serverless deployment&amp;rsquo;.&lt;/p>
&lt;h2 id="using-the-iam-access-analyser-to-build-fine-grained-policies">Using the IAM Access Analyser to Build Fine-Grained Policies&lt;/h2>
&lt;p>The process for generating fine grained policies with the IAM Access Analyser is quite simple:&lt;/p>
&lt;ol>
&lt;li>Ensure you are using CloudTrail to track access to resources&lt;/li>
&lt;li>Create an Access Analyser&lt;/li>
&lt;li>Run the process you want to create fine-grained permissions for, initially with wide permissions&lt;/li>
&lt;li>Use the Access Analyser to generate a policy based on the events in CloudTrail&lt;/li>
&lt;li>Refine the policy&lt;/li>
&lt;li>Document, document, document&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;m going to demonstrate this end-to-end with a &amp;lsquo;serverless framework deployment&amp;rsquo; process. Please keep an eye out for an article I&amp;rsquo;ll be writing soon on how to build a REST application with the Serverless Framework - for now don&amp;rsquo;t worry about the application itself too much, this could be any kind of deployment or operational process we&amp;rsquo;re running.&lt;/p>
&lt;h3 id="step-1-enable-cloudtrail">Step 1: Enable CloudTrail&lt;/h3>
&lt;p>We need to enable CloudTrail so that we have a log of API calls which Access Advisor can analyse.&lt;/p>
&lt;p>&lt;strong>Warning&lt;/strong>: if you trying these features out please be aware that they may fall outside of the &lt;a href="https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&amp;amp;all-free-tier.sort-order=asc&amp;amp;awsf.Free%20Tier%20Types=*all&amp;amp;awsf.Free%20Tier%20Categories=*all">AWS Free Tier&lt;/a> and so may incur a cost. Please be aware of this if you are testing these features.&lt;/p>
&lt;p>Open the CloudTrail portal and ensure that you have a trail setup which logs API calls. Once this is setup you should see something like this:&lt;/p>
&lt;img src="./images/cloudtrail.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/cloudtrail.png" class="img-zoomable">
&lt;p>You can use the code below as an example of how to create a trail:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>aws cloudtrail create-trail &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name management-events &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --s3-bucket-name cloudtrail-account123-management-events-bucket &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --is-multi-region-trail
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that if you don&amp;rsquo;t use the &lt;code>--is-multi-region-trail&lt;/code> flag then the trail is created for the current region only.&lt;/p>
&lt;p>You can read more about this command on the &lt;a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail-by-using-the-aws-cli-create-trail.html">Using create-trail&lt;/a> documentation page.&lt;/p>
&lt;h3 id="step-2-create-the-access-analyser">Step 2: Create the Access Analyser&lt;/h3>
&lt;p>You should be able to find the Access Analyser tool in the IAM page:&lt;/p>
&lt;img src="./images/access-analyser.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/access-analyser.png" class="img-zoomable">
&lt;p>You can now choose to create an analyser. Some things to note:&lt;/p>
&lt;ul>
&lt;li>Analysers are region specific - if you have many regions, you need many analysers&lt;/li>
&lt;li>You can provide a name, not surprising, but it might be useful to be highly descriptive here&lt;/li>
&lt;li>You have an option to specify the &amp;lsquo;zone of trust&amp;rsquo; - this might be an account or an entire organisation&lt;/li>
&lt;/ul>
&lt;p>Choose &amp;lsquo;Create Analyser&amp;rsquo; to create the access analyser.&lt;/p>
&lt;p>You&amp;rsquo;ll shortly see the created access analyser and likely some &amp;lsquo;findings&amp;rsquo; as well:&lt;/p>
&lt;img src="./images/access-analyser-created.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/access-analyser-created.png" class="img-zoomable">
&lt;p>&amp;lsquo;Findings&amp;rsquo; are the descriptions of the policies that grant access to a resource to a principal which is &lt;em>outside of your zone of trust&lt;/em>. This is a quite complex topic, there are more details on the &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-findings.html">Access Analyser findings&lt;/a> documentation. But we are essentially seeing that my policies are set up in a which is exposing my resources to principals outside of the defined zone of trust - these findings are entirely correct, as I have set up &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html">Cross Account Access&lt;/a> in this environment.&lt;/p>
&lt;p>Feel free to read more about findings - these findings can potentially be very useful for a security team to be aware. For now we&amp;rsquo;ll leave these findings alone and move onto the next step in creating fine grained policies, which is to run the process we want to secure.&lt;/p>
&lt;h3 id="step-3-run-your-process">Step 3: Run your process&lt;/h3>
&lt;p>Now you should run the process you want to create fine grained permissions for. To save rework, try and make sure you run &lt;em>all&lt;/em> part of the process which will be needed. For example, my Serverless Framework policy should cover creation of the stack, updating of the stack as well as deleting.&lt;/p>
&lt;p>To exercise this, I just need to run the following commands from my local project:&lt;/p>
&lt;pre tabindex="0">&lt;code># This command deploys a new stack...
serverless --stage dev deploy
# This command makes a change to a file, allowing us to update the stack...
echo &amp;#34;// testing a change to the stack...&amp;#34; &amp;gt;&amp;gt;&amp;gt; lambda_functions/my_function.js
serverless --stage dev deploy # this will update the stack
# This command deletes the stack, we then reset the changes to the file.
serverless --stage dev remove # this will destroy the stack
git checkout lambda_functions/my_function.js
&lt;/code>&lt;/pre>&lt;p>At this point you can run any process your want to secure. As CloudTrail is enable, API calls will be recorded.&lt;/p>
&lt;h3 id="step-4-create-a-policy-based-on-access-advisor">Step 4: Create a Policy based on Access Advisor&lt;/h3>
&lt;p>Now we get to the interesting part. Open the Roles view in AWS, select the role which is used when running your process and choose &amp;lsquo;Generate Policy&amp;rsquo;. Here&amp;rsquo;s how this will look in the portal:&lt;/p>
&lt;img src="./images/generate-policy.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/generate-policy.png" class="img-zoomable">
&lt;p>Note that this screenshot shows &lt;em>exactly why&lt;/em> fine grained permissions are so important. I have run the Serverless Framework deployment from my local machine using the &amp;lsquo;Cross Account Administrator&amp;rsquo; role. This is an extremely high-privilege role which is used when I administer AWS accounts which are part of my organisation. This is &lt;em>not&lt;/em> an appropriate role for the Serverless Framework binary to assume outside of a sandbox or proof of concept context. However - remember at this stage we want to use a high-privilege role so that the process runs to completion, so that we can see the permissions needed and then create a more refined role.&lt;/p>
&lt;p>When you choose to generate a policy, you&amp;rsquo;ll have the option of specifying which trail to use and which region. You can also choose a date range for events to analyse. It will be a lot easier to build an appropriate policy if you make this window as short as possible - so try and run the entire process you want to create a policy for and then immediately generate the policy.&lt;/p>
&lt;p>It can take a few minutes to generate the policy. When it is complete, you&amp;rsquo;ll see the option to view the generated policy:&lt;/p>
&lt;img src="./images/view-generated-policy.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/view-generated-policy.png" class="img-zoomable">
&lt;p>Opening it up, you&amp;rsquo;ll see the services and actions used, as well as options to add more actions:&lt;/p>
&lt;img src="./images/generated-policy.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/generated-policy.png" class="img-zoomable">
&lt;p>When you move to the next screen you&amp;rsquo;ll get the option to customise the permissions:&lt;/p>
&lt;img src="./images/customise-permissions.png" alt="Building Least Privilege Policies with the AWS Policy Advisor - and a Demo with the Serverless Application Framework ./images/customise-permissions.png" class="img-zoomable">
&lt;p>At this stage I would suggest copying the policy, saving it to a local file and then moving onto the next step - refining the policy.&lt;/p>
&lt;h3 id="step-5-refine-the-policy">Step 5: Refine the Policy&lt;/h3>
&lt;p>The generated policy will likely not be suitable for use yet. It might to too specific - limiting access to the specific resources which were used. You will also have services and actions listed for &lt;em>any&lt;/em> calls which have been made using the role - which might also include calls used for &lt;em>other&lt;/em> processes than just the one you tested earlier. For example, my policy has some permissions to list analyzers. That is &lt;em>not&lt;/em> needed by serverless - that has been included because I was using the same cross-account administrator role to create the analyzer earlier on, and this has been picked up.&lt;/p>
&lt;p>This also highlights the importance of using separate roles for separate processes - if you use a small number of roles to do a lot of different things, it can be very hard to understand why certain actions were taken. Again, for a quick test it might be OK to use the same role that you access the portal with, but it is good to get into the habit of quickly creating roles for specific purposes.&lt;/p>
&lt;p>This is where I would suggest going through the policy in detail and using it as a template for the &amp;lsquo;final&amp;rsquo; policy. This is what we&amp;rsquo;ll discus in the final step.&lt;/p>
&lt;h3 id="step-6-document-document-document">Step 6: Document, Document, Document&lt;/h3>
&lt;p>The final policy we create should be clearly documented. It is really important to explain &lt;em>why&lt;/em> certain permissions are needed. If people cannot reason about why a policy is set up in a specific way, then it will be very hard for them to maintain it over time or decide whether the permissions are appropriate or not.&lt;/p>
&lt;p>Even more so than with &amp;rsquo;normal&amp;rsquo; code, code which relates to security has to be comprehensible by others (or yourself when you come back to it). If you cannot understand why a policy grants a certain permission, then when you review the policy you don&amp;rsquo;t know whether the remove the permissions or leave them in (this is an example of &lt;a href="https://github.com/dwmkerr/hacker-laws#chestertons-fence">Chesterton&amp;rsquo;s Fence&lt;/a>.&lt;/p>
&lt;p>Whether you document this policy by saving it in a file, adding comments and checking it into source control, or turning it into a re-usable Terraform module, or using Pulimi to define the policy, or some other solution, is not too important. What &lt;em>is&lt;/em> important is documenting the policy and making this documentation transparent to others - and ideally making sure that others can maintain it over time.&lt;/p>
&lt;p>As an example, this is how I might define the S3 permissions in a Terraform file:&lt;/p>
&lt;pre tabindex="0">&lt;code># This statement allows the creation and management of buckets, which are used
# by serverless for CloudFormation files. Because the bucket name is
# non-deterministic we have to allow the creation of _any_ bucket.
statement {
sid = &amp;#34;ServerlessFrameworkS3&amp;#34;
effect = &amp;#34;Allow&amp;#34;
actions = [
&amp;#34;s3:CreateBucket&amp;#34;,
&amp;#34;s3:DeleteBucket&amp;#34;,
&amp;#34;s3:DeleteBucketPolicy&amp;#34;,
&amp;#34;s3:GetBucketAcl&amp;#34;,
&amp;#34;s3:GetBucketPolicy&amp;#34;,
&amp;#34;s3:GetBucketPolicyStatus&amp;#34;,
&amp;#34;s3:GetBucketPublicAccessBlock&amp;#34;,
&amp;#34;s3:GetEncryptionConfiguration&amp;#34;,
&amp;#34;s3:GetObject&amp;#34;,
&amp;#34;s3:ListBucket&amp;#34;,
&amp;#34;s3:PutBucketPolicy&amp;#34;,
&amp;#34;s3:PutBucketPublicAccessBlock&amp;#34;,
&amp;#34;s3:PutBucketTagging&amp;#34;,
&amp;#34;s3:PutEncryptionConfiguration&amp;#34;,
&amp;#34;s3:PutObject&amp;#34;,
&amp;#34;s3:SetBucketEncryption&amp;#34;,
]
resources = [
&amp;#34;arn:aws:s3:::*&amp;#34;
]
}
&lt;/code>&lt;/pre>&lt;p>I&amp;rsquo;m being very explicit with my comments, giving the statement id a meaningful value and creating separate statements for &lt;em>each service&lt;/em>.&lt;/p>
&lt;p>How you structure your policies will depend on the tools you use and your own preferred processes but the principle will remain the same - document carefully!&lt;/p>
&lt;h2 id="thats-it">That&amp;rsquo;s It!&lt;/h2>
&lt;p>The IAM Access Advisor is a powerful feature and should be of interest to anyone managing sensitive environments in the cloud.&lt;/p>
&lt;p>It is not limited to creating fine grained permissions - it can also help identify &lt;em>external access&lt;/em> to resources. This means it can be used to identify when resources such as S3 buckets are accessed via processes such as Cross-Account access.&lt;/p>
&lt;p>There are a lot of exciting features here and it will be interesting to see how the Access Advisor works over time!&lt;/p>
&lt;p>As usual, please do share any comments, suggestions or observations!&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Of course this is only a tiny part of the world of security best practices. To learn more I highly recommend &lt;a href="https://github.com/veeral-patel/how-to-secure-anything">Veeral Patel&amp;rsquo;s amazing &amp;lsquo;How to secure anything&amp;rsquo; project&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>The exact permissions required are documented at &lt;a href="https://serverless-stack.com/chapters/customize-the-serverless-iam-policy.html">https://serverless-stack.com/chapters/customize-the-serverless-iam-policy.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Dynamic and Configurable Availability Zones in Terraform</title><link>https://dwmkerr.com/dynamic-and-configurable-availability-zones-in-terraform/</link><pubDate>Tue, 11 Dec 2018 21:24:34 +0000</pubDate><guid>https://dwmkerr.com/dynamic-and-configurable-availability-zones-in-terraform/</guid><description>&lt;p>When building Terraform modules, it is a common requirement to want to allow the client to be able to choose which region resources are created in, and which availability zones are used.&lt;/p>
&lt;p>I&amp;rsquo;ve seen a few ways of doing this, none of which felt entirely satisfactory. After a bit of experimentation I&amp;rsquo;ve come up with a solution which I think really works nicely. This solution avoids having to know in advance how many availability zones we&amp;rsquo;ll support.&lt;/p>
&lt;p>&lt;img src="images/screenshot-1.jpg" alt="screenshot">&lt;/p>
&lt;p>To demonstrate, I&amp;rsquo;ve set up a module which deploys a cluster of web servers. My goal is to be able to configure the region, VPC CIDR block, subnets and subnet CIDR blocks as below:&lt;/p>
&lt;pre tabindex="0">&lt;code>module &amp;#34;cluster&amp;#34; {
source = &amp;#34;github.com/dwmkerr/terraform-aws-vpc&amp;#34;
# Note how we can specify any number of availability zones here...
region = &amp;#34;ap-northeast-2&amp;#34;
vpc_cidr = &amp;#34;10.0.0.0/16&amp;#34;
subnets = {
ap-northeast-2a = &amp;#34;10.0.1.0/24&amp;#34;
ap-northeast-2b = &amp;#34;10.0.2.0/24&amp;#34;
ap-northeast-2c = &amp;#34;10.0.3.0/24&amp;#34;
}
# This just defines the number of web servers to deploy, and uses
# adds my public key so I can SSH into the servers...
web_server_count = &amp;#34;3&amp;#34;
public_key_path = &amp;#34;~/.ssh/id_rsa.pub&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>The example module is at &lt;a href="https://github.com/dwmkerr/terraform-aws-vpc">github.com/dwmkerr/terraform-aws-vpc&lt;/a>. Let&amp;rsquo;s take a look at some of the key elements.&lt;/p>
&lt;h2 id="the-variables">The Variables&lt;/h2>
&lt;p>We define the required variables very explicitly, with descriptions and a variable type to avoid confusion:&lt;/p>
&lt;pre tabindex="0">&lt;code>variable &amp;#34;region&amp;#34; {
description = &amp;#34;The region to deploy the VPC in, e.g: us-east-1.&amp;#34;
type = &amp;#34;string&amp;#34;
}
variable &amp;#34;vpc_cidr&amp;#34; {
description = &amp;#34;The CIDR block for the VPC, e.g: 10.0.0.0/16&amp;#34;
type = &amp;#34;string&amp;#34;
}
variable &amp;#34;subnets&amp;#34; {
description = &amp;#34;A map of availability zones to CIDR blocks, which will be set up as subnets.&amp;#34;
type = &amp;#34;map&amp;#34;
}
&lt;/code>&lt;/pre>&lt;h2 id="the-vpc">The VPC&lt;/h2>
&lt;p>Now that we have defined the variables, we can set up the VPC:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Define the VPC.
resource &amp;#34;aws_vpc&amp;#34; &amp;#34;cluster&amp;#34; {
cidr_block = &amp;#34;${var.vpc_cidr}&amp;#34;
enable_dns_hostnames = true
}
// An Internet Gateway for the VPC.
resource &amp;#34;aws_internet_gateway&amp;#34; &amp;#34;cluster_gateway&amp;#34; {
vpc_id = &amp;#34;${aws_vpc.cluster.id}&amp;#34;
}
// Create one public subnet per key in the subnet map.
resource &amp;#34;aws_subnet&amp;#34; &amp;#34;public-subnet&amp;#34; {
count = &amp;#34;${length(var.subnets)}&amp;#34;
vpc_id = &amp;#34;${aws_vpc.cluster.id}&amp;#34;
cidr_block = &amp;#34;${element(values(var.subnets), count.index)}&amp;#34;
map_public_ip_on_launch = true
depends_on = [&amp;#34;aws_internet_gateway.cluster_gateway&amp;#34;]
availability_zone = &amp;#34;${element(keys(var.subnets), count.index)}&amp;#34;
}
// Create a route table allowing all addresses access to the IGW.
resource &amp;#34;aws_route_table&amp;#34; &amp;#34;public&amp;#34; {
vpc_id = &amp;#34;${aws_vpc.cluster.id}&amp;#34;
route {
cidr_block = &amp;#34;0.0.0.0/0&amp;#34;
gateway_id = &amp;#34;${aws_internet_gateway.cluster_gateway.id}&amp;#34;
}
}
// Now associate the route table with the public subnet - giving
// all public subnet instances access to the internet.
resource &amp;#34;aws_route_table_association&amp;#34; &amp;#34;public-subnet&amp;#34; {
count = &amp;#34;${length(var.subnets)}&amp;#34;
subnet_id = &amp;#34;${element(aws_subnet.public-subnet.*.id, count.index)}&amp;#34;
route_table_id = &amp;#34;${aws_route_table.public.id}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>There are a few things of interest here. First, we can easily build a variable number of subnets by using the &lt;code>count&lt;/code> field on the &lt;code>aws_subnet&lt;/code> resource:&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_subnet&amp;#34; &amp;#34;public-subnet&amp;#34; {
count = &amp;#34;${length(var.subnets)}&amp;#34;
availability_zone = &amp;#34;${element(keys(var.subnets), count.index)}&amp;#34;
cidr_block = &amp;#34;${element(values(var.subnets), count.index)}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>By using the &lt;a href="https://www.terraform.io/docs/configuration/interpolation.html">Terraform Interpolation Syntax&lt;/a>, and in particular the &lt;code>count&lt;/code>, &lt;code>keys&lt;/code>, &lt;code>values&lt;/code> and &lt;code>element&lt;/code> functions, we can grab the subnet name and CIDR block from the variables.&lt;/p>
&lt;h2 id="the-web-server-cluster">The Web Server Cluster&lt;/h2>
&lt;p>A cluster of web servers behind a load balancer are created by the module, to demonstrate that it works. There is little of interest in the script except for how the subnets are referenced:&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_autoscaling_group&amp;#34; &amp;#34;cluster_node&amp;#34; {
name = &amp;#34;cluster_node&amp;#34;
vpc_zone_identifier = [&amp;#34;${aws_subnet.public-subnet.*.id}&amp;#34;]
launch_configuration = &amp;#34;${aws_launch_configuration.cluster_node.name}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>Note that we can specify the entire list of subnet ids by using the &lt;code>*&lt;/code> symbol in the resource path - &lt;code>[&amp;quot;${aws_subnet.public-subnet.*.id}&amp;quot;]&lt;/code>.&lt;/p>
&lt;h2 id="thats-it">That&amp;rsquo;s It!&lt;/h2>
&lt;p>That&amp;rsquo;s really all there is to it. I quite like this approach. I think it makes it very clear what is going on with the infrastructure, and is fairly manageable.&lt;/p>
&lt;p>One question which may be raised is why I am not using the &lt;a href="https://www.terraform.io/docs/configuration/interpolation.html#cidrsubnet-iprange-newbits-netnum-">&lt;code>cidrsubnet&lt;/code>&lt;/a> function to automatically calculate the CIDR blocks for the subnets. The reason is purely one of preference - I prefer to explicitly specify the CIDR blocks and use various patterns to set conventions. For example, if I see an IP address such as &lt;code>10.0.3.121&lt;/code> then it is in the third AZ of my public subnet, or &lt;code>10.2.2.11&lt;/code> is in the second AZ of my locked down data zone.&lt;/p>
&lt;p>You can see a sample Terraform module which uses this pattern at: &lt;a href="https://github.com/dwmkerr/terraform-aws-vpc-example">github.com/dwmkerr/terraform-aws-vpc-example&lt;/a>. This module also has a basic build pipeline and is published on the &lt;a href="https://registry.terraform.io/modules/dwmkerr/vpc-example">Terraform Registry&lt;/a>. I&amp;rsquo;ll also be updating my &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">AWS Openshift&lt;/a> module to use this pattern.&lt;/p></description><category>CodeProject</category></item><item><title>Integrating OpenShift and Splunk for Docker Container Logging</title><link>https://dwmkerr.com/integrating-openshift-and-splunk-for-logging/</link><pubDate>Sun, 29 Oct 2017 07:15:04 +0000</pubDate><guid>https://dwmkerr.com/integrating-openshift-and-splunk-for-logging/</guid><description>&lt;p>In this article I&amp;rsquo;m going to show you how to set up OpenShift to integrate with Splunk for logging in a Docker container orchestration environment.&lt;/p>
&lt;p>These techniques could easily be adapted for a standard Kubernetes installation as well!&lt;/p>
&lt;p>&lt;img src="images/counter-service-splunk.png" alt="Screenshot: Counter service splunk">&lt;/p>
&lt;p>The techniques used in this article are based on the &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging">Kubernetes Logging Cluster Administration Guide&lt;/a>. I also found Jason Poon&amp;rsquo;s article &lt;a href="http://jasonpoon.ca/2017/04/03/kubernetes-logging-with-splunk/">Kubernetes Logging with Splunk&lt;/a> very helpful.&lt;/p>
&lt;p>First, clone the &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">Terraform AWS OpenShift&lt;/a> repo:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone git@github.com:dwmkerr/terraform-aws-openshift
&lt;/code>&lt;/pre>&lt;p>This repo can be used to create a vanilla OpenShift cluster. I&amp;rsquo;m adding &amp;lsquo;recipes&amp;rsquo; to the project, which will allow you to mix in more features (but still keep the main codebase clean). For now, let&amp;rsquo;s merge in the &amp;lsquo;splunk&amp;rsquo; recipe:&lt;/p>
&lt;pre tabindex="0">&lt;code>cd terraform-aws-openshift
git pull origin recipes/splunk
&lt;/code>&lt;/pre>&lt;p>Pulling this recipe in adds the extra config and scripts required to set up Splunk&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Now we&amp;rsquo;ve got the code, we can get started!&lt;/p>
&lt;h2 id="create-the-infrastructure">Create the Infrastructure&lt;/h2>
&lt;p>To create the cluster, you&amp;rsquo;ll need to install the &lt;a href="https://aws.amazon.com/cli/">AWS CLI&lt;/a> and log in, and install &lt;a href="https://www.terraform.io/downloads.html">Terraform&lt;/a>.&lt;/p>
&lt;p>Before you continue, &lt;font color="red">&lt;strong>be aware&lt;/strong>&lt;/font>: the machines on AWS we&amp;rsquo;ll create are going to run to about $250 per month:&lt;/p>
&lt;p>&lt;img src="images/aws-cost.png" alt="AWS Cost Calculator">&lt;/p>
&lt;p>Once you are logged in with the AWS CLI just run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make infrastructure
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You&amp;rsquo;ll be asked to specify a region:&lt;/p>
&lt;p>&lt;img src="images/region.png" alt="Specify Region">&lt;/p>
&lt;p>Any &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">AWS region&lt;/a> will work fine, use &lt;code>us-east-1&lt;/code> if you are not sure.&lt;/p>
&lt;p>It&amp;rsquo;ll take about 5 minutes for Terraform to build the required infrastructure, which looks like this:&lt;/p>
&lt;p>&lt;img src="images/splunk-architecture.png" alt="AWS Infrastructure">&lt;/p>
&lt;p>Once it&amp;rsquo;s done you&amp;rsquo;ll see a message like this:&lt;/p>
&lt;p>&lt;img src="images/apply-complete.png" alt="Apply Complete">&lt;/p>
&lt;p>The infrastructure is ready! A few of the most useful parameters are shown as output variables. If you log into AWS you&amp;rsquo;ll see our new instances, as well as the VPC, network settings etc etc:&lt;/p>
&lt;p>&lt;img src="images/aws.png" alt="AWS">&lt;/p>
&lt;h2 id="installing-openshift">Installing OpenShift&lt;/h2>
&lt;p>Installing OpenShift is easy:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make openshift
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This command will take quite some time to run (sometimes up to 30 minutes). Once it is complete you&amp;rsquo;ll see a message like this:&lt;/p>
&lt;p>&lt;img src="images/openshift-complete.png" alt="OpenShift Installation Complete">&lt;/p>
&lt;p>You can now open the OpenShift console. Use the public address of the master node (which you can get with &lt;code>$(terraform output master-url)&lt;/code>), or just run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make browse-openshift
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The default username and password is &lt;code>admin&lt;/code> and &lt;code>123&lt;/code>. You&amp;rsquo;ll see we have a clean installation and are ready to create our first project:&lt;/p>
&lt;p>&lt;img src="images/welcome-to-openshift.png" alt="Welcome to OpenShift">&lt;/p>
&lt;p>Close the console for now.&lt;/p>
&lt;h2 id="installing-splunk">Installing Splunk&lt;/h2>
&lt;p>You&amp;rsquo;ve probably figured out the pattern by now&amp;hellip;&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make splunk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once this command is complete, you can open the Splunk console with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make browse-splunk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Again the username and password is &lt;code>admin&lt;/code> and &lt;code>123&lt;/code>. You can change the password on login, or leave it:&lt;/p>
&lt;p>&lt;img src="images/splunk-home.png" alt="Splunk Login">&lt;/p>
&lt;p>You can close the Splunk console now, we&amp;rsquo;ll come back to it shortly.&lt;/p>
&lt;h2 id="demoing-splunk-and-openshift">Demoing Splunk and OpenShift&lt;/h2>
&lt;p>To see Splunk and OpenShift in action, it helps to have some kind of processing going on in the cluster. You can create a very basic sample project which will spin up two nodes which just write a counter every second as a way to get something running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make sample
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will create a simple &amp;lsquo;counter&amp;rsquo; service:&lt;/p>
&lt;p>&lt;img src="images/counter-service.png" alt="Screenshot: The counter service">&lt;/p>
&lt;p>We can see the logs in OpenShift:&lt;/p>
&lt;p>&lt;img src="images/counter-service-logs.png" alt="Screenshot: The counter service logs">&lt;/p>
&lt;p>Almost immediately you&amp;rsquo;ll be able to see the data in Splunk:&lt;/p>
&lt;p>&lt;img src="images/counter-service-splunk-data-summary.png" alt="Screenshot: The Splunk data explorer">&lt;/p>
&lt;p>And because of the way the log files are named, we can even rip out the namespace, pod, container and id:&lt;/p>
&lt;p>&lt;img src="images/counter-service-splunk.png" alt="Screenshot: Counter service splunk">&lt;/p>
&lt;p>That&amp;rsquo;s it! You have OpenShift running, Splunk set up and automatically forwarding of all container logs. Enjoy!&lt;/p>
&lt;h2 id="how-it-works">How It Works&lt;/h2>
&lt;p>I&amp;rsquo;ve tried to keep the setup as simple as possible. Here&amp;rsquo;s how it works.&lt;/p>
&lt;h3 id="how-log-files-are-written">How Log Files Are Written&lt;/h3>
&lt;p>The Docker Engine has a &lt;a href="https://docs.docker.com/engine/admin/logging/overview/">log driver&lt;/a> which determines how container logs are handled&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. It defaults to the &lt;code>json-file&lt;/code> driver, which means that logs are written as a json file to:&lt;/p>
&lt;pre tabindex="0">&lt;code>/var/lib/docker/containers/{container-id}/{container-id}-json.log
&lt;/code>&lt;/pre>&lt;p>Or visually:&lt;/p>
&lt;p>&lt;img src="images/logging-docker-1.png" alt="Diagram: How Docker writes log files">&lt;/p>
&lt;p>Normally we wouldn&amp;rsquo;t touch this file, in theory it is supposed to be used internally&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> and we would use &lt;code>docker logs &amp;lt;container-id&amp;gt;&lt;/code>.&lt;/p>
&lt;p>In theory, all we need to do is use a &lt;a href="http://docs.splunk.com/Documentation/Forwarder/7.0.0/Forwarder/Abouttheuniversalforwarder">Splunk Forwarder&lt;/a> to send this file to our indexer. The only problem is that we only get the container ID from the file name, finding the right container ID for your container can be a pain. However, we are running on Kubernetes, which means the picture is a little different&amp;hellip;&lt;/p>
&lt;h3 id="how-log-files-are-written---on-kubernetes">How Log Files Are Written - on Kubernetes&lt;/h3>
&lt;p>When running on Kubernetes, things are little different. On machines with &lt;code>systemd&lt;/code>, the log driver for the docker engine is set to &lt;code>journald&lt;/code> (see &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Kubernetes - Logging Architecture&lt;/a>.&lt;/p>
&lt;p>It &lt;em>is&lt;/em> possible to forward &lt;code>journald&lt;/code> to Splunk, but only by streaming it to a file and then forwarding the file. Given that we need to use a file as an intermediate, it seems easier just to change the driver back to &lt;code>json-file&lt;/code> and forward that.&lt;/p>
&lt;p>So first, we configure the docker engine to use &lt;code>json-file&lt;/code> (see &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift/blob/recipes/splunk/scripts/postinstall-master.sh">this file&lt;/a>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sed -i &lt;span style="color:#e6db74">&amp;#39;/OPTIONS=.*/c\OPTIONS=&amp;#34;--selinux-enabled --insecure-registry 172.30.0.0/16 --log-driver=json-file --log-opt max-size=1M --log-opt max-file=3&amp;#34;&amp;#39;&lt;/span> /etc/sysconfig/docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here we just change the options to default to the &lt;code>json-file&lt;/code> driver, with a max file size of 1MB (and maximum of three files, so we don&amp;rsquo;t chew all the space on the host).&lt;/p>
&lt;p>Now the cool thing about Kubernetes is that it creates symlinks to the log files, which have much more descriptive names:&lt;/p>
&lt;p>&lt;img src="images/logging-k8s.png" alt="Symlink diagram">&lt;/p>
&lt;p>We still have the original container log, in the same location. But we also have a pod container log (which is a symlink to the container log) and another container log, which is a symlink to the pod container log.&lt;/p>
&lt;p>This means we can read the container log, and extract some really useful information from the file name. The container log file name has the following format:&lt;/p>
&lt;pre tabindex="0">&lt;code>/var/log/containers/{container-id}/{container-id}-json.log
&lt;/code>&lt;/pre>&lt;h3 id="how-log-files-are-read">How Log Files Are Read&lt;/h3>
&lt;p>Now that we are writing the log files to a well defined location, reading them is straightforward. The diagram below shows how we use a splunk-forwarder to complete the picture:&lt;/p>
&lt;p>&lt;img src="images/how-logs-are-read.png" alt="Diagram: How logs are read">&lt;/p>
&lt;p>First, we create a DaemonSet, which ensures we run a specific pod on every node.&lt;/p>
&lt;p>The DaemonSet runs with a new account which has the &amp;lsquo;any id&amp;rsquo; privilege, allowing it to run as root. We then mount the log folders into the container (which are owned by root, which is why our container needs these extra permissions to read the files).&lt;/p>
&lt;p>The pod contains a splunk-forwarder container, which is configured to monitor the &lt;code>/var/log/containers&lt;/code> folder. It also monitors the docker socket, allowing us to see docker events. The forwarder is also configured with the IP address of the Splunk Indexer.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>As a reference, you can also see the recipe pull request to see what changes from a &amp;lsquo;vanilla&amp;rsquo; installation to add Splunk: &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift/pull/16">Splunk Recipe Pull Request&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>It is useful to check the documentation on logging drivers for Docker. See &lt;a href="https://docs.docker.com/engine/admin/logging/overview/#supported-logging-drivers">Configure Logging Drivers&lt;/a> and &lt;a href="https://docs.docker.com/engine/extend/plugins_logging/">Docker Log Driver Plugins&lt;/a>. It is possible to create custom log drivers. However, at the time of writing only the journald and json-file log drivers will work with the integrated logging view in OpenShift.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Get up and running with OpenShift on AWS</title><link>https://dwmkerr.com/get-up-and-running-with-openshift-on-aws/</link><pubDate>Thu, 02 Feb 2017 07:47:00 +0000</pubDate><guid>https://dwmkerr.com/get-up-and-running-with-openshift-on-aws/</guid><description>&lt;p>&lt;a href="https://www.openshift.com/">OpenShift&lt;/a> is Red Hat&amp;rsquo;s platform-as-a-service offering for hosting and scaling applications. It&amp;rsquo;s built on top of Google&amp;rsquo;s popular &lt;a href="https://kubernetes.io/">Kubernetes&lt;/a> system.&lt;/p>
&lt;p>Getting up and running with OpenShift Online is straightforward, as it is a cloud hosted solution. Setting up your own cluster is a little more complex, but in this article I&amp;rsquo;ll show you how to make it fairly painless.&lt;/p>
&lt;p>&lt;img src="images/welcome.png" alt="OpenShift Login">&lt;/p>
&lt;p>The repo for this project is at: &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">github.com/dwmkerr/terraform-aws-openshift&lt;/a>.&lt;/p>
&lt;h2 id="creating-the-infrastructure">Creating the Infrastructure&lt;/h2>
&lt;p>OpenShift has some fairly specific requirements about what hardware it runs on&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. There&amp;rsquo;s also DNS to set up, as well as internet access and so on.&lt;/p>
&lt;p>All in all, for a bare-bones setup, you&amp;rsquo;ll need something like this:&lt;/p>
&lt;p>&lt;img src="images/network-diagram-2.png" alt="Network Diagram">&lt;/p>
&lt;p>Which is (deep breath):&lt;/p>
&lt;ol>
&lt;li>A network&lt;/li>
&lt;li>A public subnet, with internet access via a gateway&lt;/li>
&lt;li>A master host, which will run the OpenShift master&lt;/li>
&lt;li>A pair of node hosts, which will run additional OpenShift nodes&lt;/li>
&lt;li>A hosted zone, which allows us to configure DNS&lt;/li>
&lt;li>A bastion, which allows us to SSH onto hosts, without directly exposing them&lt;/li>
&lt;li>Some kind of basic log aggregation, which I&amp;rsquo;m using CloudWatch for&lt;/li>
&lt;/ol>
&lt;p>This is not a production grade setup, which requires redundant masters and so on, but it provides the basics.&lt;/p>
&lt;p>Rather than setting this infrastructure up by hand, this is all scripted with &lt;a href="https://www.terraform.io/">Terraform&lt;/a>. To set up the infrastructure, clone the &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">github.com/dwmkerr/terraform-aws-openshift&lt;/a> repo:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git clone git@github.com:dwmkerr/terraform-aws-openshift
...
Resolving deltas: 100% (37/37), done.
&lt;/code>&lt;/pre>&lt;p>Then use the terraform CLI&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> to create the infrastructure:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ cd terraform-aws-openshift/
$ terraform get &amp;amp;&amp;amp; terraform apply
&lt;/code>&lt;/pre>&lt;p>You&amp;rsquo;ll be asked for a region, to deploy the network into, here I&amp;rsquo;m using &lt;code>us-west-1&lt;/code>:&lt;/p>
&lt;p>&lt;img src="images/Screenshot-at-Feb-02-21-16-44.png" alt="Enter Region">&lt;/p>
&lt;p>After a few minutes the infrastructure will be set up:&lt;/p>
&lt;p>&lt;img src="images/output.png" alt="Terraform complete">&lt;/p>
&lt;p>A quick glance at the AWS console shows the new hosts we&amp;rsquo;ve set up:&lt;/p>
&lt;p>&lt;img src="images/aws.png" alt="AWS Console">&lt;/p>
&lt;p>The next step is to install OpenShift.&lt;/p>
&lt;h2 id="installing-openshift">Installing OpenShift&lt;/h2>
&lt;p>There are a few different ways to install OpenShift, but the one we&amp;rsquo;ll use is called the &amp;lsquo;advanced installation&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&amp;rsquo;. This essentially involves:&lt;/p>
&lt;ol>
&lt;li>Creating an &amp;lsquo;inventory&amp;rsquo;, which specifies the hosts OpenShift will be installed on and the installation options&lt;/li>
&lt;li>Downloading the advanced installation code&lt;/li>
&lt;li>Running the advanced installation Ansible Playbook&lt;/li>
&lt;/ol>
&lt;p>To create the inventory, we just run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sed &lt;span style="color:#e6db74">&amp;#34;s/\${aws_instance.master.public_ip}/&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>terraform output master-public_ip&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">/&amp;#34;&lt;/span> inventory.template.cfg &amp;gt; inventory.cfg
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This takes our &amp;lsquo;inventory template&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&amp;rsquo; and populates it with the public IP of our master node, which is recorded in a Terraform output variable.&lt;/p>
&lt;p>We can then copy the inventory to the bastion:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ssh-add ~/.ssh/id_rsa
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>scp ./inventory.cfg ec2-user@&lt;span style="color:#66d9ef">$(&lt;/span>terraform output bastion-public_dns&lt;span style="color:#66d9ef">)&lt;/span>:~
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can again use the Terraform output variables, this time to get the bastion IP. Finally, we pipe our install script to the bastion host:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat install-from-bastion.sh | ssh -A ec2-user@&lt;span style="color:#66d9ef">$(&lt;/span>terraform output bastion-public_dns&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There&amp;rsquo;s a &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift/issues/1">bug&lt;/a> which means you might see &lt;code>ansible-playbook: command not found&lt;/code>, if so, just run the script again. The install script clones the installation scripts and runs them, using the inventory we&amp;rsquo;ve provided:&lt;/p>
&lt;p>&lt;img src="images/ansible.png" alt="Ansible Output">&lt;/p>
&lt;p>This&amp;rsquo;ll probably take about 10 minutes to run. And that&amp;rsquo;s it, OpenShift is installed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>open &lt;span style="color:#e6db74">&amp;#34;https://&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>terraform output master-public_dns&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">:8443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hit &amp;lsquo;advanced&amp;rsquo; and continue, as we&amp;rsquo;re using a self-signed certificate most browsers will complain:&lt;/p>
&lt;p>&lt;img src="images/console1.png" alt="Invalid Certificate">&lt;/p>
&lt;p>Enter any username and password (the system is configured to allow anyone to access it by default) and you&amp;rsquo;ll be presented with the OpenShift console:&lt;/p>
&lt;p>&lt;img src="images/console2.png" alt="OpenShift console">&lt;/p>
&lt;p>As the setup requires three t2.large instances, which are not available on the free plan, you might want to clean up when you are done with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>terraform destroy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="wrapping-up">Wrapping Up&lt;/h2>
&lt;p>Hopefully you&amp;rsquo;ve found this useful, there are more details and references on the README of the github repo:&lt;/p>
&lt;p>&lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">https://github.com/dwmkerr/terraform-aws-openshift&lt;/a>&lt;/p>
&lt;p>Comments and feedback are always welcome!&lt;/p>
&lt;hr>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>See &lt;a href="https://docs.openshift.org/latest/install_config/install/prerequisites.html#system-requirements">https://docs.openshift.org/latest/install_config/install/prerequisites.html#system-requirements&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Use &amp;lsquo;brew install terraform&amp;rsquo;, full instructions in the &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">README.md&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>See &lt;a href="https://docs.openshift.org/latest/install_config/install/advanced_install.html">https://docs.openshift.org/latest/install_config/install/advanced_install.html&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>See &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift/blob/master/inventory.template.cfg">https://github.com/dwmkerr/terraform-aws-openshift/blob/master/inventory.template.cfg&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Creating a Resilient Consul Cluster for Docker Microservice Discovery with Terraform and AWS</title><link>https://dwmkerr.com/creating-a-resilient-consul-cluster-for-docker-microservice-discovery-with-terraform-and-aws/</link><pubDate>Mon, 09 Jan 2017 07:10:40 +0000</pubDate><guid>https://dwmkerr.com/creating-a-resilient-consul-cluster-for-docker-microservice-discovery-with-terraform-and-aws/</guid><description>&lt;p>In this article I&amp;rsquo;m going to show you how to create a resilient Consul cluster, using Terraform and AWS. We can use this cluster for microservice discovery and management. No prior knowledge of the technologies or patterns is required!&lt;/p>
&lt;p>The final code is at &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster">github.com/dwmkerr/terraform-consul-cluster&lt;/a>. Note that it has evolved somewhat since the time of writing, see the Appendices at the end of the article for details.&lt;/p>
&lt;h2 id="consul-terraform--aws">Consul, Terraform &amp;amp; AWS&lt;/h2>
&lt;p>&lt;a href="https://www.consul.io/">Consul&lt;/a> is a technology which enables &lt;em>Service Discovery&lt;/em>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, a pattern which allows services to locate each other via a central authority.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/">Terraform&lt;/a> is a technology which allows us to script the provisioning of infrastructure and systems. This allows us to practice the &lt;em>Infrastructure as Code&lt;/em> pattern. The rigour of code control (versioning, history, user access control, diffs, pull requests etc) can be applied to our systems.&lt;/p>
&lt;p>And why &lt;a href="https://aws.amazon.com/">AWS&lt;/a>? We need to create many servers and build a network to see this system in action. We can simulate parts of this locally with tools such as &lt;a href="https://www.vagrantup.com/">Vagrant&lt;/a>, but we can use the arguably most popular&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> IaaS platfom for this job at essentially zero cost, and learn some valuable skills which are readily applicable to other projects at the same time.&lt;/p>
&lt;p>A lot of what we will learn is not really AWS specific - and the Infrastructure as Code pattern which Terraform helps us apply allows us to apply these techniques easily with other providers.&lt;/p>
&lt;h2 id="the-goal">The Goal&lt;/h2>
&lt;p>The goal is to create a system like this:&lt;/p>
&lt;p>&lt;img src="images/img-0-goal.png" alt="Overall System Diagram">&lt;/p>
&lt;p>In a nutshell:&lt;/p>
&lt;ul>
&lt;li>We have a set of homogenous Consul nodes&lt;/li>
&lt;li>The nodes form a cluster and automatically elect a leader&lt;/li>
&lt;li>The nodes span more than one availability zone, meaning the system is redundant and can survive the failure of an entire availability zone (i.e. data centre)&lt;/li>
&lt;li>The Consul UI is available to view via a gateway&lt;/li>
&lt;li>We have two example microservices which register themselves on the cluster, so we can actually see some registered services in the console&lt;/li>
&lt;/ul>
&lt;p>As a quick caveat, in reality this setup would typically live in a private subnet, not directly accessible to the outside work except via public facing load balancers. This adds a bit more complexity to the Terraform setup but not much value to the walk-though. A network diagram of how it might look is below, I invite interested readers to try and move to this model as a great exercise to cement the concepts!&lt;/p>
&lt;h2 id="step-1---creating-our-network">Step 1 - Creating our Network&lt;/h2>
&lt;p>The first logical step is to create the network itself. This means:&lt;/p>
&lt;ul>
&lt;li>The network (in AWS terminology, this is a &lt;em>VPC&lt;/em> or &lt;em>Virtual Private Cloud&lt;/em>)&lt;/li>
&lt;li>The &amp;lsquo;public&amp;rsquo; subnet, which defines our IP ranges for hosts&lt;/li>
&lt;li>The internet gateway, which provides an entry/exit point for traffic from/to the internet&lt;/li>
&lt;li>The firewall rules, which define what traffic can come in and out of the network&lt;/li>
&lt;/ul>
&lt;p>All together, that&amp;rsquo;s this:&lt;/p>
&lt;p>&lt;img src="images/img-1-network.png" alt="">&lt;/p>
&lt;p>Our solution will be made more resilient by ensuring we host our Consul nodes across multiple &lt;em>availability zones&lt;/em>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;p>Creating a VPC and building a subnet is fairly trivial if you have done some network setup before or spent much time working with AWS, if not, you may be a little lost already. There&amp;rsquo;s a good course on Udemy&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> which will take you through the process of setting up a VPC which I recommend if you are interested in this, as it is quite hands on. It&amp;rsquo;ll also show you how to build a more &amp;lsquo;realistic&amp;rsquo; network, which also contains a private subnet and NAT, but that&amp;rsquo;s beyond the scope of this write-up. Instead, I&amp;rsquo;ll take you through the big parts.&lt;/p>
&lt;h3 id="the-network">The Network&lt;/h3>
&lt;p>We&amp;rsquo;re using AWS, we need to create a VPC. A VPC is a Virtual Private Cloud. The key thing is that it is &lt;em>isolated&lt;/em>. Things you create in this network will be able to talk to each other if you let them, but cannot communicate with the outside world, unless you specifically create the parts needed for them to do so.&lt;/p>
&lt;p>A private network is probably something you regularly use if you work in a company&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Most companies have their own internal network - when you use a computer on that network it can talk to other company computers (such as the company mail server). When you are off that network, you might not be able to access your company email (unless it is publicly available, like gmail, or over a VPN [and by accessing a VPN, you are actually &lt;em>joining&lt;/em> the network again, albeit remotely]).&lt;/p>
&lt;p>Perhaps the most immediately obvious part of a VPC is that &lt;em>you control the IP addresses&lt;/em>. You specify the &lt;em>range&lt;/em> of IP addresses which are available to give to machines on the network. When a machine joins, it is given an IP in that range. I&amp;rsquo;m not going to go into too much detail here, if you are interested let me know and I&amp;rsquo;ll write up an article on VPCs in detail!&lt;/p>
&lt;p>&lt;img src="images/img-3-vpc.png" alt="">&lt;/p>
&lt;p>Here&amp;rsquo;s how I&amp;rsquo;d suggest scripting AWS infrastructure with Terraform if you haven&amp;rsquo;t done this before.&lt;/p>
&lt;ol>
&lt;li>Use the AWS console to create what you want&lt;/li>
&lt;li>Search the Terraform documentation for the entity you want to create (e.g. &lt;a href="https://www.terraform.io/docs/providers/aws/r/vpc.html">VPC&lt;/a>), &lt;em>script&lt;/em> the component and &lt;em>apply&lt;/em> the provisioning&lt;/li>
&lt;li>Compare the hand-made VPC to the script-made VPC, if the two are the same, you are done&lt;/li>
&lt;li>If the two are different, check the documentation and try again&lt;/li>
&lt;/ol>
&lt;p>Ensure you have an AWS account, and note your Secret Key and Access Key. We&amp;rsquo;ll need these to remotely control it. Here&amp;rsquo;s the terraform script to create a VPC:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Setup the core provider information.
provider &amp;#34;aws&amp;#34; {
access_key = &amp;#34;${var.access_key}&amp;#34;
secret_key = &amp;#34;${var.secret_key}&amp;#34;
region = &amp;#34;${var.region}&amp;#34;
}
// Define the VPC.
resource &amp;#34;aws_vpc&amp;#34; &amp;#34;consul-cluster&amp;#34; {
cidr_block = &amp;#34;10.0.0.0/16&amp;#34; // i.e. 10.0.0.0 to 10.0.255.255
enable_dns_hostnames = true
tags {
Name = &amp;#34;Consul Cluster VPC&amp;#34;
Project = &amp;#34;consul-cluster&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>This script uses &lt;a href="https://www.terraform.io/docs/configuration/variables.html">Terraform Variables&lt;/a>, such as &lt;code>var.access_key&lt;/code>, which we keep in a &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/blob/master/variables.tf">variables.tf&lt;/a> file. Terraform will use the default values defined in the file if they are present, or ask the user to supply them. Let&amp;rsquo;s build the network:&lt;/p>
&lt;pre tabindex="0">&lt;code>terraform apply
&lt;/code>&lt;/pre>&lt;p>After supplying the values for the variables, Terraform will provision the network, using the AWS SDK internally.&lt;/p>
&lt;p>&lt;img src="images/img-2-terraform-apply.png" alt="">&lt;/p>
&lt;p>You&amp;rsquo;ll see lots of info about what it is creating, then a success message.&lt;/p>
&lt;h3 id="the-public-subnet">The Public Subnet&lt;/h3>
&lt;p>You don&amp;rsquo;t put hosts directly into a VPC, they need to go into a structure called a &amp;lsquo;subnet&amp;rsquo;, which is a &lt;em>part&lt;/em> of a VPC. Subnets get their own subset of the VPC&amp;rsquo;s available IP addresses, which you specify.&lt;/p>
&lt;p>Subnets are used to build &lt;em>zones&lt;/em> in a network. Why would you need this? Typically it is to manage security. You might have a &amp;lsquo;public zone&amp;rsquo; in which all hosts can be accessed from the internet, and a &amp;lsquo;private zone&amp;rsquo; which is inaccessible directly (and therefore a better location for hosts with sensitive data). You might have an &amp;lsquo;operator&amp;rsquo; zone, which only sysadmins can access, but they can use to get diagnostic information.&lt;/p>
&lt;p>Here&amp;rsquo;s a common subnet layout for multi-tiered applications:&lt;/p>
&lt;p>&lt;img src="images/img-4-subnets.png" alt="">&lt;/p>
&lt;p>The defining characteristics of zones is that they are used to create &lt;em>boundaries&lt;/em> to isolate hosts. These boundaries are normally secured by firewalls, traversed via gateways or NATs etc. We&amp;rsquo;re going to create two public subnets, one in each of the availability zones&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Create a public subnet for each AZ.
resource &amp;#34;aws_subnet&amp;#34; &amp;#34;public-a&amp;#34; {
vpc_id = &amp;#34;${aws_vpc.consul-cluster.id}&amp;#34;
cidr_block = &amp;#34;10.0.1.0/24&amp;#34; // i.e. 10.0.1.0 to 10.0.1.255
availability_zone = &amp;#34;ap-southeast-1a&amp;#34;
map_public_ip_on_launch = true
}
resource &amp;#34;aws_subnet&amp;#34; &amp;#34;public-b&amp;#34; {
vpc_id = &amp;#34;${aws_vpc.consul-cluster.id}&amp;#34;
cidr_block = &amp;#34;10.0.2.0/24&amp;#34; // i.e. 10.0.2.0 to 10.0.1.255
availability_zone = &amp;#34;ap-southeast-1b&amp;#34;
map_public_ip_on_launch = true
}
&lt;/code>&lt;/pre>&lt;p>With Terraform, resources can depend on each other. In this case, the subnets need to reference the ID of the VPC we want to place them in (so we use &lt;code>aws_vpc.consul-cluster.id&lt;/code>).&lt;/p>
&lt;h3 id="the-internet-gateway-route-tables-and-security-groups">The Internet Gateway, Route Tables and Security Groups&lt;/h3>
&lt;p>The final parts of the network you can see in the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/blob/master/network.tf">./infrastructure/network.tf&lt;/a> script. These are the Internet Gateway, Route Table and Security Group resources. Essentially they are for controlling access between hosts and the internet. AWS have a &lt;a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html">good guide&lt;/a> if you are not familiar with these resources; they don&amp;rsquo;t add much to the article so I&amp;rsquo;ll leave you to explore on your own.&lt;/p>
&lt;p>That&amp;rsquo;s it for the network, we now have the following structure:&lt;/p>
&lt;p>&lt;img src="images/img-1-network-1.png" alt="">&lt;/p>
&lt;p>If you want to see the code as it stands now, check the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/tree/step-1">Step 1&lt;/a> branch. Now we need to look at creating the hosts to install Consul on.&lt;/p>
&lt;h2 id="step-2---creating-the-consul-hosts">Step 2 - Creating the Consul Hosts&lt;/h2>
&lt;p>The Consul documentation recommends running in a cluster or 3 or 5 nodes&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. We want to set up a system which is self-healing - if we lose a node, we want to create a new one.&lt;/p>
&lt;p>Enter &lt;a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroup.html">Auto-Scaling Groups&lt;/a>. Auto-scaling groups allow us to define a template for an instance, and ask AWS to make sure there are always a certain number of these instances. If we lose an instance, a new one will be created to keep the group at the correct size&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>So we now need to create:&lt;/p>
&lt;ol>
&lt;li>A &amp;lsquo;Launch Configuration&amp;rsquo; which determines what instances our Auto-scaling Group creates&lt;/li>
&lt;li>A &amp;lsquo;user data script&amp;rsquo; which runs on newly created instances, which must install and start Consul&lt;/li>
&lt;li>An Auto-scaling group, configured to run five instances across the two public subnets&lt;/li>
&lt;li>A load balancer, configured to pass incoming requests for the Consul Admin console to the nodes&lt;/li>
&lt;/ol>
&lt;p>Or visually:&lt;/p>
&lt;p>&lt;img src="images/img-5-cluster-basic-2.png" alt="Basic Cluster Diagram">&lt;/p>
&lt;p>Let&amp;rsquo;s get to it.&lt;/p>
&lt;h3 id="the-launch-configuration--auto-scaling-group">The Launch Configuration &amp;amp; Auto-scaling Group&lt;/h3>
&lt;p>The Launch Configuration will define the characteristics of our instances and the auto-scaling group determines the size of our cluster:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Launch configuration for the consul cluster auto-scaling group.
resource &amp;#34;aws_launch_configuration&amp;#34; &amp;#34;consul-cluster-lc&amp;#34; {
name_prefix = &amp;#34;consul-node-&amp;#34;
image_id = &amp;#34;${lookup(var.ami_ecs_optimised, var.region)}&amp;#34;
instance_type = &amp;#34;t2.micro&amp;#34;
security_groups = [&amp;#34;${aws_security_group.consul-cluster-vpc.id}&amp;#34;]
lifecycle {
create_before_destroy = true
}
}
// Auto-scaling group for our cluster.
resource &amp;#34;aws_autoscaling_group&amp;#34; &amp;#34;consul-cluster-asg&amp;#34; {
name = &amp;#34;consul-asg&amp;#34;
launch_configuration = &amp;#34;${aws_launch_configuration.consul-cluster-lc.name}&amp;#34;
min_size = 5
max_size = 5
vpc_zone_identifier = [
&amp;#34;${aws_subnet.public-a.id}&amp;#34;,
&amp;#34;${aws_subnet.public-b.id}&amp;#34;
]
lifecycle {
create_before_destroy = true
}
}
&lt;/code>&lt;/pre>&lt;p>A few key things to note:&lt;/p>
&lt;ol>
&lt;li>I have omitted the &lt;code>tag&lt;/code> properties in the scripts for brevity&lt;/li>
&lt;li>The &amp;lsquo;image&amp;rsquo; for the launch configuration is looked up based on the region we&amp;rsquo;ve specified - we&amp;rsquo;re a basic linux image&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/li>
&lt;li>We are using micro instances, which are free-tier eligible&lt;/li>
&lt;li>The auto-scaling group spans both availability zones.&lt;/li>
&lt;/ol>
&lt;p>Once we run &lt;code>terraform apply&lt;/code>, we&amp;rsquo;ll see our auto-scaling group, which references the new launch configuration and works over multiple availability zones:&lt;/p>
&lt;p>&lt;img src="images/img-6-lc-asg.png" alt="Auto scaling group and launch configuration">&lt;/p>
&lt;p>We can also see the new instances:&lt;/p>
&lt;p>&lt;img src="images/img-7-instances.png" alt="Instances">&lt;/p>
&lt;p>These instances don&amp;rsquo;t do much yet though, we&amp;rsquo;ve not installed Docker or Consul.&lt;/p>
&lt;h3 id="installing-consul-and-accessing-the-admin-interface">Installing Consul and Accessing the Admin Interface&lt;/h3>
&lt;p>To set up our instances we use a &amp;lsquo;userdata&amp;rsquo; script&amp;rsquo; A userdata runs once when an instance is created. We can create a script in our repository, and reference it in our Terraform files.&lt;/p>
&lt;p>We add a new file called &lt;code>consul-node.sh&lt;/code> to a &lt;code>files&lt;/code> folder. This script installs Docker and runs Consul:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>yum install -y docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>usermod -a -G docker ec2-user
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service docker start
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Get my IP address.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>curl http://169.254.169.254/latest/meta-data/local-ipv4&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Instance IP is: &lt;/span>$IP&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Start the Consul server.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run -d --net&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name&lt;span style="color:#f92672">=&lt;/span>consul &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> consul agent -server -ui &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$IP&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -client&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;0.0.0.0&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bootstrap-expect&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here&amp;rsquo;s a breakdown of what we&amp;rsquo;re doing:&lt;/p>
&lt;ol>
&lt;li>Install Docker. These scripts run as root, so we add the ec2-user to the Docker group, meaning when we log in later on via SSH, we can run Docker&lt;/li>
&lt;li>Get our IP address. AWS provide a magic address (169.254.169.254) which lets you query data about your instance, see &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">Instance Metadata &amp;amp; User Metadata&lt;/a>&lt;/li>
&lt;li>Run the Consul docker image in server mode, with the UI enabled, expecting only one instance&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>The actual scripts contains more!&lt;/strong> Getting userdata scripts right, testing and debugging them is tricky. See how I do it in detail in &lt;a href="#Appendix-1-Logging">Appendix 1: Logging&lt;/a>.&lt;/p>
&lt;p>Now we need to tell Terraform to include this script as part of the instance metadata. Here&amp;rsquo;s how we do that:&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_launch_configuration&amp;#34; &amp;#34;consul-cluster-lc&amp;#34; {
/// ...add the line below....
user_data = &amp;#34;${file(&amp;#34;files/consul-node.sh&amp;#34;)}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>When Consul is running with the &lt;code>-ui&lt;/code> option, it provides an admin UI. You can try it by running Consul locally with &lt;code>docker run -p8500:8500 consul&lt;/code> and navigating to http://localhost:8500/ui.&lt;/p>
&lt;p>We can install a load balancer in front of our auto-scaling group, to automatically forward incoming traffic to a host. Here&amp;rsquo;s the config:&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_elb&amp;#34; &amp;#34;consul-lb&amp;#34; {
name = &amp;#34;consul-lb-a&amp;#34;
security_groups = [
&amp;#34;${aws_security_group.consul-cluster-vpc.id}&amp;#34;,
&amp;#34;${aws_security_group.web.id}&amp;#34;
]
subnets = [
&amp;#34;${aws_subnet.public-a.id}&amp;#34;,
&amp;#34;${aws_subnet.public-b.id}&amp;#34;
]
listener {
instance_port = 8500
instance_protocol = &amp;#34;http&amp;#34;
lb_port = 80
lb_protocol = &amp;#34;http&amp;#34;
}
health_check {
healthy_threshold = 2
unhealthy_threshold = 2
timeout = 3
target = &amp;#34;HTTP:8500/ui/&amp;#34;
interval = 30
}
}
&lt;/code>&lt;/pre>&lt;p>Blow-by-blow:&lt;/p>
&lt;ol>
&lt;li>Create a load balancer, with the same security groups as the rest of the VPC, but also a security group which allows web access&lt;/li>
&lt;li>Point to two subnets first subnet&lt;/li>
&lt;li>Forward HTTP 8500 traffic&lt;/li>
&lt;li>Configure a healthcheck&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>The final change we make is to add an &lt;code>outputs.tf&lt;/code> file, which lists all of the properties Terraform knows about which we want to save. All it includes is:&lt;/p>
&lt;pre tabindex="0">&lt;code>output &amp;#34;consul-dns&amp;#34; {
value = &amp;#34;${aws_elb.consul-lb.dns_name}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>When we finally run &lt;code>terraform apply&lt;/code>, we see the public DNS of our load balancer:&lt;/p>
&lt;p>&lt;img src="images/img-8-cluster-dns.png" alt="Screenshot showing &amp;rsquo;terraform apply&amp;rsquo; output, indicating our newly generated ELB&amp;rsquo;s public DNS">&lt;/p>
&lt;p>And running in a browser on port 8500 we see the Consul admin interface:&lt;/p>
&lt;p>&lt;img src="images/img-9-admin-ui.png" alt="Screenshot showing the Consul admin interface">&lt;/p>
&lt;p>Every time we refresh we will likely see a different node. We&amp;rsquo;ve actually created five clusters each of one node - what we now need to do is connect them all together into a single cluster of five nodes.&lt;/p>
&lt;p>If you want to see the code as it stands now, check the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/tree/step-2">Step 2&lt;/a> branch.&lt;/p>
&lt;h2 id="step-3---creating-the-cluster">Step 3 - Creating the Cluster&lt;/h2>
&lt;p>Creating the cluster is now not too much of a challenge. We will update the userdata script to tell the consul process we are expecting 5 nodes (via the &lt;a href="https://www.consul.io/docs/agent/options.html#_bootstrap_expect">&lt;code>bootstrap-expect&lt;/code>&lt;/a> flag.&lt;/p>
&lt;p>Here&amp;rsquo;s the updated script:&lt;/p>
&lt;pre tabindex="0">&lt;code># Get my IP address.
IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)
echo &amp;#34;Instance IP is: $IP&amp;#34;
# Start the Consul server.
docker run -d --net=host \
--name=consul \
consul agent -server -ui \
-bind=&amp;#34;$IP&amp;#34; \
-client=&amp;#34;0.0.0.0&amp;#34; \
-bootstrap-expect=&amp;#34;5&amp;#34;
&lt;/code>&lt;/pre>&lt;p>The problem is &lt;strong>this won&amp;rsquo;t work&lt;/strong>&amp;hellip; We need to tell each node the address of &lt;em>another&lt;/em> server in the cluster. For example, if we start five nodes, we should tell nodes 2-5 the address of node 1, so that the nodes can discover each other.&lt;/p>
&lt;p>The challenge is how do we get the IP of node 1? The IP addresses are determined by the network, we don&amp;rsquo;t preset them so cannot hard code them. Also, we can expect nodes to occasionally die and get recreated, so the IP addresses of nodes will in fact change over time.&lt;/p>
&lt;h3 id="getting-the-ip-addresses-of-nodes-in-the-cluster">Getting the IP addresses of nodes in the cluster&lt;/h3>
&lt;p>There&amp;rsquo;s a nice trick we can use here. We can ask AWS to give us the IP addresses of each host in the auto-scaling group. If we tell each node the addresses of the &lt;em>other nodes&lt;/em>, then they will elect a leader themselves&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="images/img-12-choose-leader-1.png" alt="Diagram showing how we decide on a leader IP">&lt;/p>
&lt;p>There are a couple of things we need to do to get this right. First, update the userdata script to provide the IPs of other nodes when we&amp;rsquo;re starting up, then update the &lt;strong>role&lt;/strong> of our nodes so that they have permissions to use the APIs we&amp;rsquo;re going to call.&lt;/p>
&lt;h3 id="getting-the-cluster-ips">Getting the Cluster IPs&lt;/h3>
&lt;p>This is actually fairly straightforward. We update our userdata script to the below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># A few variables we will refer to later...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ASG_NAME&lt;span style="color:#f92672">=&lt;/span>consul-asg
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>REGION&lt;span style="color:#f92672">=&lt;/span>ap-southeast-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EXPECTED_SIZE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Return the id of each instance in the cluster.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">function&lt;/span> cluster-instance-ids &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Grab every line which contains &amp;#39;InstanceId&amp;#39;, cut on double quotes and grab the ID:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># &amp;#34;InstanceId&amp;#34;: &amp;#34;i-example123&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">#....^..........^..^.....#4.....^...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> aws --region&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$REGION&lt;span style="color:#e6db74">&amp;#34;&lt;/span> autoscaling describe-auto-scaling-groups &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --auto-scaling-group-name $ASG_NAME &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | grep InstanceId &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | cut -d &lt;span style="color:#e6db74">&amp;#39;&amp;#34;&amp;#39;&lt;/span> -f4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Return the private IP of each instance in the cluster.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">function&lt;/span> cluster-ips &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> id in &lt;span style="color:#66d9ef">$(&lt;/span>cluster-instance-ids&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> aws --region&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$REGION&lt;span style="color:#e6db74">&amp;#34;&lt;/span> ec2 describe-instances &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --query&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Reservations[].Instances[].[PrivateIpAddress]&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --output&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --instance-ids&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$id&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Wait until we have as many cluster instances as we are expecting.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">while&lt;/span> COUNT&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cluster-instance-ids | wc -l&lt;span style="color:#66d9ef">)&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#f92672">[&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$COUNT&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -lt &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$EXPECTED_SIZE&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> echo &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$COUNT&lt;span style="color:#e6db74"> instances in the cluster, waiting for &lt;/span>$EXPECTED_SIZE&lt;span style="color:#e6db74"> instances to warm up...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sleep &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Get my IP address, all IPs in the cluster, then just the &amp;#39;other&amp;#39; IPs...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>curl http://169.254.169.254/latest/meta-data/local-ipv4&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mapfile -t ALL_IPS &amp;lt; &amp;lt;&lt;span style="color:#f92672">(&lt;/span>cluster-ips&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>OTHER_IPS&lt;span style="color:#f92672">=(&lt;/span> &lt;span style="color:#e6db74">${&lt;/span>ALL_IPS[@]/&lt;span style="color:#e6db74">${&lt;/span>IP&lt;span style="color:#e6db74">}}&lt;/span>/&lt;span style="color:#f92672">}&lt;/span> &lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Instance IP is: &lt;/span>$IP&lt;span style="color:#e6db74">, Cluster IPs are: &lt;/span>&lt;span style="color:#e6db74">${&lt;/span>CLUSTER_IPS[@]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">, Other IPs are: &lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[@]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Start the Consul server.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run -d --net&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name&lt;span style="color:#f92672">=&lt;/span>consul &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> consul agent -server -ui &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$IP&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[0]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[1]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[2]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[3]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bootstrap-expect&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$EXPECTED_SIZE&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Right, here&amp;rsquo;s what&amp;rsquo;s going on:&lt;/p>
&lt;ol>
&lt;li>We create a few variables we&amp;rsquo;ll use repeatedly&lt;/li>
&lt;li>We create a &lt;code>cluster-instance-ids&lt;/code> function which returns the ID of each instance in the auto-scaling group&lt;/li>
&lt;li>We create a &lt;code>cluster-ips&lt;/code> function which returns the private IP address of each instance in the cluster.&lt;/li>
&lt;li>We wait until the auto-scaling group has our expected number of instances (it can take a while for them all to be created)&lt;/li>
&lt;li>We get the 5 IP addresses&lt;/li>
&lt;li>We remove our IP from the array, leaving us with the IPs of the &lt;em>other&lt;/em> nodes&lt;/li>
&lt;li>We start the Consul agent in server mode, expecting 5 nodes and offering the IP of each other agent&lt;/li>
&lt;/ol>
&lt;p>The problem is, if we try to run the script we will fail, because calling the AWS APIs requires some permissions we don&amp;rsquo;t have. Let&amp;rsquo;s fix that.&lt;/p>
&lt;h3 id="creating-a-role-for-our-nodes">Creating a Role for our nodes&lt;/h3>
&lt;p>Our nodes now have a few special requirements. They need to be able to query the details of an auto-scaling group and get the IP of an instance&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We will need to create a policy which describes the permissions we need, create a role, attach the policy to the role and then ensure our instances are assigned the correct role. This is &lt;code>consul-node-role.tf&lt;/code> file:&lt;/p>
&lt;pre tabindex="0">&lt;code>// This policy allows an instance to discover a consul cluster leader.
resource &amp;#34;aws_iam_policy&amp;#34; &amp;#34;leader-discovery&amp;#34; {
name = &amp;#34;consul-node-leader-discovery&amp;#34;
path = &amp;#34;/&amp;#34;
policy = &amp;lt;&amp;lt;EOF
{
&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,
&amp;#34;Statement&amp;#34;: [
{
&amp;#34;Sid&amp;#34;: &amp;#34;Stmt1468377974000&amp;#34;,
&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,
&amp;#34;Action&amp;#34;: [
&amp;#34;autoscaling:DescribeAutoScalingInstances&amp;#34;,
&amp;#34;autoscaling:DescribeAutoScalingGroups&amp;#34;,
&amp;#34;ec2:DescribeInstances&amp;#34;
],
&amp;#34;Resource&amp;#34;: [
&amp;#34;*&amp;#34;
]
}
]
}
EOF
}
// Create a role which consul instances will assume.
// This role has a policy saying it can be assumed by ec2
// instances.
resource &amp;#34;aws_iam_role&amp;#34; &amp;#34;consul-instance-role&amp;#34; {
name = &amp;#34;consul-instance-role&amp;#34;
assume_role_policy = &amp;lt;&amp;lt;EOF
{
&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,
&amp;#34;Statement&amp;#34;: [
{
&amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34;,
&amp;#34;Principal&amp;#34;: {
&amp;#34;Service&amp;#34;: &amp;#34;ec2.amazonaws.com&amp;#34;
},
&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,
&amp;#34;Sid&amp;#34;: &amp;#34;&amp;#34;
}
]
}
EOF
}
// Attach the policy to the role.
resource &amp;#34;aws_iam_policy_attachment&amp;#34; &amp;#34;consul-instance-leader-discovery&amp;#34; {
name = &amp;#34;consul-instance-leader-discovery&amp;#34;
roles = [&amp;#34;${aws_iam_role.consul-instance-role.name}&amp;#34;]
policy_arn = &amp;#34;${aws_iam_policy.leader-discovery.arn}&amp;#34;
}
// Create a instance profile for the role.
resource &amp;#34;aws_iam_instance_profile&amp;#34; &amp;#34;consul-instance-profile&amp;#34; {
name = &amp;#34;consul-instance-profile&amp;#34;
roles = [&amp;#34;${aws_iam_role.consul-instance-role.name}&amp;#34;]
}
&lt;/code>&lt;/pre>&lt;p>Terraform is a little verbose here! Finally, we update our launch configuration to ensure that the instances assume this role.&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_launch_configuration&amp;#34; &amp;#34;consul-cluster-lc&amp;#34; {
// Add this line!!
iam_instance_profile = &amp;#34;${aws_iam_instance_profile.consul-instance-profile.id}&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s create the cluster again, with &lt;code>terraform apply&lt;/code>. When we log into the UI we should now see a cluster containing all five nodes:&lt;/p>
&lt;p>&lt;img src="images/img-13-cluster.png" alt="Screenshot of the Consul UI, showing that the Consul server is running on five nodes in the Datacenter">&lt;/p>
&lt;p>This code is all in the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/tree/step-3">Step 3&lt;/a> branch.&lt;/p>
&lt;p>If you are familiar with Consul, this may be all you need. If not, you might be interested in seeing how we actually create a new instance to host a service, register it with Consul and query its address.&lt;/p>
&lt;h2 id="step-4---adding-a-microservice">Step 4 - Adding a Microservice&lt;/h2>
&lt;p>I&amp;rsquo;ve created a docker image for as simple a microservice as you can get. It returns a quote from Futurama&amp;rsquo;s Zapp Brannigan. The image is tagged as &lt;code>dwmkerr/zapp-service&lt;/code>.&lt;/p>
&lt;p>On a new EC2 instance, running in either subnet, with the same roles as the Consul nodes, we run the following commands:&lt;/p>
&lt;pre tabindex="0">&lt;code># Install Docker
sudo su
yum update -y
yum install -y docker
service docker start
# Get my IP and the IP of any node in the server cluster.
IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)
NODE_ID=$(aws --region=&amp;#34;ap-southeast-1&amp;#34; autoscaling describe-auto-scaling-groups --auto-scaling-group-name &amp;#34;consul-asg&amp;#34; \
| grep InstanceId \
| cut -d &amp;#39;&amp;#34;&amp;#39; -f4 \
| head -1)
NODE_IP=$(aws --region=&amp;#34;ap-southeast-1&amp;#34; ec2 describe-instances \
--query=&amp;#34;Reservations[].Instances[].[PrivateIpAddress]&amp;#34; \
--output=&amp;#34;text&amp;#34; \
--instance-ids=&amp;#34;$NODE_ID&amp;#34;)
# Run the consul agent.
docker run -d --net=host \
consul agent \
-bind=&amp;#34;$IP&amp;#34; \
-join=$NODE_IP
# Run registrator - any Docker images will then be auto registered.
docker run -d \
--name=registrator \
--net=host \
--volume=/var/run/docker.sock:/tmp/docker.sock \
gliderlabs/registrator:latest \
consul://localhost:8500
# Run the example microservice - registrator will take care of letting consul know.
docker run -d -p 5000:5000 dwmkerr/zapp-service
&lt;/code>&lt;/pre>&lt;p>What&amp;rsquo;s going on here?&lt;/p>
&lt;ol>
&lt;li>We grab our own IP address and the IP address of the first instance we find in the server cluster, using the same tricks as before&lt;/li>
&lt;li>We run the Consul agent - telling it the IP to use to join the cluster&lt;/li>
&lt;li>We run &lt;a href="https://github.com/gliderlabs/registrator">Registrator&lt;/a>, a handy utility which will automatically register any new services we run to Consul&lt;/li>
&lt;li>We run a goofy sample microservice (which registrator will register for us)&lt;/li>
&lt;/ol>
&lt;p>Now we can check the Consul UI:&lt;/p>
&lt;p>&lt;img src="images/img-15-sample-service.png" alt="The Consul UI showing a new service">&lt;/p>
&lt;p>And there we have it. Our new node joins the cluster (as a client), we can register a new service with Consul.&lt;/p>
&lt;p>We can call this service from any node in the subnet, seeing output like the below:&lt;/p>
&lt;p>&lt;img src="images/img-x-zapp.png" alt="Screenshot of the Zapp service">&lt;/p>
&lt;p>In this example, I used a DNS SRV query to ask where the &lt;code>zapp-service&lt;/code> is, was told it was at &lt;code>10.0.2.158&lt;/code> on port &lt;code>5000&lt;/code>, then called the service, receiving a response. I can discover any service using this method, from any node. As services are added, removed, moved etc, I can ask Consul for accurate information on where to find them.&lt;/p>
&lt;p>Check the &lt;a href="">Step 4&lt;/a> branch to see the code in its current state.&lt;/p>
&lt;h2 id="step-5---spanner-throwing">Step 5 - Spanner Throwing&lt;/h2>
&lt;p>We can now try to throw some spanners in the works, to see how resilient the system is.&lt;/p>
&lt;p>According to the &lt;a href="https://www.consul.io/docs/internals/consensus.html#deployment-table">Deployment Table&lt;/a> from the Consul documentation, a cluster of five nodes means we have a quorum of three nodes (i.e. a minimum of three nodes are needed for a working system). This means we can tolerate the failure of two nodes.&lt;/p>
&lt;p>The easiest way to test this is to simply manually kill two nodes:&lt;/p>
&lt;p>&lt;img src="images/img-16-terminate.png" alt="Screenshot showing two AWS instances being terminated">&lt;/p>
&lt;p>If we pick two random nodes, as above, and terminate them, we see the cluster determines that we have two failed nodes but will still function (if one was the leader, a new leader will be automatically elected):&lt;/p>
&lt;p>&lt;img src="images/img-17-node-failure.png" alt="Screenshot showing the cluster highlighting two failed nodes">&lt;/p>
&lt;p>What&amp;rsquo;s nice about this setup is that no manual action is needed to recover. Our load balancer will notice the nodes are unhealthy and stop forwarding traffic. Our auto-scaling group will see the nodes have terminated and create two new ones, which will join the cluster in the same way as the original nodes. Once they join, the load balancer will find them healthy and bring them back into rotation.&lt;/p>
&lt;p>We can see from the load balancer monitoring that it notices we have unhealthy nodes and also notices when new ones come into service:&lt;/p>
&lt;p>&lt;img src="images/img-18-recovery-1.png" alt="Screenshot showing the load balancer monitoring">&lt;/p>
&lt;p>A quick check of the admin dashboard shows we now have a recovered system, with five healthy nodes:&lt;/p>
&lt;p>&lt;img src="images/img-18b-recovered.png" alt="Screenshot showing recovered system">&lt;/p>
&lt;p>The nodes which were terminated are still listed as failing. After 72 hours Consul will stop trying to periodically reconnect to these nodes and completely remove them&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="wrapping-up">Wrapping Up&lt;/h2>
&lt;p>Hopefully this should provide a good starting point to think about building your own resilient and robust systems for services like Consul.&lt;/p>
&lt;p>Interesting areas to look into to extend the project would be:&lt;/p>
&lt;ol>
&lt;li>Setting up alerts so that if we lose more than one node, we are informed&lt;/li>
&lt;li>Automating resilience tests by programatically bringing down servers and monitoring how long it takes the system to return to five nodes&lt;/li>
&lt;li>Instead of using a userdata script to set up a node, bake it into a new custom AMI with &lt;a href="https://www.packer.io/">Packer&lt;/a>&lt;/li>
&lt;li>Adding alerts for if we lose three of more nodes, which always requires manual intervention (see &lt;a href="https://www.consul.io/docs/guides/outage.html">Outage Recovery&lt;/a>)&lt;/li>
&lt;/ol>
&lt;p>As always, any questions or comments are welcome! All code is available at &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster">github.com/dwmkerr/terraform-consul-cluster&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="appendix-1-logging">Appendix 1: Logging&lt;/h2>
&lt;p>Small typos or mistakes in the userdata script are almost impossible to effectively diagnose. The scripts were actually built in the following way:&lt;/p>
&lt;ol>
&lt;li>Draft a script on my local machine which configures script logging and CloudWatch&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>&lt;/li>
&lt;li>Spin up a new EC2 instance manually&lt;/li>
&lt;li>SSH onto the instance, and run the script line by line until I&amp;rsquo;m sure it&amp;rsquo;s right&lt;/li>
&lt;li>Ensure the logs are forwarded to CloudWatch, then add the more complex features and repeatedly test&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;ve included CloudWatch logging in the code. In this write-up I&amp;rsquo;ve omitted this code as it is purely for diagnostics and doesn&amp;rsquo;t contribute to the main topic. The setup is in the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/blob/master/files/consul-node.sh">&lt;code>consul-node.sh&lt;/code>&lt;/a> and &lt;a href="%60https://github.com/dwmkerr/terraform-consul-cluster/blob/master/consul-node-role.tf">&lt;code>consul-node-role.tf&lt;/code>&lt;/a> files.&lt;/p>
&lt;p>If you want more details, let me know, or just check the code. I would heartily recommend setting up logging like this for all but the most straightforward projects:&lt;/p>
&lt;p>&lt;img src="images/img-19-cloudwatch-1.png" alt="Screenshot showing logs">&lt;/p>
&lt;p>Being able to diagnose issues like this is vital when working with distributed systems which may be generating many log files.&lt;/p>
&lt;h2 id="appendix-2-modularisaton">Appendix 2: Modularisaton&lt;/h2>
&lt;p>I got some a great PR from &lt;a href="https://github.com/arehmandev">arehmandev&lt;/a> which modularises the code. This makes it more reusable and cleans up the structure significantly. If you want to see the before/after, check the original PR at &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/pull/4">https://github.com/dwmkerr/terraform-consul-cluster/pull/4&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Footnotes&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Further Reading&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.consul.io/docs/internals/consensus.html">Consul - Consensus Protocol&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sitano.github.io/2015/10/06/abt-consul-outage/">What you have to know about Consul and how to beat the outage problem&lt;/a>, John Koepi&lt;/li>
&lt;/ol>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>This kind of pattern is critical in the world of microservices, where many small services will be running on a cluster. Services may die, due to errors or failing hosts, and be recreated on new hosts. Their IPs and ports may be ephemeral.It is essential that the system as a whole has a registry of where each service lives and how to access it. Such a registry must be &lt;em>resilient&lt;/em>, as it is an essential part of the system.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Most popular is a fairly loose term. Well ranked by Gartner and anecdotally with the largest infrastructure footprint. &lt;a href="https://www.gartner.com/doc/reprints?id=1-2G2O5FC&amp;amp;ct=150519&amp;amp;st=sb">https://www.gartner.com/doc/reprints?id=1-2G2O5FC&amp;amp;ct=150519&amp;amp;st=sb&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>This is AWS parlance again. An availabilty zone is an isolated datacenter. Theoretically, spreading nodes across AZs will increase resilience as it is less likely to have catastrophic failures or outages across multiple zones.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>I don&amp;rsquo;t get money from Udemy or anyone else for writing anything on this blog. All opinions are purely my own and influenced by my own experience, not sponsorship. Your milage may vary (yada yada) but I found the course quite good: &lt;a href="https://www.udemy.com/aws-certified-solutions-architect-associate/">https://www.udemy.com/aws-certified-solutions-architect-associate/&lt;/a>.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>For more expert readers that may sound horribly patronising, I don&amp;rsquo;t mean it to be. For many less experienced technologists the basics of networking might be more unfamiliar!&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>See &lt;a href="https://www.consul.io/docs/internals/consensus.html">https://www.consul.io/docs/internals/consensus.html&lt;/a>.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>A common pattern is to actually make the group size dynamic, responding to events. For example, we could have a group of servers which increases in size if the average CPU load of the hosts stays above 80% for five minutes, and scales down if it goes below 10% for ten minutes. This is more common for app and web servers and not needed for our system.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Specifically, the current latest &lt;a href="https://aws.amazon.com/amazon-linux-ami/">Amazon Linux AMI&lt;/a>.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Check the admin UI every 30 seconds, more than 3 seconds indicates a timeout and failure. Two failures in a row means an unhealthy host, which will be destroyed, two successes in a row for a new host means healthy, which means it will receive traffic.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>This is a fairly sophisticated topic in itself, see &lt;a href="https://www.consul.io/docs/internals/consensus.html">Consul - Consensus Protocol&lt;/a> for details.&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>In fact, we actually have more permissions required, because in the &amp;lsquo;real&amp;rsquo; code we also have logs forwarded to CloudWatch.&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>These nodes can be removed manually, see &lt;a href="https://www.consul.io/docs/commands/force-leave.html">Consul Force Leave&lt;/a>.&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Amazon&amp;rsquo;s service for managing and aggregating logs&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Run Amazon DynamoDB locally with Docker</title><link>https://dwmkerr.com/run-amazon-dynamodb-locally-with-docker/</link><pubDate>Thu, 27 Oct 2016 08:06:00 +0000</pubDate><guid>https://dwmkerr.com/run-amazon-dynamodb-locally-with-docker/</guid><description>&lt;p>&lt;strong>tl;dr:&lt;/strong> Run DynamoDB locally using Docker:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d -p 8000:8000 dwmkerr/dynamodb
&lt;/code>&lt;/pre>&lt;p>Try it out by opening the shell, &lt;a href="http://localhost:8000/shell">localhost:8000/shell&lt;/a>:&lt;/p>
&lt;p>&lt;img src="images/banner.jpg" alt="DynamoDB Shell">&lt;/p>
&lt;p>That&amp;rsquo;s all there is to it!&lt;/p>
&lt;h2 id="dynamodb">DynamoDB&lt;/h2>
&lt;p>&lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html">Amazon DynamoDB&lt;/a> is a NoSQL database-as-a-service, which provides a flexible and convenient repository for your services.&lt;/p>
&lt;p>Building applications which use DynamoDB is straightforward, there are APIs and clients for many languages and platforms.&lt;/p>
&lt;p>One common requirement is to be able to run a local version of DynamoDB, for testing and development purposes. To do this, you need to:&lt;/p>
&lt;ol>
&lt;li>Hit the &lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html">DynamoDB Local&lt;/a> documentation page&lt;/li>
&lt;li>Download an archive&lt;/li>
&lt;li>Extract it to a sensible location&lt;/li>
&lt;li>Run the extracted JAR, perhaps passing in some options&lt;/li>
&lt;/ol>
&lt;p>This can be a little cumbersome if you regularly use DynamoDB, so here&amp;rsquo;s a easier way:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -p 8000:8000 dwmkerr/dynamodb
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>dwmkerr/dynamodb&lt;/code> image runs the JAR in a container, exposing the database on port 8000 by default.&lt;/p>
&lt;p>You can see the &lt;a href="dockeri.co/image/dwmkerr/dynamodb">image on the Docker Hub&lt;/a> and the source code at &lt;a href="https://github.com/dwmkerr/docker-dynamodb">github.com/dwmkerr/docker-dynamodb&lt;/a>.&lt;/p>
&lt;h2 id="customising-dynamodb">Customising DynamoDB&lt;/h2>
&lt;p>You can pass any of &lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html">the documented commandline flags to DynamoDB&lt;/a>. There are instructions on the GitHub page. Here&amp;rsquo;s an example of how you can pass in a data directory, which allows DynamoDB data to be persisted after restarting a container (the image is ephemeral by default, as per &lt;a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/">Dockerfile best practices&lt;/a>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d -p 8000:8000 -v /tmp/data:/data/ dwmkerr/dynamodb -dbPath /data/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Running DynamoDB in a container gives an extra degree of flexibility and can speed up your workflow too!&lt;/p></description><category>CodeProject</category></item><item><title>Failures Connecting from Elastic Beanstalk servers to MongoDB on EC?</title><link>https://dwmkerr.com/failures-connecting-from-elastic-beanstalk-servers-to-mongodb-on-ec/</link><pubDate>Mon, 16 Mar 2015 10:34:04 +0000</pubDate><guid>https://dwmkerr.com/failures-connecting-from-elastic-beanstalk-servers-to-mongodb-on-ec/</guid><description>&lt;p>tl;dr?&lt;/p>
&lt;blockquote>
&lt;p>Check your mongodb.conf &lt;code>bind_ip&lt;/code> settings to make sure that you&amp;rsquo;re not allowing connections only from localhost.&lt;/p>
&lt;/blockquote>
&lt;p>This may just end up being the first part of a wider troubleshooting guide, but this is one I&amp;rsquo;ve spent a few hours fixing, after assuming I was making terrible mistakes with my security groups.&lt;/p>
&lt;p>If you find you cannot connect to your MongoDB server from an EB app server (or anything for that matter), before you spend ages checking your Elastic IP, VPC and Security Group config, don&amp;rsquo;t forget that you may have simply used &lt;code>bind_ip&lt;/code> in your config file.&lt;/p>
&lt;p>Check for:&lt;/p>
&lt;pre tabindex="0">&lt;code>bind_ip = 127.0.0.1
&lt;/code>&lt;/pre>&lt;p>Comment it out or remove it and restart:&lt;/p>
&lt;pre tabindex="0">&lt;code>service mongod restart
&lt;/code>&lt;/pre>&lt;p>Don&amp;rsquo;t forget to make sure your firewall is still set up correctly - only allow connections from IPs or even better other security groups you trust.&lt;/p></description><category>CodeProject</category></item></channel></rss>