<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Docker on dwmkerr.com</title><link>https://dwmkerr.com/tags/docker/</link><description>Recent content in Docker on dwmkerr.com</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><copyright>Copright &amp;copy; Dave Kerr</copyright><lastBuildDate>Thu, 04 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://dwmkerr.com/tags/docker/index.xml" rel="self" type="application/rss+xml"/><item><title>Observations, tips and tricks for the CKA certification</title><link>https://dwmkerr.com/tips-for-cka/</link><pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate><guid>https://dwmkerr.com/tips-for-cka/</guid><description>&lt;p>In this article I&amp;rsquo;ll share some observations, tips and tricks for the &lt;a href="https://www.linuxfoundation.org/">Linux Foundation&amp;rsquo;s&lt;/a> &amp;ldquo;&lt;a href="https://training.linuxfoundation.org/certification/certified-kubernetes-administrator-cka/">Certified Kubernetes Administrator&lt;/a> certification and exam.&lt;/p>
&lt;p>I&amp;rsquo;ve been operating Kubernetes in multiple environments for a few years now. I thought this would be an easy certification to get, but I was surprised by how hard it was!&lt;/p>
&lt;p>I took this exam without doing any formal training, I mostly focused on the areas of the curriculum which I knew I was a little weak at. The task-based structure for the exam I thought was really excellent. It took me two attempts to pass, and I learnt a few things along the way.&lt;/p>
&lt;p>Here I&amp;rsquo;ll share some thoughts on the certification which hopefully will be useful if you are considering taking it!&lt;/p>
&lt;!-- vim-markdown-toc GFM -->
&lt;ul>
&lt;li>&lt;a href="#tip-do-the-right-certification">Tip: Do the right Certification!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tip-understand-the-format">Tip: Understand the Format!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tip-know-your-vim">Tip: Know your Vim&lt;/a>&lt;/li>
&lt;li>&lt;a href="#you-need-to-know-the-architecture-of-kubernetes">You need to know the architecture of Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tip-you-need-to-know-linux-sysadmin">Tip: You Need to know Linux Sysadmin&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tip-dry-run-is-your-friend">Tip: &amp;ldquo;Dry Run&amp;rdquo; is your friend&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tip-know-how-to-troubleshoot-networking">Tip: Know how to troubleshoot networking&lt;/a>&lt;/li>
&lt;li>&lt;a href="#tip-nail-the-easy-questions-quickly">Tip: Nail the easy questions quickly&lt;/a>&lt;/li>
&lt;li>&lt;a href="#thats-it">That&amp;rsquo;s it!&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- vim-markdown-toc -->
&lt;h2 id="tip-do-the-right-certification">Tip: Do the right Certification!&lt;/h2>
&lt;p>The CKA exam tests &lt;em>administration&lt;/em> and &lt;em>operation&lt;/em> skills and techniques for Kubernetes. If you have set up and administered clusters before, this will likely not be too challenging. But if you&amp;rsquo;ve never set up a cluster by hand, troubleshot weird issues, fixed clusters and so on, then this is likely going to be very hard.&lt;/p>
&lt;p>There is a certification which is much more geared towards developers who use Kubernetes, but don&amp;rsquo;t necessarily administer it - that&amp;rsquo;s the &lt;a href="https://www.cncf.io/certification/ckad/">CKAD&lt;/a> exam and might be the one to take if you are not too familiar with system administration.&lt;/p>
&lt;h2 id="tip-understand-the-format">Tip: Understand the Format!&lt;/h2>
&lt;p>This is not a multiple choice question exam. It&amp;rsquo;s a task based exam, meaning you have about 22 or so specific tasks to complete, in a web browser which has a terminal connected to a cluster.&lt;/p>
&lt;p>It is open-book - meaning that you can use the &lt;a href="https://kubernetes.io/docs/home/">Kubernetes Documentation&lt;/a> during the exam. It&amp;rsquo;s not a memory test of specific flags for commands or whatever, it will really require you to work with a running cluster. This means you&amp;rsquo;ll have to be pretty familiar with &lt;code>kubectl&lt;/code>, &lt;code>kubeadm&lt;/code> and also Linux in general!&lt;/p>
&lt;h2 id="tip-know-your-vim">Tip: Know your Vim&lt;/h2>
&lt;p>In the two exams I took, &lt;code>nano&lt;/code> was available. But if you are using &lt;code>nano&lt;/code> to work with files you may struggle for time.&lt;/p>
&lt;p>I spent a &lt;em>lot&lt;/em> of time in &lt;code>vim&lt;/code> in the exam. &lt;code>vim&lt;/code> is my main text editor for day to day work, so I&amp;rsquo;m fairly familiar with it. Knowing how to quickly copy a file (lets say for example a file which represents a deployment) and quickly manipulate the text in it will be crucial. Make sure you are going to be using a text editor which you can be efficient in!&lt;/p>
&lt;p>You won&amp;rsquo;t be using a graphical text editor to work with files, so being competent in a terminal editor like &lt;code>vim&lt;/code> or &lt;code>emacs&lt;/code> could make a big difference. Of course you could install your favourite text editor, but you won&amp;rsquo;t be able to use a graphical editor like VS Code.&lt;/p>
&lt;p>Also, as in most Linux distributions, &lt;code>screen&lt;/code> is available out of the box, and &lt;code>tmux&lt;/code> can also be installed. If you are familiar with either of these terminal mutliplexers it could save you a tonne of time, for example being able to run &lt;code>watch -n 5 -d kubectl get pods&lt;/code> in one pane while applying resources in another.&lt;/p>
&lt;h2 id="you-need-to-know-the-architecture-of-kubernetes">You need to know the architecture of Kubernetes&lt;/h2>
&lt;p>This exam will require you to deal with trivial tasks such as running a deployment or creating a volume. But the questions which focus on that tend to only count for one or two percent of the overall grade each. Questions which deal with troubleshooting actual Kubernetes issues could count for six or seven percent each.&lt;/p>
&lt;p>This means you &lt;em>need&lt;/em> to know how Kubernetes is architecture. The &lt;code>kubelet&lt;/code> which runs on nodes, the API server, the &lt;code>etcd&lt;/code> store, all of these things you &lt;em>have&lt;/em> to understand how they work and how they fit together.&lt;/p>
&lt;p>The online documentation covers the architecture in detail, here&amp;rsquo;s the best place to start:&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/overview/components/">https://kubernetes.io/docs/concepts/overview/components/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/overview/components/">&lt;img src="./images/k8s-architecture.png" alt="Kubernetes Architecture">&lt;/a>&lt;/p>
&lt;p>You will need to know how the control plane works, how nodes communicate, how transport of messages works and is secured if you are going to have a chance at dealing with the harder questions.&lt;/p>
&lt;h2 id="tip-you-need-to-know-linux-sysadmin">Tip: You Need to know Linux Sysadmin&lt;/h2>
&lt;p>If you are not familiar with &lt;code>systemctl&lt;/code>, &lt;code>journalctl&lt;/code>, &lt;code>apt&lt;/code>, &lt;code>systemd&lt;/code> units and how the core Kubernetes components are configured, you&amp;rsquo;ll really struggle.&lt;/p>
&lt;p>Look over the &lt;a href="https://github.com/cncf/curriculum">CNCF curriculum&lt;/a> - expect to not just have to know how to deal with &amp;lsquo;happy path&amp;rsquo; situations, but also broken clusters, incorrect configuration and so on.&lt;/p>
&lt;h2 id="tip-dry-run-is-your-friend">Tip: &amp;ldquo;Dry Run&amp;rdquo; is your friend&lt;/h2>
&lt;p>One thing which helped me a lot in my second attempt at the exam was the &lt;code>--dry-run&lt;/code> flag. Before you create resources or change anything, run the operation with the &lt;code>--dry-run&lt;/code> flag and see whether the output is what you would expect.&lt;/p>
&lt;p>This is a quick and easy way to see the changes to the cluster which you are going to apply - and troubleshoot them - before making any actual changes.&lt;/p>
&lt;h2 id="tip-know-how-to-troubleshoot-networking">Tip: Know how to troubleshoot networking&lt;/h2>
&lt;p>Networking in Kubernetes is complex. You must be able to troubleshoot networking issues in the cluster to be able to deal with the more complex tasks.&lt;/p>
&lt;p>This means that you should know how to be able to run typical networking tools like &lt;code>dig&lt;/code>, &lt;code>nslookup&lt;/code>, &lt;code>telnet&lt;/code> etc, in the cluster itself.&lt;/p>
&lt;p>If you are not familiar with these tools you might need to take an online course in Kubernetes or Linux Networking Administration before considering this certification. The &lt;a href="https://training.linuxfoundation.org/certification/linux-foundation-certified-sysadmin-lfcs/">Linux Certified Systems Administrator&lt;/a> training would be a good place to start.&lt;/p>
&lt;p>If you have taken the &lt;a href="https://success.docker.com/certification">Docker Certified Associate&lt;/a> exam then some of this should be familiar. If you are not very familiar with how Docker itself works, you&amp;rsquo;ll likely struggle with Kubernetes.&lt;/p>
&lt;h2 id="tip-nail-the-easy-questions-quickly">Tip: Nail the easy questions quickly&lt;/h2>
&lt;p>There are a lot of tasks which only count for one or two percent each; these ones you should be able to complete in a few minutes. You&amp;rsquo;ll need all the time in the exam to work on the really hard questions which deal with diagnosing and fixing cluster issues.&lt;/p>
&lt;p>Know your core Kubernetes concepts; if you have done the CKAD exam you should be good, if not, check the curriculum and make sure you can quickly complete all of the trivial tasks without wasting too much time.&lt;/p>
&lt;h2 id="thats-it">That&amp;rsquo;s it!&lt;/h2>
&lt;p>Hopefully this was helpful! Good luck if you are taking the exam and hopefully you&amp;rsquo;ll find it a challenging but rewarding experience. I&amp;rsquo;ve taken many exams over the years but this was one of the most challenging, but also one of the most enjoyable, I really felt like it was testing practical techniques rather than your ability to just remember random commands and flags.&lt;/p>
&lt;p>As always, if you have any comments or questions, please just add them in the section below!&lt;/p>
&lt;p>&lt;img src="./images/cka-cert.png" alt="CKA Certification">&lt;/p></description><category>CodeProject</category></item><item><title>Manipulating Istio and other Custom Kubernetes Resources in Golang</title><link>https://dwmkerr.com/manipulating-istio-and-other-custom-kubernetes-resources-in-golang/</link><pubDate>Mon, 08 Oct 2018 21:34:02 +0000</pubDate><guid>https://dwmkerr.com/manipulating-istio-and-other-custom-kubernetes-resources-in-golang/</guid><description>&lt;p>In this article I&amp;rsquo;ll demonstrate how to use Golang to manipulate Kubernetes Custom Resources, with Istio as an example. No knowledge of Istio is needed, I&amp;rsquo;ll just use it to demonstrate the concepts!&lt;/p>
&lt;p>&lt;img src="images/code-2.jpg" alt="code">&lt;/p>
&lt;p>&lt;a href="https://istio.io">Istio&lt;/a> is a highly popular Service Mesh platform which allows engineers to quickly add telemetry, advanced traffic management and more to their service-based applications.&lt;/p>
&lt;p>One interesting element of how Istio works is that when deployed into a Kubernetes cluster, many key configuration objects are handled as &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources&lt;/a>. Custom Resources are a very powerful Kubernetes feature, which allow you to create your own &amp;lsquo;first class&amp;rsquo; resources (just like Pods, ReplicaSets, Deployments or whatever) and then interface with them using &lt;code>kubectl&lt;/code> or the Kubernetes APIs.&lt;/p>
&lt;p>In this article I&amp;rsquo;ll show you how to interface with these Custom Resources using the Golang Kubernetes client.&lt;/p>
&lt;h2 id="crds-a-quick-overview">CRDs: A Quick Overview&lt;/h2>
&lt;p>When you set up Istio for your cluster, one common thing you will likely do is specify how you will route traffic. This can be quite sophisticated, as shown below:&lt;/p>
&lt;p>&lt;img src="images/TrafficManagementOverview.svg" alt="TrafficManagementOverview">&lt;/p>
&lt;p>&lt;a href="https://istio.io/docs/concepts/traffic-management/">Figure 1: Istio Traffic Management Examples, from istio.io&lt;/a>&lt;/p>
&lt;p>One way for a system like this to be configured would be to have a ConfigMap which contains the definition of how services are routed.&lt;/p>
&lt;p>However, Istio actually registers new types of resources (Custom Resource Definitions) which represent things like Gateways or Services. We can create/update/delete/manipulate them just like any other Kubernetes object.&lt;/p>
&lt;p>For example, I could create a virtual service for the example above with something like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF | kubectl create -f -
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">kind: VirtualService
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> name: service2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> hosts:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - &amp;#34;*&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> gateways:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - demo1-gateway
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> http:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - route:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - destination:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> host: service2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> subset: v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> weight: 95
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - destination:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> host: service2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> subset: v2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> weight: 5
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Again, the important thing is not the specific content of this resource, more the fact that I can treat my Istio resources just like I would any other Kubernetes object:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get virtualservices.networking.istio.io
NAME AGE
service2 93s
&lt;/code>&lt;/pre>&lt;p>Or:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl delete virtualservices.networking.istio.io/service2
&lt;/code>&lt;/pre>&lt;p>I can use &lt;code>edit&lt;/code>, &lt;code>describe&lt;/code>, register lifecycle events, watch for changes, and so on.&lt;/p>
&lt;h2 id="working-with-crds-in-golang">Working with CRDs in Golang&lt;/h2>
&lt;p>The &lt;a href="https://github.com/kubernetes/client-go">Golang Kubernetes Client&lt;/a> allows you to create strongly defined types which you can then use to interface with CRDs. An example is in the Red Hat blog post &lt;a href="https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/">Kubernetes Deep Dive: Code Generation for Custom Resources&lt;/a>.&lt;/p>
&lt;p>This is an excellent approach, but can feel pretty heavy if you want to quickly access some data, and don&amp;rsquo;t want to have to generate a lot of code.&lt;/p>
&lt;p>There is an alternative, which is to use the &lt;a href="https://github.com/kubernetes/client-go/blob/master/dynamic/interface.go">&lt;code>DynamicClient&lt;/code>&lt;/a>. The &lt;em>preferred&lt;/em> approach seems to be the first, which involves code generation, so little documentation exists for the second approach. However, it is actually very simple.&lt;/p>
&lt;p>Here&amp;rsquo;s an example of how you can list all Istio &lt;code>VirtualService&lt;/code> resources, without having to generate any code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">metav1&lt;/span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/dynamic&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Create a Dynamic Client to interface with CRDs.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">dynamicClient&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">dynamic&lt;/span>.&lt;span style="color:#a6e22e">NewForConfig&lt;/span>(&lt;span style="color:#a6e22e">config&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Create a GVR which represents an Istio Virtual Service.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">virtualServiceGVR&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">GroupVersionResource&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Group&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;networking.istio.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Version&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1alpha3&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Resource&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;virtualservices&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// List all of the Virtual Services.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">virtualServices&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">dynamicClient&lt;/span>.&lt;span style="color:#a6e22e">Resource&lt;/span>(&lt;span style="color:#a6e22e">virtualServiceGVR&lt;/span>).&lt;span style="color:#a6e22e">Namespace&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;default&amp;#34;&lt;/span>).&lt;span style="color:#a6e22e">List&lt;/span>(&lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ListOptions&lt;/span>{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">_&lt;/span>, &lt;span style="color:#a6e22e">virtualService&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#66d9ef">range&lt;/span> &lt;span style="color:#a6e22e">virtualServices&lt;/span>.&lt;span style="color:#a6e22e">Items&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Printf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;VirtualService: %s\n&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">virtualService&lt;/span>.&lt;span style="color:#a6e22e">GetName&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This snippet omits setup and error-handling for clarity, the full example is in the &lt;a href="https://gist.github.com/dwmkerr/09ac0fd98595460456e17d5ef0c77667">k8s-list-virtualservices.go&lt;/a> gist.&lt;/p>
&lt;h2 id="patching-crds-in-golang">Patching CRDs in Golang&lt;/h2>
&lt;p>You may have noticed that the &lt;code>.Resource().Namespace().List()&lt;/code> code looks very similar to the structure for making API calls when using the Kubernetes &lt;code>Clientset&lt;/code>. In fact, it is essentially the same. Looking at &lt;a href="https://github.com/kubernetes/client-go/blob/master/dynamic/interface.go">the interface&lt;/a>, you can see you have all of the operations you&amp;rsquo;d expect:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Create&lt;/code>&lt;/li>
&lt;li>&lt;code>Update&lt;/code>&lt;/li>
&lt;li>&lt;code>Delete&lt;/code>&lt;/li>
&lt;li>&lt;code>Get&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>And so on. This is nice because you can use the same trick in my article &amp;lsquo;&lt;a href="https://www.dwmkerr.com/patching-kubernetes-resources-in-golang/">Patching Kubernetes Resources in Golang&lt;/a>&amp;rsquo; to manipulate these entities, without ever having to create a structure to represent it.&lt;/p>
&lt;p>Here&amp;rsquo;s another abbreviated example, this time showing how we can adjust the weight of the routing from the services to 50%/50%:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">metav1&lt;/span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/dynamic&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Create a GVR which represents an Istio Virtual Service.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">virtualServiceGVR&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">GroupVersionResource&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Group&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;networking.istio.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Version&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1alpha3&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Resource&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;virtualservices&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Weight the two routes - 50/50.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">patchPayload&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> make([]&lt;span style="color:#a6e22e">PatchUInt32Value&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchPayload&lt;/span>[&lt;span style="color:#ae81ff">0&lt;/span>].&lt;span style="color:#a6e22e">Op&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;replace&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchPayload&lt;/span>[&lt;span style="color:#ae81ff">0&lt;/span>].&lt;span style="color:#a6e22e">Path&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;/spec/http/0/route/0/weight&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchPayload&lt;/span>[&lt;span style="color:#ae81ff">0&lt;/span>].&lt;span style="color:#a6e22e">Value&lt;/span> = &lt;span style="color:#ae81ff">50&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchPayload&lt;/span>[&lt;span style="color:#ae81ff">1&lt;/span>].&lt;span style="color:#a6e22e">Op&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;replace&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchPayload&lt;/span>[&lt;span style="color:#ae81ff">1&lt;/span>].&lt;span style="color:#a6e22e">Path&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;/spec/http/0/route/1/weight&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchPayload&lt;/span>[&lt;span style="color:#ae81ff">1&lt;/span>].&lt;span style="color:#a6e22e">Value&lt;/span> = &lt;span style="color:#ae81ff">50&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">patchBytes&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">Marshal&lt;/span>(&lt;span style="color:#a6e22e">patchPayload&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Apply the patch to the &amp;#39;service2&amp;#39; service.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">_&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">dynamicClient&lt;/span>.&lt;span style="color:#a6e22e">Resource&lt;/span>(&lt;span style="color:#a6e22e">virtualServiceGVR&lt;/span>).&lt;span style="color:#a6e22e">Namespace&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;default&amp;#34;&lt;/span>).&lt;span style="color:#a6e22e">Patch&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;service2&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">types&lt;/span>.&lt;span style="color:#a6e22e">JSONPatchType&lt;/span>, &lt;span style="color:#a6e22e">patchBytes&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>See the full example in the gist &lt;a href="https://gist.github.com/dwmkerr/7332888e092156ce8ce4ea551b0c321f">k8s-patch-virtualservice.go&lt;/a>&lt;/p>
&lt;p>After running the sample, you can use the Kubernetes CLI to verify the changes:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get virtualservices.networking.istio.io/service2 -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
clusterName: &amp;#34;&amp;#34;
creationTimestamp: 2018-10-08T09:53:16Z
generation: 0
name: service2
namespace: default
resourceVersion: &amp;#34;487435&amp;#34;
selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/service2
uid: fac5930c-cadf-11e8-90a2-42010a94005b
spec:
gateways:
- demo1-gateway
hosts:
- &amp;#39;*&amp;#39;
http:
- route:
- destination:
host: service2
subset: v1
weight: 50
- destination:
host: service2
subset: v2
weight: 50
&lt;/code>&lt;/pre>&lt;h2 id="keep-it-simple">Keep It Simple!&lt;/h2>
&lt;p>That&amp;rsquo;s it! This trick made something I was working on a &lt;em>lot&lt;/em> easier, but it took a little bit of experimentation to get right. I hope you find the approach useful. Please share any thoughts/questions in the comments.&lt;/p>
&lt;h2 id="further-reading">Further Reading&lt;/h2>
&lt;p>The following articles were using in working out this approach:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/">Red Hat: Deep Dive: Code Generation for Custom Resources&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Kubernetes Docs: Custom Resources&lt;/a>&lt;/li>
&lt;/ul></description><category>CodeProject</category></item><item><title>Patching Kubernetes Resources in Golang</title><link>https://dwmkerr.com/patching-kubernetes-resources-in-golang/</link><pubDate>Tue, 24 Jul 2018 06:33:17 +0000</pubDate><guid>https://dwmkerr.com/patching-kubernetes-resources-in-golang/</guid><description>&lt;p>Recently I needed to be able to quickly adjust the number of replicas in a Kubernetes Replication Controller. The original solution I&amp;rsquo;d seen pulled down the spec, modified it, then updated it. There&amp;rsquo;s a better way!&lt;/p>
&lt;p>&lt;img src="images/patch-1.jpg" alt="Kuberentes Patch API">&lt;/p>
&lt;p>There&amp;rsquo;s a &lt;a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/">patch API for Kubernetes resources&lt;/a>. Patching resources is faster and easier than pulling them and updating the spec wholesale. However, the documentation is a little limited.&lt;/p>
&lt;p>After some trial and error I got it working, here&amp;rsquo;s the solution. I thought it might be helpful to share for others!&lt;/p>
&lt;h3 id="the-solution">The Solution&lt;/h3>
&lt;p>I&amp;rsquo;ll start with the solution. If this is all you need, you are good to go. The details of how this works are presented afterwards. In this example I&amp;rsquo;ll update the number of replicas in the &lt;code>my-rc&lt;/code> controller:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">package&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;encoding/json&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;fmt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">types&lt;/span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/apimachinery/pkg/types&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/kubernetes&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/plugin/pkg/client/auth&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Leave blank for the default context in your kube config.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">context&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Name of the replication controller to scale, and the desired number of replicas.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">replicationControllerName&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;my-rc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">replicas&lt;/span> = uint32(&lt;span style="color:#ae81ff">3&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// patchStringValue specifies a patch operation for a string.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">patchStringValue&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Op&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;op&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Path&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;path&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Value&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// patchStringValue specifies a patch operation for a uint32.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">patchUInt32Value&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Op&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;op&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Path&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;path&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Value&lt;/span> &lt;span style="color:#66d9ef">uint32&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">scaleReplicationController&lt;/span>(&lt;span style="color:#a6e22e">clientSet&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">kubernetes&lt;/span>.&lt;span style="color:#a6e22e">Clientset&lt;/span>, &lt;span style="color:#a6e22e">replicasetName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>, &lt;span style="color:#a6e22e">scale&lt;/span> &lt;span style="color:#66d9ef">uint32&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">payload&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> []&lt;span style="color:#a6e22e">patchUInt32Value&lt;/span>{{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Op&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;replace&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Path&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/spec/replicas&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Value&lt;/span>: &lt;span style="color:#a6e22e">scale&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">payloadBytes&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">Marshal&lt;/span>(&lt;span style="color:#a6e22e">payload&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">_&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">clientSet&lt;/span>.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">CoreV1&lt;/span>().
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ReplicationControllers&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;default&amp;#34;&lt;/span>).
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Patch&lt;/span>(&lt;span style="color:#a6e22e">replicasetName&lt;/span>, &lt;span style="color:#a6e22e">types&lt;/span>.&lt;span style="color:#a6e22e">JSONPatchType&lt;/span>, &lt;span style="color:#a6e22e">payloadBytes&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Get the local kube config.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Printf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Connecting to Kubernetes Context %v\n&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">context&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">config&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">clientcmd&lt;/span>.&lt;span style="color:#a6e22e">NewNonInteractiveDeferredLoadingClientConfig&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">clientcmd&lt;/span>.&lt;span style="color:#a6e22e">NewDefaultClientConfigLoadingRules&lt;/span>(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">clientcmd&lt;/span>.&lt;span style="color:#a6e22e">ConfigOverrides&lt;/span>{&lt;span style="color:#a6e22e">CurrentContext&lt;/span>: &lt;span style="color:#a6e22e">context&lt;/span>}).&lt;span style="color:#a6e22e">ClientConfig&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> panic(&lt;span style="color:#a6e22e">err&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Creates the clientset
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">clientset&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">kubernetes&lt;/span>.&lt;span style="color:#a6e22e">NewForConfig&lt;/span>(&lt;span style="color:#a6e22e">config&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> panic(&lt;span style="color:#a6e22e">err&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Scale our replication controller.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Printf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Scaling replication controller %v to %v\n&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">replicationControllerName&lt;/span>, &lt;span style="color:#a6e22e">replicas&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">err&lt;/span> = &lt;span style="color:#a6e22e">scaleReplicationController&lt;/span>(&lt;span style="color:#a6e22e">clientset&lt;/span>, &lt;span style="color:#a6e22e">replicationControllerName&lt;/span>, &lt;span style="color:#a6e22e">replicas&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> panic(&lt;span style="color:#a6e22e">err&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This code is also available in the &lt;a href="https://gist.github.com/dwmkerr/447692c8bba28929ef914239781c4e59">k8s-patch.go&lt;/a> gist.&lt;/p>
&lt;h3 id="the-mechanism">The Mechanism&lt;/h3>
&lt;p>The Kubernetes Patch API supports a few different methods for modifying resources. It is important to be aware that there is not a universally accepted &amp;lsquo;standard&amp;rsquo; approach to representing a &lt;em>change&lt;/em> to a resource in a REST API.&lt;/p>
&lt;p>There are three strategies you can use to patch:&lt;/p>
&lt;ol>
&lt;li>&lt;code>merge&lt;/code>: follows the &lt;a href="https://tools.ietf.org/html/rfc7386">JSON Merge Patch Spec (RFC 7386)&lt;/a>&lt;/li>
&lt;li>&lt;code>stragetic&lt;/code>: A strategic merge, which addresses some limitations of the merge patch (noted in &lt;a href="%5Bdocs/devel/api-conventions.md#patch-operations%5D(https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/devel/api-conventions.md#patch-operations)">this doc&lt;/a>.&lt;/li>
&lt;li>&lt;code>json&lt;/code>: follows the &lt;a href="https://tools.ietf.org/html/rfc6902">JSON Patch Spec (RFC 6902)&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>These are documented in detail at:&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/devel/api-conventions.md#patch-operations">docs/devel/api-conventions.md#patch-operations&lt;/a>&lt;/p>
&lt;p>The mechanism I&amp;rsquo;ve used here is &lt;code>json&lt;/code>, which I think is the clearest to the reader. To use this strategy we need to build a payload describing what we are changing. This might look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;op&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;replace&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;path&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/spec/replicas&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>op&lt;/code> field can be &lt;code>remove&lt;/code>, &lt;code>replace&lt;/code>, &lt;code>add&lt;/code> etc etc (all the details are in the &lt;a href="https://tools.ietf.org/html/rfc6902">RFC 6902)&lt;/a>, or the slightly more readable &lt;a href="jsonpatch.com">jsonpatch.com&lt;/a>). This allows the operation to be very &lt;em>explicit&lt;/em> to the reader, which is helpful. We create a struct which represents an operation on a string or integer (or whatever data type we need), serialize it and pass to the API.&lt;/p>
&lt;p>Under the hood, the Golang client will simply translate this into an HTTP call which will look like something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>PATCH /api/v1/namespaces/default/replicationcontrollers/app-server-blue HTTP/1.1
Host: 127.0.0.1
Content-Type: application/json-patch+json
Content-Length: 70
[{
&amp;#34;op&amp;#34;: &amp;#34;replace&amp;#34;,
&amp;#34;path&amp;#34;: &amp;#34;/spec/replicas&amp;#34;,
&amp;#34;value&amp;#34;: 4
}]
&lt;/code>&lt;/pre>&lt;p>This corresponds to the documentation on the &lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/devel/api-conventions.md#patch-operations">Patch Operations&lt;/a>. Note that the patch operation type is specified in the &lt;code>Content-Type&lt;/code> header.&lt;/p>
&lt;p>Hopefully this&amp;rsquo;ll help you if you need to patch resources, are struggling with the docs and are a Go noob like me! Any tips on how to make the code cleaner or more idomatic would be welcome.&lt;/p>
&lt;p>Thanks to the following articles and issues which helped me unpick this:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stackoverflow.com/questions/43415728/kubernetes-go-client-patch-example">Stack Overflow: Kubernetes Go Client Patch Example&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/">Kubernetes Docs: Update API Objects in Place Using kubectl patch&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/devel/api-conventions.md#patch-operations">Kubernetes Docs: Patch Operations&lt;/a>&lt;/li>
&lt;/ul></description><category>CodeProject</category></item><item><title>The Death of Microservice Madness in 2018</title><link>https://dwmkerr.com/the-death-of-microservice-madness-in-2018/</link><pubDate>Fri, 12 Jan 2018 10:52:25 +0000</pubDate><guid>https://dwmkerr.com/the-death-of-microservice-madness-in-2018/</guid><description>&lt;p>&lt;a href="https://www.campusmvp.es/recursos/post/la-muerte-de-la-locura-de-los-microservicios-en-2018.aspx">En EspaÃ±ol&lt;/a> | &lt;a href="https://www.reddit.com/r/programming/comments/7pxriw/the_death_of_microservice_madness_in_2018/">Reddit Thread&lt;/a> | &lt;a href="https://news.ycombinator.com/item?id=16200007">Hacker News Thread&lt;/a>&lt;/p>
&lt;p>Microservices became a very popular topic over the last couple of years&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. &amp;lsquo;Microservice madness&amp;rsquo; goes something like this:&lt;/p>
&lt;blockquote>
&lt;p>Netflix are great at devops.
Netflix do microservices.
Therefore: If I do microservices, I am great at devops.&lt;/p>
&lt;/blockquote>
&lt;p>There are many cases where great efforts have been made to adopt microservice patterns without necessarily understanding how the costs and benefits will apply to the specifics of the problem at hand.&lt;/p>
&lt;p>I&amp;rsquo;m going to describe in detail what microservices are, why the pattern is so appealing, and also some of the key challenges that they present.&lt;/p>
&lt;p>I&amp;rsquo;ll finish with a set of simple questions might be valuable to ask yourself when you are considering whether microservices are the right pattern &lt;em>for you&lt;/em>. The questions are at the end of the article.&lt;/p>
&lt;p>&lt;img src="images/letterbox.png" alt="Letterbox sample of diagram">&lt;/p>
&lt;h2 id="what-are-microservices-and-why-are-they-so-popular">What are microservices, and why are they so popular?&lt;/h2>
&lt;p>Let&amp;rsquo;s start with the basics. Here is how a hypothetical video sharing platform might be implemented, first in the form of a monolith (single large unit) and then in the form of microservices:&lt;/p>
&lt;p>&lt;img src="images/video-platform-monolith-microservices.png" alt="Diagram: Comparison of a Video Sharing Platform, Monolith vs Microservice">&lt;/p>
&lt;p>The difference between the two systems is that the first is a single large unit; a monolith. The second is a set of small, specific services. Each service has a specific role.&lt;/p>
&lt;p>When the diagram is drawn &lt;em>at this level of detail&lt;/em>, it is easy to see the appeal. There are a whole host of potential benefits:&lt;/p>
&lt;p>&lt;strong>Independent Development&lt;/strong>: Small, independent components can be built by small, independent teams. A group can work on a change to the &amp;lsquo;Upload&amp;rsquo; service without interfering with the &amp;lsquo;Transcode&amp;rsquo; service, or even knowing about it. The amount of time to learn about a component is greatly reduced, and it is easier to develop new features.&lt;/p>
&lt;p>&lt;strong>Independent Deployment&lt;/strong>: Each individual component can be deployed independently. This allows new features to be released with greater velocity and less risk. Fixes or features for the &amp;lsquo;Streaming&amp;rsquo; component can be deployed without requiring other components to be deployed.&lt;/p>
&lt;p>&lt;strong>Independent Scalability&lt;/strong>: Each component can be scaled independently of each other. During busy periods when new shows are released, the &amp;lsquo;Download&amp;rsquo; component can be scaled up to handle the increased load, without having to scale up every component, which makes elastic scaling more feasible and reduces costs.&lt;/p>
&lt;p>&lt;strong>Reusability&lt;/strong>: Components fulfil a small, specific function. This means that they can more easily be adapted for use in other systems, services or products. The &amp;lsquo;Transcode&amp;rsquo; component could be used by other business units, or even turned into a new business, perhaps offering transcoding services for other groups.&lt;/p>
&lt;p>At this level of detail, the benefits of a microservice model over a monolithic model seem obvious. So if that&amp;rsquo;s the case - why is this pattern only recently in vogue? Where has it been all my life?&lt;/p>
&lt;h2 id="if-this-is-so-great-why-hasnt-it-been-done-before">If this is so great, why hasn&amp;rsquo;t it been done before?&lt;/h2>
&lt;p>There are two answers to this question. One is that &lt;em>it has&lt;/em> - to the best of our technical capabilities, and the other is that more recent technical advances have allowed us to take it to a new level.&lt;/p>
&lt;p>When I started writing the answer to this question, it turned into a &lt;em>long&lt;/em> description, so I&amp;rsquo;m actually going to separate it into another article and publish it a little later&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. At this stage, I will skip the journey from single program to many programs, ignore ESBs and Service Orientated Architecture, component design and bounded contexts, and so on.&lt;/p>
&lt;p>Those who are interested can read more about the journey separately. Instead I&amp;rsquo;ll say that in many ways we&amp;rsquo;ve been doing this for a while, but with the recent explosion in popularity of container technology (Docker in particular) and in orchestration technology (such as Kubernetes, Mesos, Consul and so on) this pattern has become much more viable to implement from a technical standpoint.&lt;/p>
&lt;p>So if we take it as a given that we &lt;em>can&lt;/em> implement a microservice arrangement, we need to think carefully about the &lt;em>should&lt;/em>. We&amp;rsquo;ve seen the high-level theoretical benefits, but what about the challenges?&lt;/p>
&lt;h2 id="whats-the-problem-with-microservices">What&amp;rsquo;s the problem with microservices?&lt;/h2>
&lt;p>If microservices are so great, what&amp;rsquo;s the big deal? Here are some of the biggest issues I&amp;rsquo;ve seen.&lt;/p>
&lt;p>&lt;strong>Increased complexity for developers&lt;/strong>&lt;/p>
&lt;p>Things &lt;em>can&lt;/em> get a lot harder for developers. In the case where a developer wants to work on a &lt;em>journey&lt;/em>, or feature which might span many services, that developer has to run them all on their machine, or connect to them. This is often more complex than simply running a single program.&lt;/p>
&lt;p>This challenge can be partially mitigated with tooling&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>, but as the number of services which makes up a system increases, the more challenges developers will face when running the system as a whole.&lt;/p>
&lt;p>&lt;strong>Increased complexity for operators&lt;/strong>&lt;/p>
&lt;p>For teams who don&amp;rsquo;t develop services, but maintain them, there is an explosion in potential complexity. Instead of perhaps managing a few running services, they are managing dozens, hundreds or thousands of running services. There are more services, more communication paths, and more areas of potential failure.&lt;/p>
&lt;p>&lt;strong>Increased complexity for devops&lt;/strong>&lt;/p>
&lt;p>Reading the two points above, it may grate that operations and development are treated separately, especially given the popularity of devops as a practice (which I am a big proponent of). Doesn&amp;rsquo;t devops mitigate this?&lt;/p>
&lt;p>The challenge is that many organisations still run with separated development and operations teams - and a organisation that does is much more likely to struggle with adoption of microservices.&lt;/p>
&lt;p>For organisations which have adopted devops, it&amp;rsquo;s still hard. Being both a developer and an operator is already tough (but critical to build good software), but having to also understand the nuances of container orchestration systems, particularly systems which are evolving at a rapid pace, is very hard. Which brings me onto the next point.&lt;/p>
&lt;p>&lt;strong>It requires serious expertise&lt;/strong>&lt;/p>
&lt;p>When done by experts, the results can be wonderful. But imagine an organisation where perhaps things are not running smoothly with a single monolithic system. What possible reason would there be that things would be any better by increasing the number of systems, which increases the operational complexity?&lt;/p>
&lt;p>Yes, with effective automation, monitoring, orchestration and so on, this is all possible. But the challenge is rarely the technology - the challenge is finding people who can use it effectively. These skillsets are currently in very high demand, and may be difficult to find.&lt;/p>
&lt;p>&lt;strong>Real world systems often have poorly defined boundaries&lt;/strong>&lt;/p>
&lt;p>In all of the examples we used to describe the benefits of microservices, we spoke about &lt;em>independent&lt;/em> components. However in many cases components are simply not independent. On paper, certain domains may look bounded, but as you get into the muddy details, you may find that they are more challenging to model than you anticipated.&lt;/p>
&lt;p>This is where things can get &lt;em>extremely&lt;/em> complex. If your boundaries are actually not well defined, then what happens is that even though &lt;em>theoretically&lt;/em> services can be deployed in isolation, you find that due to the inter-dependencies between services, you have to deploy &lt;em>sets&lt;/em> of services as a group.&lt;/p>
&lt;p>This then means that you need to manage coherent versions of services which are proven and tested when working together, you don&amp;rsquo;t actually have an independently deployable system, because to deploy a new feature, you need to carefully orchestrate the simultaneous deployment of many services.&lt;/p>
&lt;p>&lt;strong>The complexities of state are often ignored&lt;/strong>&lt;/p>
&lt;p>In the previous example, I mentioned that a feature deployment may require the simultaneous rollout of many versions of many services in tandem. It is tempting to assume that sensible deployment techniques will mitigate this, for example blue/green deployments (which most service orchestration platforms handle with little effort), or multiple versions of a service being run in parallel, with consuming channels deciding which version to use.&lt;/p>
&lt;p>These techniques mitigate a large number of the challenges &lt;em>if the services are stateless&lt;/em>. But stateless services are quite frankly, easy to deal with. In fact, if you have stateless services, then I&amp;rsquo;d be inclined to consider skipping microservices altogether and consider using a serverless model.&lt;/p>
&lt;p>In reality, many services require state. An example from our video sharing platform might be the subscription service. A new version of the subscriptions service may store data in the subscriptions database in a different shape. If you are running both services in parallel, you are running the system with two schemas at once. If you do a blue green deployment, and other services depend on data in the new shape, then they must be updated &lt;em>at the same time&lt;/em>, and if the subscription service deployment fails and rolls back, they might need to roll back too, with cascading consequences.&lt;/p>
&lt;p>Again, it might be tempting to think that with NoSQL databases these issues of schema go away, but they don&amp;rsquo;t. Databases which don&amp;rsquo;t enforce schema do not lead to schemaless systems - they just mean that schema tends to be managed at the application level, rather than the database level. The fundamental challenge of understanding the shape of your data, and how it evolves, cannot be eliminated.&lt;/p>
&lt;p>&lt;strong>The complexitities of communication are often ignored&lt;/strong>&lt;/p>
&lt;p>As you build a large network of services which depend on each other, the liklihood is that there will be a lot of inter-service communication. This leads to a few challenges. Firstly, there are a lot more points at which things can fail. We must expect that network calls will fail, which means when one service calls another, it should expect to have to retry a number of times at the least. Now when a service has to potentially call many services, we end up in a complicated situation.&lt;/p>
&lt;p>Imagine a user uploads a video in the video sharing service. We might need to run the upload service, pass data to the transcode service, update subscriptions, update recommendations and so on. All of these calls require a degree of orchestration, if things fail we need to retry.&lt;/p>
&lt;p>This retry logic can get hard to manage. Trying to do things synchronously often ends up being untenable, there are too many points of failure. In this case, a more reliable solution is to use asynchronous patterns to handle communication. The challenge here is that asynchronous patterns inherently make a system stateful. As mentioned in the previous point, stateful systems and systems with distributed state are very hard to handle.&lt;/p>
&lt;p>When a microservice system uses message queues for intra-service communication, you essentially have a large database (the message queue or broker) glueing the services together. Again, although it might not seem like a challenge at first, schema will come back to bite you. A service at version X might write a message with a certain format, services which depend on this message will also need to be updated when the sending service changes the details of the message it sends.&lt;/p>
&lt;p>It is possible to have services which can handle messages in many different formats, but this is hard to manage. Now when deploying new versions of services, you will have times where two different versions of a service may be trying to process messages from the same queue, perhaps even messages sent by different versions of a sending service. This can lead to complicated edge cases. To avoid these edge cases, it may be easier to only allow certain versions of messages to exist, meaning that you need to deploy a set of versions of a set of services as a coherent whole, ensuring messages of older versions are drained appropriately first.&lt;/p>
&lt;p>This highlights again that the idea of independent deployments may not hold as expected when you get into the details.&lt;/p>
&lt;p>&lt;strong>Versioning can be hard&lt;/strong>&lt;/p>
&lt;p>To mitigate the challenges mentioned previously, versioning needs to be very carefully managed. Again, there can be a tendency to assume that following a standard such as semver[4] will solve the problem. It doesn&amp;rsquo;t. Semver is a sensible convention to use, but you will still have to track the versions of services and APIs which can work together.&lt;/p>
&lt;p>This can get very challenging very quickly, and may get to the point where you don&amp;rsquo;t know which versions of services will actually work properly together.&lt;/p>
&lt;p>Managing dependencies in software systems is notoriously hard, whether it is node modules, Java modules, C libraries or whatever. The challenges of &lt;em>conflicts between independent components&lt;/em> when consumed by a single entity are very hard to deal with.&lt;/p>
&lt;p>These challenges are hard to deal with when the dependencies are static, and can be patched, updated, edited and so on, but if the dependencies are themselves &lt;em>live services&lt;/em>, then you may not be able to just update them - you may have to run many versions (with the challenges already described) or bring down the system until it is fixed holistically.&lt;/p>
&lt;p>&lt;strong>Distributed Transactions&lt;/strong>&lt;/p>
&lt;p>In situations where you need transaction integrity across an operation, microservices can be very painful. Distributed state is hard to deal with, many small units which can fail make orchestrating transactions very hard.&lt;/p>
&lt;p>It may be tempting to attempt to avoid the problem by making operations idempotent, offering retry mechanisms and so on, and in many cases this might work. But you may have scenarios where you simply need a transaction to fail or succeed, and never be in an intermediate state. The effort involved in working around this or implementing it in a microservice model may be very high.&lt;/p>
&lt;p>&lt;strong>Microservices can be monoliths in disguise&lt;/strong>&lt;/p>
&lt;p>Yes, individual services and components &lt;em>may&lt;/em> be deployed in isolation, however in most cases you are going to have to be running some kind of orchestration platform, such as Kubernetes. If you are using a managed service, such as Google&amp;rsquo;s GKE&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> or Amazon&amp;rsquo;s EKS&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>, then a large amount of the complexity of managing the cluster is handled for you.&lt;/p>
&lt;p>However, if you are managing the cluster yourself, you are managing a large, complicated, mission critical system. Although the individual services may have all of the benefits described earlier, you need to very carefully manage your cluster. Deployments of this system can be hard, updates can be hard, failover can be hard and so on.&lt;/p>
&lt;p>In many cases the overall benefits are still there, but it is important not to trivialise or underestimate the additional complexity of managing another big, complex system. Managed services may help, but in many cases these services are nascent (Amazon EKS was only announced at the end of 2017 for example).&lt;/p>
&lt;p>&lt;strong>Networking Nightmares&lt;/strong>&lt;/p>
&lt;p>A more traditional model of services running on known hosts, with known addresses, has a fairly simple networking setup.&lt;/p>
&lt;p>However, when using microservices, generally there will be many services distributed across many nodes, which typically means there&amp;rsquo;s going to be a &lt;em>much&lt;/em> more complicated networking arrangement. There will be load balancing between services, DNS may be more heavily used, virtual networking layers, etc etc, to attempt to &amp;lsquo;hide&amp;rsquo; the complexity of this networking.&lt;/p>
&lt;p>However, as per &lt;a href="https://github.com/dwmkerr/hacker-laws/#the-law-of-conservation-of-complexity-teslers-law">Tesler&amp;rsquo;s Law&lt;/a> (or the Law of Conservation of Compexlity), this networking complexity is inherent - when you are finding real, runtime issues in larger scale clusters, it can often be at a very low networking level. These sorts of issues can be &lt;em>very&lt;/em> hard to diagnose. I have started tracking some examples at the end of the article, but I think that &lt;a href="https://medium.com/@tinder.engineering/tinders-move-to-kubernetes-cda2a6372f44">Tinder&amp;rsquo;s Migration to Kuberenetes&lt;/a> shows this challenge very well.&lt;/p>
&lt;p>Overall - the transition is still likely to be for the best, but doesn&amp;rsquo;t come without some serious challenges at the networking level, which will require some serious expertise to deal with!&lt;/p>
&lt;h2 id="the-death-of-microservice-madness">The Death of Microservice Madness!&lt;/h2>
&lt;p>Avoid the madness by making careful and considered decisions. To help out on this I&amp;rsquo;ve noted a few questions you might want to ask yourself, and what the answers might indicate:&lt;/p>
&lt;p>&lt;img src="images/questions.png" alt="Diagram: Questions to ask yourself when considering microservices">&lt;/p>
&lt;p>You can download a PDF copy here: &lt;a href="https://github.com/dwmkerr/blog/blob/master/articles/2018/microservice-madness/images/microservice-questions.pdf">microservice-questions.pdf&lt;/a>&lt;/p>
&lt;h2 id="final-thoughts-dont-confuse-microservices-with-architecture">Final Thoughts: Don&amp;rsquo;t Confuse Microservices with Architecture&lt;/h2>
&lt;p>I&amp;rsquo;ve deliberately avoided the &amp;lsquo;a&amp;rsquo; word in this article. But my friend &lt;a href="http://twitter.com/zoltanarvai">Zoltan&lt;/a> made a very good point when proofing this article (which he has contributed to).&lt;/p>
&lt;p>There is no microservice architecture. Microservices are just another pattern or implementation of components, nothing more, nothing less. Whether they are present in a system or not does not mean that the architecture of the system is solved.&lt;/p>
&lt;p>Microservices relate in many ways more to the technical processes around packaging and operations rather than the intrinsic design of the system. Appropriate boundaries for components continues to be one of the most important challenges in engineering systems.&lt;/p>
&lt;p>Regardless of the size of your services, whether they are in Docker containers or not, you will always need to think carefully about how to put a system together. There are no right answers, and there are a &lt;em>lot&lt;/em> of options.&lt;/p>
&lt;p>I hope you found this article interesting! As always, please do comment below if you have any questions or thoughts. You can also follow some lively discussions on:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.reddit.com/r/programming/comments/7pxriw/the_death_of_microservice_madness_in_2018/">Reddit - The Death of Microservice Madness&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://news.ycombinator.com/item?id=16200007">Hacker News - The Death of Microservice Madness&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix-further-reading">Appendix: Further Reading&lt;/h2>
&lt;p>The following links might be of interest:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://martinfowler.com/bliki/BoundedContext.html">Martin Fowler - Bounded Context&lt;/a> - Martin&amp;rsquo;s articles are great, I&amp;rsquo;d thoroughly recommend this.&lt;/li>
&lt;li>&lt;a href="https://martinfowler.com/articles/microservices.html">Martin Fowler - Microservices&lt;/a> - An often recommended introduction to the pattern.&lt;/li>
&lt;li>&lt;a href="https://r2m.se/microservices-good-or-bad/">Microservices - Good or Bad?&lt;/a> - BjÃ¶rn FrantzÃ©n&amp;rsquo;s thoughts on microservices, after reading this article.&lt;/li>
&lt;li>&lt;a href="http://blog.christianposta.com/microservices/when-not-to-do-microservices/">When Not To Do Microservices&lt;/a> - Excellent post on the topic from Christian Posta&lt;/li>
&lt;li>&lt;a href="http://www.iheavy.com/2017/03/13/30-questions-to-ask-a-serverless-fanboy/">Sean Hull - 30 questions to ask a serverless fanboy&lt;/a> - Interesting thoughts on the challenges of serverless, from a serverless fan!&lt;/li>
&lt;li>&lt;a href="https://youtu.be/NVb7aljfKYo?t=6657">Dave Kerr - Monoliths to Microservices - Practical tips for CI/CD and DevOps in the Microservice world&lt;/a> - A recent conference presentation I did on devops with microservices.&lt;/li>
&lt;li>&lt;a href="https://yermakov.net/microservices-without-fundamentals/">Alexander Yermakov - Microservices without fundamentals&lt;/a> - A response to this article, with Alex&amp;rsquo;s thoughts and counterpoints to the points raised here (see also &lt;a href="https://yermakov.net/microservices-as-a-self-sufficient-concept/">Microservices as a self sufficient concept&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Please do share anything else you think makes great reading or watching on the topic!&lt;/p>
&lt;hr>
&lt;h2 id="thanks">Thanks&lt;/h2>
&lt;p>Thanks JosÃ© from &lt;a href="https://www.campusmvp.es">campusmvp.es&lt;/a> for having the article translated in Spanish - &lt;a href="https://www.campusmvp.es/recursos/post/la-muerte-de-la-locura-de-los-microservicios-en-2018.aspx">La muerte de la locura de los microservicios en 2018&lt;/a>!&lt;/p>
&lt;h2 id="case-studies">Case Studies&lt;/h2>
&lt;p>Some interesting examples of experiences I am collecting of larger organisations who have made large scale transitions to microservices:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://medium.com/@tinder.engineering/tinders-move-to-kubernetes-cda2a6372f44">Tinder&amp;rsquo;s Move to Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="https://trends.google.com/trends/explore?date=today%205-y&amp;amp;q=microservice">https://trends.google.com/trends/explore?date=today%205-y&amp;amp;q=microservice&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>If you don&amp;rsquo;t want to miss the article, you can subscribe to the &lt;a href="http://www.dwmkerr.com/rss/">RSS Feed&lt;/a>, or follow me on &lt;a href="https://www.linkedin.com/in/dwmkerr/">LinkedIn&lt;/a> or &lt;a href="https://twitter.com/dwmkerr">Twitter&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Docker Compose is a good solution, &lt;a href="https://github.com/apparatus/fuge">Fuge&lt;/a> is very clever, and there is also the option of running orchestration locally as is the case with something like MiniKube.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Google Kubernetes Engine, a managed service from Google Cloud Platform for Kubernetes: &lt;a href="https://cloud.google.com/kubernetes-engine/">https://cloud.google.com/kubernetes-engine/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Amazon Elastic Container Services for Kubernetes, a managed service from Amazon Web Services for Kubernetes: &lt;a href="https://aws.amazon.com/eks/">https://aws.amazon.com/eks/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Get up and running with OpenShift on AWS</title><link>https://dwmkerr.com/get-up-and-running-with-openshift-on-aws/</link><pubDate>Thu, 02 Feb 2017 07:47:00 +0000</pubDate><guid>https://dwmkerr.com/get-up-and-running-with-openshift-on-aws/</guid><description>&lt;p>&lt;a href="https://www.openshift.com/">OpenShift&lt;/a> is Red Hat&amp;rsquo;s platform-as-a-service offering for hosting and scaling applications. It&amp;rsquo;s built on top of Google&amp;rsquo;s popular &lt;a href="https://kubernetes.io/">Kubernetes&lt;/a> system.&lt;/p>
&lt;p>Getting up and running with OpenShift Online is straightforward, as it is a cloud hosted solution. Setting up your own cluster is a little more complex, but in this article I&amp;rsquo;ll show you how to make it fairly painless.&lt;/p>
&lt;p>&lt;img src="images/welcome.png" alt="OpenShift Login">&lt;/p>
&lt;p>The repo for this project is at: &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">github.com/dwmkerr/terraform-aws-openshift&lt;/a>.&lt;/p>
&lt;h2 id="creating-the-infrastructure">Creating the Infrastructure&lt;/h2>
&lt;p>OpenShift has some fairly specific requirements about what hardware it runs on&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. There&amp;rsquo;s also DNS to set up, as well as internet access and so on.&lt;/p>
&lt;p>All in all, for a bare-bones setup, you&amp;rsquo;ll need something like this:&lt;/p>
&lt;p>&lt;img src="images/network-diagram-2.png" alt="Network Diagram">&lt;/p>
&lt;p>Which is (deep breath):&lt;/p>
&lt;ol>
&lt;li>A network&lt;/li>
&lt;li>A public subnet, with internet access via a gateway&lt;/li>
&lt;li>A master host, which will run the OpenShift master&lt;/li>
&lt;li>A pair of node hosts, which will run additional OpenShift nodes&lt;/li>
&lt;li>A hosted zone, which allows us to configure DNS&lt;/li>
&lt;li>A bastion, which allows us to SSH onto hosts, without directly exposing them&lt;/li>
&lt;li>Some kind of basic log aggregation, which I&amp;rsquo;m using CloudWatch for&lt;/li>
&lt;/ol>
&lt;p>This is not a production grade setup, which requires redundant masters and so on, but it provides the basics.&lt;/p>
&lt;p>Rather than setting this infrastructure up by hand, this is all scripted with &lt;a href="https://www.terraform.io/">Terraform&lt;/a>. To set up the infrastructure, clone the &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">github.com/dwmkerr/terraform-aws-openshift&lt;/a> repo:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git clone git@github.com:dwmkerr/terraform-aws-openshift
...
Resolving deltas: 100% (37/37), done.
&lt;/code>&lt;/pre>&lt;p>Then use the terraform CLI&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> to create the infrastructure:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ cd terraform-aws-openshift/
$ terraform get &amp;amp;&amp;amp; terraform apply
&lt;/code>&lt;/pre>&lt;p>You&amp;rsquo;ll be asked for a region, to deploy the network into, here I&amp;rsquo;m using &lt;code>us-west-1&lt;/code>:&lt;/p>
&lt;p>&lt;img src="images/Screenshot-at-Feb-02-21-16-44.png" alt="Enter Region">&lt;/p>
&lt;p>After a few minutes the infrastructure will be set up:&lt;/p>
&lt;p>&lt;img src="images/output.png" alt="Terraform complete">&lt;/p>
&lt;p>A quick glance at the AWS console shows the new hosts we&amp;rsquo;ve set up:&lt;/p>
&lt;p>&lt;img src="images/aws.png" alt="AWS Console">&lt;/p>
&lt;p>The next step is to install OpenShift.&lt;/p>
&lt;h2 id="installing-openshift">Installing OpenShift&lt;/h2>
&lt;p>There are a few different ways to install OpenShift, but the one we&amp;rsquo;ll use is called the &amp;lsquo;advanced installation&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&amp;rsquo;. This essentially involves:&lt;/p>
&lt;ol>
&lt;li>Creating an &amp;lsquo;inventory&amp;rsquo;, which specifies the hosts OpenShift will be installed on and the installation options&lt;/li>
&lt;li>Downloading the advanced installation code&lt;/li>
&lt;li>Running the advanced installation Ansible Playbook&lt;/li>
&lt;/ol>
&lt;p>To create the inventory, we just run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sed &lt;span style="color:#e6db74">&amp;#34;s/\${aws_instance.master.public_ip}/&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>terraform output master-public_ip&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">/&amp;#34;&lt;/span> inventory.template.cfg &amp;gt; inventory.cfg
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This takes our &amp;lsquo;inventory template&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&amp;rsquo; and populates it with the public IP of our master node, which is recorded in a Terraform output variable.&lt;/p>
&lt;p>We can then copy the inventory to the bastion:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ssh-add ~/.ssh/id_rsa
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>scp ./inventory.cfg ec2-user@&lt;span style="color:#66d9ef">$(&lt;/span>terraform output bastion-public_dns&lt;span style="color:#66d9ef">)&lt;/span>:~
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can again use the Terraform output variables, this time to get the bastion IP. Finally, we pipe our install script to the bastion host:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat install-from-bastion.sh | ssh -A ec2-user@&lt;span style="color:#66d9ef">$(&lt;/span>terraform output bastion-public_dns&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There&amp;rsquo;s a &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift/issues/1">bug&lt;/a> which means you might see &lt;code>ansible-playbook: command not found&lt;/code>, if so, just run the script again. The install script clones the installation scripts and runs them, using the inventory we&amp;rsquo;ve provided:&lt;/p>
&lt;p>&lt;img src="images/ansible.png" alt="Ansible Output">&lt;/p>
&lt;p>This&amp;rsquo;ll probably take about 10 minutes to run. And that&amp;rsquo;s it, OpenShift is installed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>open &lt;span style="color:#e6db74">&amp;#34;https://&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>terraform output master-public_dns&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">:8443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hit &amp;lsquo;advanced&amp;rsquo; and continue, as we&amp;rsquo;re using a self-signed certificate most browsers will complain:&lt;/p>
&lt;p>&lt;img src="images/console1.png" alt="Invalid Certificate">&lt;/p>
&lt;p>Enter any username and password (the system is configured to allow anyone to access it by default) and you&amp;rsquo;ll be presented with the OpenShift console:&lt;/p>
&lt;p>&lt;img src="images/console2.png" alt="OpenShift console">&lt;/p>
&lt;p>As the setup requires three t2.large instances, which are not available on the free plan, you might want to clean up when you are done with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>terraform destroy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="wrapping-up">Wrapping Up&lt;/h2>
&lt;p>Hopefully you&amp;rsquo;ve found this useful, there are more details and references on the README of the github repo:&lt;/p>
&lt;p>&lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">https://github.com/dwmkerr/terraform-aws-openshift&lt;/a>&lt;/p>
&lt;p>Comments and feedback are always welcome!&lt;/p>
&lt;hr>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>See &lt;a href="https://docs.openshift.org/latest/install_config/install/prerequisites.html#system-requirements">https://docs.openshift.org/latest/install_config/install/prerequisites.html#system-requirements&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Use &amp;lsquo;brew install terraform&amp;rsquo;, full instructions in the &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift">README.md&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>See &lt;a href="https://docs.openshift.org/latest/install_config/install/advanced_install.html">https://docs.openshift.org/latest/install_config/install/advanced_install.html&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>See &lt;a href="https://github.com/dwmkerr/terraform-aws-openshift/blob/master/inventory.template.cfg">https://github.com/dwmkerr/terraform-aws-openshift/blob/master/inventory.template.cfg&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Creating a Resilient Consul Cluster for Docker Microservice Discovery with Terraform and AWS</title><link>https://dwmkerr.com/creating-a-resilient-consul-cluster-for-docker-microservice-discovery-with-terraform-and-aws/</link><pubDate>Mon, 09 Jan 2017 07:10:40 +0000</pubDate><guid>https://dwmkerr.com/creating-a-resilient-consul-cluster-for-docker-microservice-discovery-with-terraform-and-aws/</guid><description>&lt;p>In this article I&amp;rsquo;m going to show you how to create a resilient Consul cluster, using Terraform and AWS. We can use this cluster for microservice discovery and management. No prior knowledge of the technologies or patterns is required!&lt;/p>
&lt;p>The final code is at &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster">github.com/dwmkerr/terraform-consul-cluster&lt;/a>. Note that it has evolved somewhat since the time of writing, see the Appendices at the end of the article for details.&lt;/p>
&lt;h2 id="consul-terraform--aws">Consul, Terraform &amp;amp; AWS&lt;/h2>
&lt;p>&lt;a href="https://www.consul.io/">Consul&lt;/a> is a technology which enables &lt;em>Service Discovery&lt;/em>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, a pattern which allows services to locate each other via a central authority.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/">Terraform&lt;/a> is a technology which allows us to script the provisioning of infrastructure and systems. This allows us to practice the &lt;em>Infrastructure as Code&lt;/em> pattern. The rigour of code control (versioning, history, user access control, diffs, pull requests etc) can be applied to our systems.&lt;/p>
&lt;p>And why &lt;a href="https://aws.amazon.com/">AWS&lt;/a>? We need to create many servers and build a network to see this system in action. We can simulate parts of this locally with tools such as &lt;a href="https://www.vagrantup.com/">Vagrant&lt;/a>, but we can use the arguably most popular&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> IaaS platfom for this job at essentially zero cost, and learn some valuable skills which are readily applicable to other projects at the same time.&lt;/p>
&lt;p>A lot of what we will learn is not really AWS specific - and the Infrastructure as Code pattern which Terraform helps us apply allows us to apply these techniques easily with other providers.&lt;/p>
&lt;h2 id="the-goal">The Goal&lt;/h2>
&lt;p>The goal is to create a system like this:&lt;/p>
&lt;p>&lt;img src="images/img-0-goal.png" alt="Overall System Diagram">&lt;/p>
&lt;p>In a nutshell:&lt;/p>
&lt;ul>
&lt;li>We have a set of homogenous Consul nodes&lt;/li>
&lt;li>The nodes form a cluster and automatically elect a leader&lt;/li>
&lt;li>The nodes span more than one availability zone, meaning the system is redundant and can survive the failure of an entire availability zone (i.e. data centre)&lt;/li>
&lt;li>The Consul UI is available to view via a gateway&lt;/li>
&lt;li>We have two example microservices which register themselves on the cluster, so we can actually see some registered services in the console&lt;/li>
&lt;/ul>
&lt;p>As a quick caveat, in reality this setup would typically live in a private subnet, not directly accessible to the outside work except via public facing load balancers. This adds a bit more complexity to the Terraform setup but not much value to the walk-though. A network diagram of how it might look is below, I invite interested readers to try and move to this model as a great exercise to cement the concepts!&lt;/p>
&lt;h2 id="step-1---creating-our-network">Step 1 - Creating our Network&lt;/h2>
&lt;p>The first logical step is to create the network itself. This means:&lt;/p>
&lt;ul>
&lt;li>The network (in AWS terminology, this is a &lt;em>VPC&lt;/em> or &lt;em>Virtual Private Cloud&lt;/em>)&lt;/li>
&lt;li>The &amp;lsquo;public&amp;rsquo; subnet, which defines our IP ranges for hosts&lt;/li>
&lt;li>The internet gateway, which provides an entry/exit point for traffic from/to the internet&lt;/li>
&lt;li>The firewall rules, which define what traffic can come in and out of the network&lt;/li>
&lt;/ul>
&lt;p>All together, that&amp;rsquo;s this:&lt;/p>
&lt;p>&lt;img src="images/img-1-network.png" alt="">&lt;/p>
&lt;p>Our solution will be made more resilient by ensuring we host our Consul nodes across multiple &lt;em>availability zones&lt;/em>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;p>Creating a VPC and building a subnet is fairly trivial if you have done some network setup before or spent much time working with AWS, if not, you may be a little lost already. There&amp;rsquo;s a good course on Udemy&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> which will take you through the process of setting up a VPC which I recommend if you are interested in this, as it is quite hands on. It&amp;rsquo;ll also show you how to build a more &amp;lsquo;realistic&amp;rsquo; network, which also contains a private subnet and NAT, but that&amp;rsquo;s beyond the scope of this write-up. Instead, I&amp;rsquo;ll take you through the big parts.&lt;/p>
&lt;h3 id="the-network">The Network&lt;/h3>
&lt;p>We&amp;rsquo;re using AWS, we need to create a VPC. A VPC is a Virtual Private Cloud. The key thing is that it is &lt;em>isolated&lt;/em>. Things you create in this network will be able to talk to each other if you let them, but cannot communicate with the outside world, unless you specifically create the parts needed for them to do so.&lt;/p>
&lt;p>A private network is probably something you regularly use if you work in a company&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Most companies have their own internal network - when you use a computer on that network it can talk to other company computers (such as the company mail server). When you are off that network, you might not be able to access your company email (unless it is publicly available, like gmail, or over a VPN [and by accessing a VPN, you are actually &lt;em>joining&lt;/em> the network again, albeit remotely]).&lt;/p>
&lt;p>Perhaps the most immediately obvious part of a VPC is that &lt;em>you control the IP addresses&lt;/em>. You specify the &lt;em>range&lt;/em> of IP addresses which are available to give to machines on the network. When a machine joins, it is given an IP in that range. I&amp;rsquo;m not going to go into too much detail here, if you are interested let me know and I&amp;rsquo;ll write up an article on VPCs in detail!&lt;/p>
&lt;p>&lt;img src="images/img-3-vpc.png" alt="">&lt;/p>
&lt;p>Here&amp;rsquo;s how I&amp;rsquo;d suggest scripting AWS infrastructure with Terraform if you haven&amp;rsquo;t done this before.&lt;/p>
&lt;ol>
&lt;li>Use the AWS console to create what you want&lt;/li>
&lt;li>Search the Terraform documentation for the entity you want to create (e.g. &lt;a href="https://www.terraform.io/docs/providers/aws/r/vpc.html">VPC&lt;/a>), &lt;em>script&lt;/em> the component and &lt;em>apply&lt;/em> the provisioning&lt;/li>
&lt;li>Compare the hand-made VPC to the script-made VPC, if the two are the same, you are done&lt;/li>
&lt;li>If the two are different, check the documentation and try again&lt;/li>
&lt;/ol>
&lt;p>Ensure you have an AWS account, and note your Secret Key and Access Key. We&amp;rsquo;ll need these to remotely control it. Here&amp;rsquo;s the terraform script to create a VPC:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Setup the core provider information.
provider &amp;#34;aws&amp;#34; {
access_key = &amp;#34;${var.access_key}&amp;#34;
secret_key = &amp;#34;${var.secret_key}&amp;#34;
region = &amp;#34;${var.region}&amp;#34;
}
// Define the VPC.
resource &amp;#34;aws_vpc&amp;#34; &amp;#34;consul-cluster&amp;#34; {
cidr_block = &amp;#34;10.0.0.0/16&amp;#34; // i.e. 10.0.0.0 to 10.0.255.255
enable_dns_hostnames = true
tags {
Name = &amp;#34;Consul Cluster VPC&amp;#34;
Project = &amp;#34;consul-cluster&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>This script uses &lt;a href="https://www.terraform.io/docs/configuration/variables.html">Terraform Variables&lt;/a>, such as &lt;code>var.access_key&lt;/code>, which we keep in a &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/blob/master/variables.tf">variables.tf&lt;/a> file. Terraform will use the default values defined in the file if they are present, or ask the user to supply them. Let&amp;rsquo;s build the network:&lt;/p>
&lt;pre tabindex="0">&lt;code>terraform apply
&lt;/code>&lt;/pre>&lt;p>After supplying the values for the variables, Terraform will provision the network, using the AWS SDK internally.&lt;/p>
&lt;p>&lt;img src="images/img-2-terraform-apply.png" alt="">&lt;/p>
&lt;p>You&amp;rsquo;ll see lots of info about what it is creating, then a success message.&lt;/p>
&lt;h3 id="the-public-subnet">The Public Subnet&lt;/h3>
&lt;p>You don&amp;rsquo;t put hosts directly into a VPC, they need to go into a structure called a &amp;lsquo;subnet&amp;rsquo;, which is a &lt;em>part&lt;/em> of a VPC. Subnets get their own subset of the VPC&amp;rsquo;s available IP addresses, which you specify.&lt;/p>
&lt;p>Subnets are used to build &lt;em>zones&lt;/em> in a network. Why would you need this? Typically it is to manage security. You might have a &amp;lsquo;public zone&amp;rsquo; in which all hosts can be accessed from the internet, and a &amp;lsquo;private zone&amp;rsquo; which is inaccessible directly (and therefore a better location for hosts with sensitive data). You might have an &amp;lsquo;operator&amp;rsquo; zone, which only sysadmins can access, but they can use to get diagnostic information.&lt;/p>
&lt;p>Here&amp;rsquo;s a common subnet layout for multi-tiered applications:&lt;/p>
&lt;p>&lt;img src="images/img-4-subnets.png" alt="">&lt;/p>
&lt;p>The defining characteristics of zones is that they are used to create &lt;em>boundaries&lt;/em> to isolate hosts. These boundaries are normally secured by firewalls, traversed via gateways or NATs etc. We&amp;rsquo;re going to create two public subnets, one in each of the availability zones&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Create a public subnet for each AZ.
resource &amp;#34;aws_subnet&amp;#34; &amp;#34;public-a&amp;#34; {
vpc_id = &amp;#34;${aws_vpc.consul-cluster.id}&amp;#34;
cidr_block = &amp;#34;10.0.1.0/24&amp;#34; // i.e. 10.0.1.0 to 10.0.1.255
availability_zone = &amp;#34;ap-southeast-1a&amp;#34;
map_public_ip_on_launch = true
}
resource &amp;#34;aws_subnet&amp;#34; &amp;#34;public-b&amp;#34; {
vpc_id = &amp;#34;${aws_vpc.consul-cluster.id}&amp;#34;
cidr_block = &amp;#34;10.0.2.0/24&amp;#34; // i.e. 10.0.2.0 to 10.0.1.255
availability_zone = &amp;#34;ap-southeast-1b&amp;#34;
map_public_ip_on_launch = true
}
&lt;/code>&lt;/pre>&lt;p>With Terraform, resources can depend on each other. In this case, the subnets need to reference the ID of the VPC we want to place them in (so we use &lt;code>aws_vpc.consul-cluster.id&lt;/code>).&lt;/p>
&lt;h3 id="the-internet-gateway-route-tables-and-security-groups">The Internet Gateway, Route Tables and Security Groups&lt;/h3>
&lt;p>The final parts of the network you can see in the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/blob/master/network.tf">./infrastructure/network.tf&lt;/a> script. These are the Internet Gateway, Route Table and Security Group resources. Essentially they are for controlling access between hosts and the internet. AWS have a &lt;a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html">good guide&lt;/a> if you are not familiar with these resources; they don&amp;rsquo;t add much to the article so I&amp;rsquo;ll leave you to explore on your own.&lt;/p>
&lt;p>That&amp;rsquo;s it for the network, we now have the following structure:&lt;/p>
&lt;p>&lt;img src="images/img-1-network-1.png" alt="">&lt;/p>
&lt;p>If you want to see the code as it stands now, check the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/tree/step-1">Step 1&lt;/a> branch. Now we need to look at creating the hosts to install Consul on.&lt;/p>
&lt;h2 id="step-2---creating-the-consul-hosts">Step 2 - Creating the Consul Hosts&lt;/h2>
&lt;p>The Consul documentation recommends running in a cluster or 3 or 5 nodes&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. We want to set up a system which is self-healing - if we lose a node, we want to create a new one.&lt;/p>
&lt;p>Enter &lt;a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroup.html">Auto-Scaling Groups&lt;/a>. Auto-scaling groups allow us to define a template for an instance, and ask AWS to make sure there are always a certain number of these instances. If we lose an instance, a new one will be created to keep the group at the correct size&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>So we now need to create:&lt;/p>
&lt;ol>
&lt;li>A &amp;lsquo;Launch Configuration&amp;rsquo; which determines what instances our Auto-scaling Group creates&lt;/li>
&lt;li>A &amp;lsquo;user data script&amp;rsquo; which runs on newly created instances, which must install and start Consul&lt;/li>
&lt;li>An Auto-scaling group, configured to run five instances across the two public subnets&lt;/li>
&lt;li>A load balancer, configured to pass incoming requests for the Consul Admin console to the nodes&lt;/li>
&lt;/ol>
&lt;p>Or visually:&lt;/p>
&lt;p>&lt;img src="images/img-5-cluster-basic-2.png" alt="Basic Cluster Diagram">&lt;/p>
&lt;p>Let&amp;rsquo;s get to it.&lt;/p>
&lt;h3 id="the-launch-configuration--auto-scaling-group">The Launch Configuration &amp;amp; Auto-scaling Group&lt;/h3>
&lt;p>The Launch Configuration will define the characteristics of our instances and the auto-scaling group determines the size of our cluster:&lt;/p>
&lt;pre tabindex="0">&lt;code>// Launch configuration for the consul cluster auto-scaling group.
resource &amp;#34;aws_launch_configuration&amp;#34; &amp;#34;consul-cluster-lc&amp;#34; {
name_prefix = &amp;#34;consul-node-&amp;#34;
image_id = &amp;#34;${lookup(var.ami_ecs_optimised, var.region)}&amp;#34;
instance_type = &amp;#34;t2.micro&amp;#34;
security_groups = [&amp;#34;${aws_security_group.consul-cluster-vpc.id}&amp;#34;]
lifecycle {
create_before_destroy = true
}
}
// Auto-scaling group for our cluster.
resource &amp;#34;aws_autoscaling_group&amp;#34; &amp;#34;consul-cluster-asg&amp;#34; {
name = &amp;#34;consul-asg&amp;#34;
launch_configuration = &amp;#34;${aws_launch_configuration.consul-cluster-lc.name}&amp;#34;
min_size = 5
max_size = 5
vpc_zone_identifier = [
&amp;#34;${aws_subnet.public-a.id}&amp;#34;,
&amp;#34;${aws_subnet.public-b.id}&amp;#34;
]
lifecycle {
create_before_destroy = true
}
}
&lt;/code>&lt;/pre>&lt;p>A few key things to note:&lt;/p>
&lt;ol>
&lt;li>I have omitted the &lt;code>tag&lt;/code> properties in the scripts for brevity&lt;/li>
&lt;li>The &amp;lsquo;image&amp;rsquo; for the launch configuration is looked up based on the region we&amp;rsquo;ve specified - we&amp;rsquo;re a basic linux image&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/li>
&lt;li>We are using micro instances, which are free-tier eligible&lt;/li>
&lt;li>The auto-scaling group spans both availability zones.&lt;/li>
&lt;/ol>
&lt;p>Once we run &lt;code>terraform apply&lt;/code>, we&amp;rsquo;ll see our auto-scaling group, which references the new launch configuration and works over multiple availability zones:&lt;/p>
&lt;p>&lt;img src="images/img-6-lc-asg.png" alt="Auto scaling group and launch configuration">&lt;/p>
&lt;p>We can also see the new instances:&lt;/p>
&lt;p>&lt;img src="images/img-7-instances.png" alt="Instances">&lt;/p>
&lt;p>These instances don&amp;rsquo;t do much yet though, we&amp;rsquo;ve not installed Docker or Consul.&lt;/p>
&lt;h3 id="installing-consul-and-accessing-the-admin-interface">Installing Consul and Accessing the Admin Interface&lt;/h3>
&lt;p>To set up our instances we use a &amp;lsquo;userdata&amp;rsquo; script&amp;rsquo; A userdata runs once when an instance is created. We can create a script in our repository, and reference it in our Terraform files.&lt;/p>
&lt;p>We add a new file called &lt;code>consul-node.sh&lt;/code> to a &lt;code>files&lt;/code> folder. This script installs Docker and runs Consul:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>yum install -y docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>usermod -a -G docker ec2-user
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service docker start
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Get my IP address.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>curl http://169.254.169.254/latest/meta-data/local-ipv4&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Instance IP is: &lt;/span>$IP&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Start the Consul server.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run -d --net&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name&lt;span style="color:#f92672">=&lt;/span>consul &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> consul agent -server -ui &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$IP&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -client&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;0.0.0.0&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bootstrap-expect&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here&amp;rsquo;s a breakdown of what we&amp;rsquo;re doing:&lt;/p>
&lt;ol>
&lt;li>Install Docker. These scripts run as root, so we add the ec2-user to the Docker group, meaning when we log in later on via SSH, we can run Docker&lt;/li>
&lt;li>Get our IP address. AWS provide a magic address (169.254.169.254) which lets you query data about your instance, see &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">Instance Metadata &amp;amp; User Metadata&lt;/a>&lt;/li>
&lt;li>Run the Consul docker image in server mode, with the UI enabled, expecting only one instance&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>The actual scripts contains more!&lt;/strong> Getting userdata scripts right, testing and debugging them is tricky. See how I do it in detail in &lt;a href="#Appendix-1-Logging">Appendix 1: Logging&lt;/a>.&lt;/p>
&lt;p>Now we need to tell Terraform to include this script as part of the instance metadata. Here&amp;rsquo;s how we do that:&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_launch_configuration&amp;#34; &amp;#34;consul-cluster-lc&amp;#34; {
/// ...add the line below....
user_data = &amp;#34;${file(&amp;#34;files/consul-node.sh&amp;#34;)}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>When Consul is running with the &lt;code>-ui&lt;/code> option, it provides an admin UI. You can try it by running Consul locally with &lt;code>docker run -p8500:8500 consul&lt;/code> and navigating to http://localhost:8500/ui.&lt;/p>
&lt;p>We can install a load balancer in front of our auto-scaling group, to automatically forward incoming traffic to a host. Here&amp;rsquo;s the config:&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_elb&amp;#34; &amp;#34;consul-lb&amp;#34; {
name = &amp;#34;consul-lb-a&amp;#34;
security_groups = [
&amp;#34;${aws_security_group.consul-cluster-vpc.id}&amp;#34;,
&amp;#34;${aws_security_group.web.id}&amp;#34;
]
subnets = [
&amp;#34;${aws_subnet.public-a.id}&amp;#34;,
&amp;#34;${aws_subnet.public-b.id}&amp;#34;
]
listener {
instance_port = 8500
instance_protocol = &amp;#34;http&amp;#34;
lb_port = 80
lb_protocol = &amp;#34;http&amp;#34;
}
health_check {
healthy_threshold = 2
unhealthy_threshold = 2
timeout = 3
target = &amp;#34;HTTP:8500/ui/&amp;#34;
interval = 30
}
}
&lt;/code>&lt;/pre>&lt;p>Blow-by-blow:&lt;/p>
&lt;ol>
&lt;li>Create a load balancer, with the same security groups as the rest of the VPC, but also a security group which allows web access&lt;/li>
&lt;li>Point to two subnets first subnet&lt;/li>
&lt;li>Forward HTTP 8500 traffic&lt;/li>
&lt;li>Configure a healthcheck&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>The final change we make is to add an &lt;code>outputs.tf&lt;/code> file, which lists all of the properties Terraform knows about which we want to save. All it includes is:&lt;/p>
&lt;pre tabindex="0">&lt;code>output &amp;#34;consul-dns&amp;#34; {
value = &amp;#34;${aws_elb.consul-lb.dns_name}&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>When we finally run &lt;code>terraform apply&lt;/code>, we see the public DNS of our load balancer:&lt;/p>
&lt;p>&lt;img src="images/img-8-cluster-dns.png" alt="Screenshot showing &amp;rsquo;terraform apply&amp;rsquo; output, indicating our newly generated ELB&amp;rsquo;s public DNS">&lt;/p>
&lt;p>And running in a browser on port 8500 we see the Consul admin interface:&lt;/p>
&lt;p>&lt;img src="images/img-9-admin-ui.png" alt="Screenshot showing the Consul admin interface">&lt;/p>
&lt;p>Every time we refresh we will likely see a different node. We&amp;rsquo;ve actually created five clusters each of one node - what we now need to do is connect them all together into a single cluster of five nodes.&lt;/p>
&lt;p>If you want to see the code as it stands now, check the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/tree/step-2">Step 2&lt;/a> branch.&lt;/p>
&lt;h2 id="step-3---creating-the-cluster">Step 3 - Creating the Cluster&lt;/h2>
&lt;p>Creating the cluster is now not too much of a challenge. We will update the userdata script to tell the consul process we are expecting 5 nodes (via the &lt;a href="https://www.consul.io/docs/agent/options.html#_bootstrap_expect">&lt;code>bootstrap-expect&lt;/code>&lt;/a> flag.&lt;/p>
&lt;p>Here&amp;rsquo;s the updated script:&lt;/p>
&lt;pre tabindex="0">&lt;code># Get my IP address.
IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)
echo &amp;#34;Instance IP is: $IP&amp;#34;
# Start the Consul server.
docker run -d --net=host \
--name=consul \
consul agent -server -ui \
-bind=&amp;#34;$IP&amp;#34; \
-client=&amp;#34;0.0.0.0&amp;#34; \
-bootstrap-expect=&amp;#34;5&amp;#34;
&lt;/code>&lt;/pre>&lt;p>The problem is &lt;strong>this won&amp;rsquo;t work&lt;/strong>&amp;hellip; We need to tell each node the address of &lt;em>another&lt;/em> server in the cluster. For example, if we start five nodes, we should tell nodes 2-5 the address of node 1, so that the nodes can discover each other.&lt;/p>
&lt;p>The challenge is how do we get the IP of node 1? The IP addresses are determined by the network, we don&amp;rsquo;t preset them so cannot hard code them. Also, we can expect nodes to occasionally die and get recreated, so the IP addresses of nodes will in fact change over time.&lt;/p>
&lt;h3 id="getting-the-ip-addresses-of-nodes-in-the-cluster">Getting the IP addresses of nodes in the cluster&lt;/h3>
&lt;p>There&amp;rsquo;s a nice trick we can use here. We can ask AWS to give us the IP addresses of each host in the auto-scaling group. If we tell each node the addresses of the &lt;em>other nodes&lt;/em>, then they will elect a leader themselves&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="images/img-12-choose-leader-1.png" alt="Diagram showing how we decide on a leader IP">&lt;/p>
&lt;p>There are a couple of things we need to do to get this right. First, update the userdata script to provide the IPs of other nodes when we&amp;rsquo;re starting up, then update the &lt;strong>role&lt;/strong> of our nodes so that they have permissions to use the APIs we&amp;rsquo;re going to call.&lt;/p>
&lt;h3 id="getting-the-cluster-ips">Getting the Cluster IPs&lt;/h3>
&lt;p>This is actually fairly straightforward. We update our userdata script to the below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># A few variables we will refer to later...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ASG_NAME&lt;span style="color:#f92672">=&lt;/span>consul-asg
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>REGION&lt;span style="color:#f92672">=&lt;/span>ap-southeast-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EXPECTED_SIZE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Return the id of each instance in the cluster.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">function&lt;/span> cluster-instance-ids &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Grab every line which contains &amp;#39;InstanceId&amp;#39;, cut on double quotes and grab the ID:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># &amp;#34;InstanceId&amp;#34;: &amp;#34;i-example123&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">#....^..........^..^.....#4.....^...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> aws --region&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$REGION&lt;span style="color:#e6db74">&amp;#34;&lt;/span> autoscaling describe-auto-scaling-groups &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --auto-scaling-group-name $ASG_NAME &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | grep InstanceId &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | cut -d &lt;span style="color:#e6db74">&amp;#39;&amp;#34;&amp;#39;&lt;/span> -f4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Return the private IP of each instance in the cluster.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">function&lt;/span> cluster-ips &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> id in &lt;span style="color:#66d9ef">$(&lt;/span>cluster-instance-ids&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> aws --region&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$REGION&lt;span style="color:#e6db74">&amp;#34;&lt;/span> ec2 describe-instances &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --query&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Reservations[].Instances[].[PrivateIpAddress]&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --output&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --instance-ids&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$id&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Wait until we have as many cluster instances as we are expecting.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">while&lt;/span> COUNT&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cluster-instance-ids | wc -l&lt;span style="color:#66d9ef">)&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#f92672">[&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$COUNT&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -lt &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$EXPECTED_SIZE&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> echo &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$COUNT&lt;span style="color:#e6db74"> instances in the cluster, waiting for &lt;/span>$EXPECTED_SIZE&lt;span style="color:#e6db74"> instances to warm up...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sleep &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Get my IP address, all IPs in the cluster, then just the &amp;#39;other&amp;#39; IPs...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>curl http://169.254.169.254/latest/meta-data/local-ipv4&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mapfile -t ALL_IPS &amp;lt; &amp;lt;&lt;span style="color:#f92672">(&lt;/span>cluster-ips&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>OTHER_IPS&lt;span style="color:#f92672">=(&lt;/span> &lt;span style="color:#e6db74">${&lt;/span>ALL_IPS[@]/&lt;span style="color:#e6db74">${&lt;/span>IP&lt;span style="color:#e6db74">}}&lt;/span>/&lt;span style="color:#f92672">}&lt;/span> &lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Instance IP is: &lt;/span>$IP&lt;span style="color:#e6db74">, Cluster IPs are: &lt;/span>&lt;span style="color:#e6db74">${&lt;/span>CLUSTER_IPS[@]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">, Other IPs are: &lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[@]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Start the Consul server.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run -d --net&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name&lt;span style="color:#f92672">=&lt;/span>consul &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> consul agent -server -ui &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$IP&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[0]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[1]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[2]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -retry-join&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>OTHER_IPS[3]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -bootstrap-expect&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$EXPECTED_SIZE&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Right, here&amp;rsquo;s what&amp;rsquo;s going on:&lt;/p>
&lt;ol>
&lt;li>We create a few variables we&amp;rsquo;ll use repeatedly&lt;/li>
&lt;li>We create a &lt;code>cluster-instance-ids&lt;/code> function which returns the ID of each instance in the auto-scaling group&lt;/li>
&lt;li>We create a &lt;code>cluster-ips&lt;/code> function which returns the private IP address of each instance in the cluster.&lt;/li>
&lt;li>We wait until the auto-scaling group has our expected number of instances (it can take a while for them all to be created)&lt;/li>
&lt;li>We get the 5 IP addresses&lt;/li>
&lt;li>We remove our IP from the array, leaving us with the IPs of the &lt;em>other&lt;/em> nodes&lt;/li>
&lt;li>We start the Consul agent in server mode, expecting 5 nodes and offering the IP of each other agent&lt;/li>
&lt;/ol>
&lt;p>The problem is, if we try to run the script we will fail, because calling the AWS APIs requires some permissions we don&amp;rsquo;t have. Let&amp;rsquo;s fix that.&lt;/p>
&lt;h3 id="creating-a-role-for-our-nodes">Creating a Role for our nodes&lt;/h3>
&lt;p>Our nodes now have a few special requirements. They need to be able to query the details of an auto-scaling group and get the IP of an instance&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We will need to create a policy which describes the permissions we need, create a role, attach the policy to the role and then ensure our instances are assigned the correct role. This is &lt;code>consul-node-role.tf&lt;/code> file:&lt;/p>
&lt;pre tabindex="0">&lt;code>// This policy allows an instance to discover a consul cluster leader.
resource &amp;#34;aws_iam_policy&amp;#34; &amp;#34;leader-discovery&amp;#34; {
name = &amp;#34;consul-node-leader-discovery&amp;#34;
path = &amp;#34;/&amp;#34;
policy = &amp;lt;&amp;lt;EOF
{
&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,
&amp;#34;Statement&amp;#34;: [
{
&amp;#34;Sid&amp;#34;: &amp;#34;Stmt1468377974000&amp;#34;,
&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,
&amp;#34;Action&amp;#34;: [
&amp;#34;autoscaling:DescribeAutoScalingInstances&amp;#34;,
&amp;#34;autoscaling:DescribeAutoScalingGroups&amp;#34;,
&amp;#34;ec2:DescribeInstances&amp;#34;
],
&amp;#34;Resource&amp;#34;: [
&amp;#34;*&amp;#34;
]
}
]
}
EOF
}
// Create a role which consul instances will assume.
// This role has a policy saying it can be assumed by ec2
// instances.
resource &amp;#34;aws_iam_role&amp;#34; &amp;#34;consul-instance-role&amp;#34; {
name = &amp;#34;consul-instance-role&amp;#34;
assume_role_policy = &amp;lt;&amp;lt;EOF
{
&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,
&amp;#34;Statement&amp;#34;: [
{
&amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34;,
&amp;#34;Principal&amp;#34;: {
&amp;#34;Service&amp;#34;: &amp;#34;ec2.amazonaws.com&amp;#34;
},
&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,
&amp;#34;Sid&amp;#34;: &amp;#34;&amp;#34;
}
]
}
EOF
}
// Attach the policy to the role.
resource &amp;#34;aws_iam_policy_attachment&amp;#34; &amp;#34;consul-instance-leader-discovery&amp;#34; {
name = &amp;#34;consul-instance-leader-discovery&amp;#34;
roles = [&amp;#34;${aws_iam_role.consul-instance-role.name}&amp;#34;]
policy_arn = &amp;#34;${aws_iam_policy.leader-discovery.arn}&amp;#34;
}
// Create a instance profile for the role.
resource &amp;#34;aws_iam_instance_profile&amp;#34; &amp;#34;consul-instance-profile&amp;#34; {
name = &amp;#34;consul-instance-profile&amp;#34;
roles = [&amp;#34;${aws_iam_role.consul-instance-role.name}&amp;#34;]
}
&lt;/code>&lt;/pre>&lt;p>Terraform is a little verbose here! Finally, we update our launch configuration to ensure that the instances assume this role.&lt;/p>
&lt;pre tabindex="0">&lt;code>resource &amp;#34;aws_launch_configuration&amp;#34; &amp;#34;consul-cluster-lc&amp;#34; {
// Add this line!!
iam_instance_profile = &amp;#34;${aws_iam_instance_profile.consul-instance-profile.id}&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s create the cluster again, with &lt;code>terraform apply&lt;/code>. When we log into the UI we should now see a cluster containing all five nodes:&lt;/p>
&lt;p>&lt;img src="images/img-13-cluster.png" alt="Screenshot of the Consul UI, showing that the Consul server is running on five nodes in the Datacenter">&lt;/p>
&lt;p>This code is all in the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/tree/step-3">Step 3&lt;/a> branch.&lt;/p>
&lt;p>If you are familiar with Consul, this may be all you need. If not, you might be interested in seeing how we actually create a new instance to host a service, register it with Consul and query its address.&lt;/p>
&lt;h2 id="step-4---adding-a-microservice">Step 4 - Adding a Microservice&lt;/h2>
&lt;p>I&amp;rsquo;ve created a docker image for as simple a microservice as you can get. It returns a quote from Futurama&amp;rsquo;s Zapp Brannigan. The image is tagged as &lt;code>dwmkerr/zapp-service&lt;/code>.&lt;/p>
&lt;p>On a new EC2 instance, running in either subnet, with the same roles as the Consul nodes, we run the following commands:&lt;/p>
&lt;pre tabindex="0">&lt;code># Install Docker
sudo su
yum update -y
yum install -y docker
service docker start
# Get my IP and the IP of any node in the server cluster.
IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)
NODE_ID=$(aws --region=&amp;#34;ap-southeast-1&amp;#34; autoscaling describe-auto-scaling-groups --auto-scaling-group-name &amp;#34;consul-asg&amp;#34; \
| grep InstanceId \
| cut -d &amp;#39;&amp;#34;&amp;#39; -f4 \
| head -1)
NODE_IP=$(aws --region=&amp;#34;ap-southeast-1&amp;#34; ec2 describe-instances \
--query=&amp;#34;Reservations[].Instances[].[PrivateIpAddress]&amp;#34; \
--output=&amp;#34;text&amp;#34; \
--instance-ids=&amp;#34;$NODE_ID&amp;#34;)
# Run the consul agent.
docker run -d --net=host \
consul agent \
-bind=&amp;#34;$IP&amp;#34; \
-join=$NODE_IP
# Run registrator - any Docker images will then be auto registered.
docker run -d \
--name=registrator \
--net=host \
--volume=/var/run/docker.sock:/tmp/docker.sock \
gliderlabs/registrator:latest \
consul://localhost:8500
# Run the example microservice - registrator will take care of letting consul know.
docker run -d -p 5000:5000 dwmkerr/zapp-service
&lt;/code>&lt;/pre>&lt;p>What&amp;rsquo;s going on here?&lt;/p>
&lt;ol>
&lt;li>We grab our own IP address and the IP address of the first instance we find in the server cluster, using the same tricks as before&lt;/li>
&lt;li>We run the Consul agent - telling it the IP to use to join the cluster&lt;/li>
&lt;li>We run &lt;a href="https://github.com/gliderlabs/registrator">Registrator&lt;/a>, a handy utility which will automatically register any new services we run to Consul&lt;/li>
&lt;li>We run a goofy sample microservice (which registrator will register for us)&lt;/li>
&lt;/ol>
&lt;p>Now we can check the Consul UI:&lt;/p>
&lt;p>&lt;img src="images/img-15-sample-service.png" alt="The Consul UI showing a new service">&lt;/p>
&lt;p>And there we have it. Our new node joins the cluster (as a client), we can register a new service with Consul.&lt;/p>
&lt;p>We can call this service from any node in the subnet, seeing output like the below:&lt;/p>
&lt;p>&lt;img src="images/img-x-zapp.png" alt="Screenshot of the Zapp service">&lt;/p>
&lt;p>In this example, I used a DNS SRV query to ask where the &lt;code>zapp-service&lt;/code> is, was told it was at &lt;code>10.0.2.158&lt;/code> on port &lt;code>5000&lt;/code>, then called the service, receiving a response. I can discover any service using this method, from any node. As services are added, removed, moved etc, I can ask Consul for accurate information on where to find them.&lt;/p>
&lt;p>Check the &lt;a href="">Step 4&lt;/a> branch to see the code in its current state.&lt;/p>
&lt;h2 id="step-5---spanner-throwing">Step 5 - Spanner Throwing&lt;/h2>
&lt;p>We can now try to throw some spanners in the works, to see how resilient the system is.&lt;/p>
&lt;p>According to the &lt;a href="https://www.consul.io/docs/internals/consensus.html#deployment-table">Deployment Table&lt;/a> from the Consul documentation, a cluster of five nodes means we have a quorum of three nodes (i.e. a minimum of three nodes are needed for a working system). This means we can tolerate the failure of two nodes.&lt;/p>
&lt;p>The easiest way to test this is to simply manually kill two nodes:&lt;/p>
&lt;p>&lt;img src="images/img-16-terminate.png" alt="Screenshot showing two AWS instances being terminated">&lt;/p>
&lt;p>If we pick two random nodes, as above, and terminate them, we see the cluster determines that we have two failed nodes but will still function (if one was the leader, a new leader will be automatically elected):&lt;/p>
&lt;p>&lt;img src="images/img-17-node-failure.png" alt="Screenshot showing the cluster highlighting two failed nodes">&lt;/p>
&lt;p>What&amp;rsquo;s nice about this setup is that no manual action is needed to recover. Our load balancer will notice the nodes are unhealthy and stop forwarding traffic. Our auto-scaling group will see the nodes have terminated and create two new ones, which will join the cluster in the same way as the original nodes. Once they join, the load balancer will find them healthy and bring them back into rotation.&lt;/p>
&lt;p>We can see from the load balancer monitoring that it notices we have unhealthy nodes and also notices when new ones come into service:&lt;/p>
&lt;p>&lt;img src="images/img-18-recovery-1.png" alt="Screenshot showing the load balancer monitoring">&lt;/p>
&lt;p>A quick check of the admin dashboard shows we now have a recovered system, with five healthy nodes:&lt;/p>
&lt;p>&lt;img src="images/img-18b-recovered.png" alt="Screenshot showing recovered system">&lt;/p>
&lt;p>The nodes which were terminated are still listed as failing. After 72 hours Consul will stop trying to periodically reconnect to these nodes and completely remove them&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="wrapping-up">Wrapping Up&lt;/h2>
&lt;p>Hopefully this should provide a good starting point to think about building your own resilient and robust systems for services like Consul.&lt;/p>
&lt;p>Interesting areas to look into to extend the project would be:&lt;/p>
&lt;ol>
&lt;li>Setting up alerts so that if we lose more than one node, we are informed&lt;/li>
&lt;li>Automating resilience tests by programatically bringing down servers and monitoring how long it takes the system to return to five nodes&lt;/li>
&lt;li>Instead of using a userdata script to set up a node, bake it into a new custom AMI with &lt;a href="https://www.packer.io/">Packer&lt;/a>&lt;/li>
&lt;li>Adding alerts for if we lose three of more nodes, which always requires manual intervention (see &lt;a href="https://www.consul.io/docs/guides/outage.html">Outage Recovery&lt;/a>)&lt;/li>
&lt;/ol>
&lt;p>As always, any questions or comments are welcome! All code is available at &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster">github.com/dwmkerr/terraform-consul-cluster&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="appendix-1-logging">Appendix 1: Logging&lt;/h2>
&lt;p>Small typos or mistakes in the userdata script are almost impossible to effectively diagnose. The scripts were actually built in the following way:&lt;/p>
&lt;ol>
&lt;li>Draft a script on my local machine which configures script logging and CloudWatch&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>&lt;/li>
&lt;li>Spin up a new EC2 instance manually&lt;/li>
&lt;li>SSH onto the instance, and run the script line by line until I&amp;rsquo;m sure it&amp;rsquo;s right&lt;/li>
&lt;li>Ensure the logs are forwarded to CloudWatch, then add the more complex features and repeatedly test&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;ve included CloudWatch logging in the code. In this write-up I&amp;rsquo;ve omitted this code as it is purely for diagnostics and doesn&amp;rsquo;t contribute to the main topic. The setup is in the &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/blob/master/files/consul-node.sh">&lt;code>consul-node.sh&lt;/code>&lt;/a> and &lt;a href="%60https://github.com/dwmkerr/terraform-consul-cluster/blob/master/consul-node-role.tf">&lt;code>consul-node-role.tf&lt;/code>&lt;/a> files.&lt;/p>
&lt;p>If you want more details, let me know, or just check the code. I would heartily recommend setting up logging like this for all but the most straightforward projects:&lt;/p>
&lt;p>&lt;img src="images/img-19-cloudwatch-1.png" alt="Screenshot showing logs">&lt;/p>
&lt;p>Being able to diagnose issues like this is vital when working with distributed systems which may be generating many log files.&lt;/p>
&lt;h2 id="appendix-2-modularisaton">Appendix 2: Modularisaton&lt;/h2>
&lt;p>I got some a great PR from &lt;a href="https://github.com/arehmandev">arehmandev&lt;/a> which modularises the code. This makes it more reusable and cleans up the structure significantly. If you want to see the before/after, check the original PR at &lt;a href="https://github.com/dwmkerr/terraform-consul-cluster/pull/4">https://github.com/dwmkerr/terraform-consul-cluster/pull/4&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Footnotes&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Further Reading&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.consul.io/docs/internals/consensus.html">Consul - Consensus Protocol&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://sitano.github.io/2015/10/06/abt-consul-outage/">What you have to know about Consul and how to beat the outage problem&lt;/a>, John Koepi&lt;/li>
&lt;/ol>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>This kind of pattern is critical in the world of microservices, where many small services will be running on a cluster. Services may die, due to errors or failing hosts, and be recreated on new hosts. Their IPs and ports may be ephemeral.It is essential that the system as a whole has a registry of where each service lives and how to access it. Such a registry must be &lt;em>resilient&lt;/em>, as it is an essential part of the system.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Most popular is a fairly loose term. Well ranked by Gartner and anecdotally with the largest infrastructure footprint. &lt;a href="https://www.gartner.com/doc/reprints?id=1-2G2O5FC&amp;amp;ct=150519&amp;amp;st=sb">https://www.gartner.com/doc/reprints?id=1-2G2O5FC&amp;amp;ct=150519&amp;amp;st=sb&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>This is AWS parlance again. An availabilty zone is an isolated datacenter. Theoretically, spreading nodes across AZs will increase resilience as it is less likely to have catastrophic failures or outages across multiple zones.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>I don&amp;rsquo;t get money from Udemy or anyone else for writing anything on this blog. All opinions are purely my own and influenced by my own experience, not sponsorship. Your milage may vary (yada yada) but I found the course quite good: &lt;a href="https://www.udemy.com/aws-certified-solutions-architect-associate/">https://www.udemy.com/aws-certified-solutions-architect-associate/&lt;/a>.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>For more expert readers that may sound horribly patronising, I don&amp;rsquo;t mean it to be. For many less experienced technologists the basics of networking might be more unfamiliar!&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>See &lt;a href="https://www.consul.io/docs/internals/consensus.html">https://www.consul.io/docs/internals/consensus.html&lt;/a>.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>A common pattern is to actually make the group size dynamic, responding to events. For example, we could have a group of servers which increases in size if the average CPU load of the hosts stays above 80% for five minutes, and scales down if it goes below 10% for ten minutes. This is more common for app and web servers and not needed for our system.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Specifically, the current latest &lt;a href="https://aws.amazon.com/amazon-linux-ami/">Amazon Linux AMI&lt;/a>.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Check the admin UI every 30 seconds, more than 3 seconds indicates a timeout and failure. Two failures in a row means an unhealthy host, which will be destroyed, two successes in a row for a new host means healthy, which means it will receive traffic.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>This is a fairly sophisticated topic in itself, see &lt;a href="https://www.consul.io/docs/internals/consensus.html">Consul - Consensus Protocol&lt;/a> for details.&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>In fact, we actually have more permissions required, because in the &amp;lsquo;real&amp;rsquo; code we also have logs forwarded to CloudWatch.&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>These nodes can be removed manually, see &lt;a href="https://www.consul.io/docs/commands/force-leave.html">Consul Force Leave&lt;/a>.&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Amazon&amp;rsquo;s service for managing and aggregating logs&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Simple Continuous Integration for Docker Images</title><link>https://dwmkerr.com/simple-continuous-integration-for-docker-images/</link><pubDate>Thu, 03 Nov 2016 05:14:35 +0000</pubDate><guid>https://dwmkerr.com/simple-continuous-integration-for-docker-images/</guid><description>&lt;p>In this article I&amp;rsquo;m going to demonstrate a few tips and tricks which can make your life easier when you are building or maintaining Dockerfiles.&lt;/p>
&lt;h2 id="the-need-for-a-build-pipeline">The need for a Build Pipeline&lt;/h2>
&lt;p>Do we really need any kind of continuous integration or build pipeline for Dockerfiles?&lt;/p>
&lt;p>There will be cases when the answer is no. However, if the answer to any of the following questions is &amp;lsquo;yes&amp;rsquo;, it might be worth considering:&lt;/p>
&lt;ol>
&lt;li>Do you want others to be able to contribute to the Dockerfile, perhaps changing the image over time?&lt;/li>
&lt;li>Are there specific functionalities in your Dockerfiles which could break if altered?&lt;/li>
&lt;li>Do you expect to need to release updates to your Dockerfile?&lt;/li>
&lt;/ol>
&lt;p>Essentially, if we are looking at providing some kind of automated quality assurance and automation around building and releasing, then a build pipeline is not a bad idea.&lt;/p>
&lt;h2 id="a-simple-build-pipeline">A simple Build Pipeline&lt;/h2>
&lt;p>Here&amp;rsquo;s what a simple build pipeline could look like. This example is for a Docker Image I just created for local DynamoDB development - &lt;a href="https://github.com/dwmkerr/docker-dynamodb">dwmkerr/docker-dynamodb&lt;/a>:&lt;/p>
&lt;p>&lt;img src="images/Simple-Docker-Image-CI.png" alt="Simple Continous Intergration Pipeline">&lt;/p>
&lt;p>Let&amp;rsquo;s dissect what we&amp;rsquo;ve got here.&lt;/p>
&lt;h3 id="the-dockerfile">The Dockerfile&lt;/h3>
&lt;p>This is the main &amp;lsquo;code&amp;rsquo; of the project if you like. The &lt;a href="https://github.com/dwmkerr/docker-dynamodb/blob/master/Dockerfile">Dockerfile&lt;/a> is the recipe for the image we create.&lt;/p>
&lt;h3 id="the-continuous-integration-service">The Continuous Integration Service&lt;/h3>
&lt;p>In this case, I am using &lt;a href="https://circleci.com/">CircleCI&lt;/a>, however the approach described would work fine with most CI systems (such as Jenkins, TravisCI and TeamCity). There &lt;em>is&lt;/em> an option to use the &lt;a href="https://docs.docker.com/docker-hub/builds/">Docker Hub Automated Builds&lt;/a>, but I&amp;rsquo;ve found this doesn&amp;rsquo;t give the flexibility I need (see &lt;a href="#appendix1whynotdockerhubautomatedbuilds">Why not Docker Hub Automated Builds&lt;/a>).&lt;/p>
&lt;p>Essentially the CI service needs to offer the option to have three distinct steps in the pipeline, each of which must pass for process to proceed:&lt;/p>
&lt;ol>
&lt;li>Build&lt;/li>
&lt;li>Test&lt;/li>
&lt;li>Deploy&lt;/li>
&lt;/ol>
&lt;h3 id="the-build">The Build&lt;/h3>
&lt;p>We can build with tools, script files, whatever. At the moment, I am leaning towards &lt;a href="https://www.gnu.org/software/make/">makefiles&lt;/a>. Normally I only need a few lines of shell script to do a build - anything more complex and the makefile can call a shell script. See also &lt;a href="#appendix2whymakefiles">Why Makefiles?&lt;/a>&lt;/p>
&lt;p>Here&amp;rsquo;s what it might look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>build:
docker build -t dwmkerr/dynamodb:latest .
ifndef BUILD_NUM
$(warning No build number is defined, skipping build number tag.)
else
docker build -t dwmkerr/dynamodb:$(BUILD_NUM) .
endif
&lt;/code>&lt;/pre>&lt;p>This command just builds the &lt;code>Dockerfile&lt;/code> and tags it as &lt;code>dwmkerr/dynamodb:lastest&lt;/code>. If a &lt;code>BUILD_NUM&lt;/code> variable is present, we also create the tag &lt;code>dwmkerr/dynamodb:BUILD_NUM&lt;/code>. This means if we want to deploy to a service such as &lt;a href="https://aws.amazon.com/ecs/">Amazon ECS&lt;/a> we can push a specific build by referring to the image with that tag.&lt;/p>
&lt;h3 id="the-tests">The Tests&lt;/h3>
&lt;p>Again I&amp;rsquo;m relying on &lt;code>make&lt;/code>. I just want to be able to run &lt;code>make test&lt;/code> - if zero is returned I&amp;rsquo;m happy. If not, the pipeline should stop and I&amp;rsquo;ll check the output. Here&amp;rsquo;s my test command:&lt;/p>
&lt;pre tabindex="0">&lt;code>test: build
./test/basics.test.sh
./test/ephemeral.test.sh
./test/persistent.test.sh
&lt;/code>&lt;/pre>&lt;p>Not a thing of beauty, but it works. These scripts I&amp;rsquo;ll discuss a little bit later on, in the delightly titled &lt;a href="#appendix3whatarethesetestscripts">What are these test scripts&lt;/a> section.&lt;/p>
&lt;p>For CircleCI, this is enough to have the main part of our pipeline. Here&amp;rsquo;s how the &lt;code>circle.yml&lt;/code> file looks at this stage:&lt;/p>
&lt;pre tabindex="0">&lt;code>machine:
services:
- docker
environment:
# Set the build number, used in makefiles.
BUILD_NUM: $CIRCLE_BUILD_NUM
test:
override:
- make test
&lt;/code>&lt;/pre>&lt;p>(Actually there&amp;rsquo;s a couple of other bits but they&amp;rsquo;re just to make sure circle uses the right version of Docker, &lt;a href="https://github.com/dwmkerr/docker-dynamodb/blob/master/circle.yml">see the full circle.yml file here&lt;/a>).&lt;/p>
&lt;h3 id="the-deployments">The Deployments&lt;/h3>
&lt;p>Deployments are trivial as all we need to do is push to the Docker Hub. The &lt;code>make deploy&lt;/code> command looks-a like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>deploy:
docker push dwmkerr/dynamodb:latest
ifndef BUILD_NUM
$(warning No build number is defined, skipping push of build number tag.)
else
docker push dwmkerr/dynamodb:$(BUILD_NUM)
endif
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;re pushing the &lt;code>latest&lt;/code> tag and &lt;code>BUILD_NUM&lt;/code> tag if present. To add this to the CircleCI pipeline, we just add the following to &lt;code>circle.yml&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>deployment:
master:
branch: master
commands:
- docker login -e $DOCKER_EMAIL -u $DOCKER_USERNAME -p $DOCKER_PASSWORD
- make deploy
&lt;/code>&lt;/pre>&lt;p>If we have a push to &lt;code>master&lt;/code>, we log in to Docker (using environment variables I configure in the CircleCI UI) and then run &lt;code>make deploy&lt;/code> to push our images.&lt;/p>
&lt;h2 id="thats-it">That&amp;rsquo;s It&lt;/h2>
&lt;p>That&amp;rsquo;s about it. This is a pretty simple approach, you can see it in action at:&lt;/p>
&lt;p>&lt;a href="https://github.com/dwmkerr/docker-dynamodb">github.com/dwmkerr/docker-dynamodb&lt;/a>&lt;/p>
&lt;p>The rest of this post is a bit of a deep dive into some specific areas I found interesting.&lt;/p>
&lt;h2 id="appendix-1-why-not-docker-hub-automated-builds">Appendix 1: Why not Docker Hub Automated Builds?&lt;/h2>
&lt;p>There are automated builds available in the Docker Hub:&lt;/p>
&lt;p>&lt;img src="images/dockerhubbuilds.png" alt="Docker Hub Automated Builds">&lt;/p>
&lt;p>I&amp;rsquo;m not using this feauture at the moment, here&amp;rsquo;s a brief roundup of what I think are the current pros and cons:&lt;/p>
&lt;p>Pros&lt;/p>
&lt;ul>
&lt;li>You don&amp;rsquo;t have to goof around installing Docker on a CI platform.&lt;/li>
&lt;li>It allows you to update the description of your Docker image automatically, from the GitHub &lt;code>README.md&lt;/code>.&lt;/li>
&lt;li>It allows you to associate the image with a specific GitHub repo (rather than just linking from the image description).&lt;/li>
&lt;li>Branch management - allowing tags to be built for specific branches.&lt;/li>
&lt;/ul>
&lt;p>Cons&lt;/p>
&lt;ul>
&lt;li>It doesn&amp;rsquo;t &lt;em>seem&lt;/em> to support any kind of configurable gating, such as a running a test command prior to deploying.&lt;/li>
&lt;li>It doesn&amp;rsquo;t &lt;em>seem&lt;/em> to support any kind of triggering of downstream processes, such as updating environments, sending notifications or whatever.&lt;/li>
&lt;/ul>
&lt;p>The lack of ability to perform tests on the image before deploying it why I&amp;rsquo;m currently not using the service.&lt;/p>
&lt;p>By doing the testing in a CI system for every pull request and only merging PRs where the tests pass we could mitigate the risk here. This service is worth watching as I&amp;rsquo;m sure it will evolve quickly.&lt;/p>
&lt;h2 id="appendix-2-why-makefiles">Appendix 2: Why Makefiles?&lt;/h2>
&lt;p>I started coding with a commandline compiler in DOS. When I used my first GUI (Borland Turbo C++) it felt like a huge leap:&lt;/p>
&lt;p>&lt;img src="images/turbocpp.png" alt="Borland Turbo C++">&lt;/p>
&lt;p>Later on I moved onto Microsoft Visual C++ 4.2:&lt;/p>
&lt;p>&lt;img src="images/visualcpp.png" alt="Visual C++ 4.2">&lt;/p>
&lt;p>And you cannot imagine the excitement when I got my boxed edition of Visual Studio .NET:&lt;/p>
&lt;p>&lt;img src="images/visualstudiodotnet.jpg" alt="Visual Studio .NET">&lt;/p>
&lt;p>Wow!&lt;/p>
&lt;p>Anyway, I digress. GNU &lt;code>make&lt;/code> was invented by Leonardo Da Vinci in 1473 to allow you to build something from the commandline, using a fairly consistent syntax.&lt;/p>
&lt;p>It is near ubiquitous on *nix systems. I am increasingly using it as an &amp;rsquo;entry point&amp;rsquo; to builds, as I use variety of languages and platforms. Being able to know that most of the time:&lt;/p>
&lt;pre tabindex="0">&lt;code>make build
make test
&lt;/code>&lt;/pre>&lt;p>Will build and test something is convenient. Makefiles actually are not that great to work with (see &lt;a href="http://stackoverflow.com/questions/448910/makefile-variable-assignment">this&lt;/a>, &lt;a href="http://stackoverflow.com/questions/10121182/multiline-bash-commands-in-makefile">this&lt;/a> and &lt;a href="http://www.conifersystems.com/whitepapers/gnu-make/">this&lt;/a>). I&amp;rsquo;ve found as long as you keep the commands simple, they&amp;rsquo;re OK. For anything really complex, I normally have a &lt;code>scripts/&lt;/code> folder, but call the scripts &lt;em>from&lt;/em> the makefile, so that there&amp;rsquo;s still a simple entrypoint.&lt;/p>
&lt;p>I&amp;rsquo;m not entirely sold on makefiles, but they tend to be my default at the moment if I know I&amp;rsquo;m going to use the commandline for builds (for example, in Java projects I&amp;rsquo;ll often write a makefile to call Maven or Gradle).&lt;/p>
&lt;p>For things like Node.js, where you have commands like &lt;code>npm test&lt;/code> or &lt;code>npm run xyz&lt;/code> I &lt;em>still&lt;/em> sometimes use makefiles, using &lt;code>npm&lt;/code> for day-to-day dev tests (&lt;code>npm start&lt;/code>) and &lt;code>make&lt;/code> if it&amp;rsquo;s something more complex (e.g. &lt;code>make deploy-sit&lt;/code> to deploy to an SIT environment).&lt;/p>
&lt;h2 id="appendix-3-what-are-these-test-scripts">Appendix 3: What are these test scripts?&lt;/h2>
&lt;p>You may have noticed:&lt;/p>
&lt;pre tabindex="0">&lt;code>test: build
./test/basics.test.sh
./test/ephemeral.test.sh
./test/persistent.test.sh
&lt;/code>&lt;/pre>&lt;p>What&amp;rsquo;s going on here?&lt;/p>
&lt;p>My Docker image is just a wrapper around &lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html">Amazon&amp;rsquo;s Local DynamoDB tool&lt;/a>. I don&amp;rsquo;t really need to test that tool. But what I wanted to test was the capabilities which lie at the &lt;em>intersection&lt;/em> between &amp;rsquo;native&amp;rsquo; Docker and &amp;rsquo;native&amp;rsquo; DynamoDB.&lt;/p>
&lt;p>For example, I know Docker supports volume mapping. I know DynamoDB supports using a data directory, to allow persistent between runs. I want to test I can combine Docker volume mapping and the DynamoDB data directory features. I know Docker images should default to being ephemeral, I want to test this holds true by default for my image.&lt;/p>
&lt;p>Testing Docker is a little hard - I want to test that I can run containers, start, stop, check state before and after and so on. This is essentially an integration test, it can be tricky to make it truly isolated and deterministic.&lt;/p>
&lt;p>I&amp;rsquo;ve given it my best go with these scripts. Here&amp;rsquo;s an example for the &amp;rsquo;ephemeral&amp;rsquo; test, where I&amp;rsquo;m trying to assert that if I run a container, create a table, stop the container and run a new one, I no longer have the table. Here&amp;rsquo;s the test:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Bomb if anything fails.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>set -e
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Kill any running dynamodb containers.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Cleaning up old containers...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker ps -a | grep dwmkerr/dynamodb | awk &lt;span style="color:#e6db74">&amp;#39;{print $1}&amp;#39;&lt;/span> | xargs docker rm -f &lt;span style="color:#f92672">||&lt;/span> true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run the container.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Checking we can run the container...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ID&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>docker run -d -p 8000:8000 dwmkerr/dynamodb&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Create a table.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>aws dynamodb --endpoint-url http://localhost:8000 --region us-east-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> create-table &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --table-name Supervillains &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --attribute-definitions AttributeName&lt;span style="color:#f92672">=&lt;/span>name,AttributeType&lt;span style="color:#f92672">=&lt;/span>S &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key-schema AttributeName&lt;span style="color:#f92672">=&lt;/span>name,KeyType&lt;span style="color:#f92672">=&lt;/span>HASH &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --provisioned-throughput ReadCapacityUnits&lt;span style="color:#f92672">=&lt;/span>1,WriteCapacityUnits&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Clean up the container. On CircleCI the FS is BTRFS, so this might fail...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Stopping and restarting...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker stop $ID &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> docker rm $ID &lt;span style="color:#f92672">||&lt;/span> true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ID&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>docker run -d -p 8000:8000 dwmkerr/dynamodb&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># List the tables - there shouldn&amp;#39;t be any!&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>COUNT&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>aws dynamodb --endpoint-url http://localhost:8000 --region us-east-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> list-tables &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> | jq &lt;span style="color:#e6db74">&amp;#39;.TableNames | length&amp;#39;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">[&lt;/span> $COUNT -ne &lt;span style="color:#e6db74">&amp;#34;0&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>; &lt;span style="color:#66d9ef">then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> echo &lt;span style="color:#e6db74">&amp;#34;Expected to find no tables, found &lt;/span>$COUNT&lt;span style="color:#e6db74">...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">fi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It&amp;rsquo;s a bit dirty - it removes containers from the host, changes things and so on. But it works.&lt;/p>
&lt;p>I did experiment with running these tests &lt;em>in a container&lt;/em>, which has the benefit of giving you a clean host to start with, which you can throw away after each test.&lt;/p>
&lt;p>I had to give up after a little while due to time constraints, but will probably revisit this process. The benefits of running these integration tests in a container is that we get a degree of isolation from the host.&lt;/p>
&lt;p>If anyone is interested, my attempts so far are on this &lt;a href="https://github.com/dwmkerr/docker-dynamodb/pull/2">RFC Pull Request&lt;/a>. Feel free to jump in!&lt;/p></description><category>CodeProject</category></item><item><title>Run Amazon DynamoDB locally with Docker</title><link>https://dwmkerr.com/run-amazon-dynamodb-locally-with-docker/</link><pubDate>Thu, 27 Oct 2016 08:06:00 +0000</pubDate><guid>https://dwmkerr.com/run-amazon-dynamodb-locally-with-docker/</guid><description>&lt;p>&lt;strong>tl;dr:&lt;/strong> Run DynamoDB locally using Docker:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d -p 8000:8000 dwmkerr/dynamodb
&lt;/code>&lt;/pre>&lt;p>Try it out by opening the shell, &lt;a href="http://localhost:8000/shell">localhost:8000/shell&lt;/a>:&lt;/p>
&lt;p>&lt;img src="images/banner.jpg" alt="DynamoDB Shell">&lt;/p>
&lt;p>That&amp;rsquo;s all there is to it!&lt;/p>
&lt;h2 id="dynamodb">DynamoDB&lt;/h2>
&lt;p>&lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html">Amazon DynamoDB&lt;/a> is a NoSQL database-as-a-service, which provides a flexible and convenient repository for your services.&lt;/p>
&lt;p>Building applications which use DynamoDB is straightforward, there are APIs and clients for many languages and platforms.&lt;/p>
&lt;p>One common requirement is to be able to run a local version of DynamoDB, for testing and development purposes. To do this, you need to:&lt;/p>
&lt;ol>
&lt;li>Hit the &lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html">DynamoDB Local&lt;/a> documentation page&lt;/li>
&lt;li>Download an archive&lt;/li>
&lt;li>Extract it to a sensible location&lt;/li>
&lt;li>Run the extracted JAR, perhaps passing in some options&lt;/li>
&lt;/ol>
&lt;p>This can be a little cumbersome if you regularly use DynamoDB, so here&amp;rsquo;s a easier way:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -p 8000:8000 dwmkerr/dynamodb
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>dwmkerr/dynamodb&lt;/code> image runs the JAR in a container, exposing the database on port 8000 by default.&lt;/p>
&lt;p>You can see the &lt;a href="dockeri.co/image/dwmkerr/dynamodb">image on the Docker Hub&lt;/a> and the source code at &lt;a href="https://github.com/dwmkerr/docker-dynamodb">github.com/dwmkerr/docker-dynamodb&lt;/a>.&lt;/p>
&lt;h2 id="customising-dynamodb">Customising DynamoDB&lt;/h2>
&lt;p>You can pass any of &lt;a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html">the documented commandline flags to DynamoDB&lt;/a>. There are instructions on the GitHub page. Here&amp;rsquo;s an example of how you can pass in a data directory, which allows DynamoDB data to be persisted after restarting a container (the image is ephemeral by default, as per &lt;a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/">Dockerfile best practices&lt;/a>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d -p 8000:8000 -v /tmp/data:/data/ dwmkerr/dynamodb -dbPath /data/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Running DynamoDB in a container gives an extra degree of flexibility and can speed up your workflow too!&lt;/p></description><category>CodeProject</category></item><item><title>Testing the Docker for Mac Beta</title><link>https://dwmkerr.com/testing-the-docker-for-mac-beta/</link><pubDate>Fri, 03 Jun 2016 10:45:24 +0000</pubDate><guid>https://dwmkerr.com/testing-the-docker-for-mac-beta/</guid><description>&lt;p>I&amp;rsquo;ve finally had a chance to install the new Docker for Mac Beta and give it a whirl. In this article I&amp;rsquo;m going to talk a bit about how Docker works, the challenges of running Docker on a Mac or Windows and how the new Beta helps.&lt;/p>
&lt;p>&lt;em>Below: The welcome message for the new Docker for Mac app&lt;/em>&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-03-at-20-33-20.png" alt="Docker for Mac Icon">&lt;/p>
&lt;h1 id="so-what-is-docker-for-mac">So What is Docker for Mac?&lt;/h1>
&lt;p>If you don&amp;rsquo;t know what Docker is, check out my article &lt;a href="http://www.dwmkerr.com/learn-docker-by-building-a-microservice/">Learn Docker by Building a Microservice&lt;/a> or the lovely &lt;a href="https://www.docker.com/what-docker">What is Docker&lt;/a> page from the docs.&lt;/p>
&lt;p>You may be aware that Docker creates processes in isolated containers using some key Linux technologies which allow for low-level isolation (such as &lt;strong>namespaces&lt;/strong> and &lt;strong>cgroups&lt;/strong>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>).&lt;/p>
&lt;p>This is described in detail on the &lt;a href="https://docs.docker.com/engine/understanding-docker/">Understand the Docker Architecture&lt;/a> page, but essentially means we can do this:&lt;/p>
&lt;p>&lt;img src="images/Docker-on-Ubuntu.png" alt="Docker Running on Ubuntu">&lt;/p>
&lt;p>Here I have:&lt;/p>
&lt;ol>
&lt;li>My machine, called &lt;code>Dave-Ubuntu&lt;/code>, which is running Ubuntu&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, with a local IP 192.168.0.1.&lt;/li>
&lt;li>The &lt;code>docker&lt;/code> executable, which I use to issue commands to&amp;hellip;&lt;/li>
&lt;li>&amp;hellip;the Docker Host, which runs the docker daemon, which actually does the work of starting/stopping/building containers and so on.&lt;/li>
&lt;li>Some containers in the Docker Host - one is based on a MySQL image and has a DB, one is based on a Node.js image and is running an app.&lt;/li>
&lt;/ol>
&lt;p>The Docker host is actually my machine - I can connect using the loopback IP 127.0.0.1 (i.e. localhost). The containers also have the IP of the host. If I want to create and connect to a MySQL DB from my machine, I just type:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d -e MYSQL_ROOT_PASSWORD=123 -p 3306:3306 mysql
mysql -uroot -p123 -h127.0.0.1
&amp;gt; show databases;
&amp;gt; ...etc...
&amp;gt; exit;
&lt;/code>&lt;/pre>&lt;p>The container was created on my machine (in the host) and addressable using my loopback IP.&lt;/p>
&lt;h2 id="so-what">So What?&lt;/h2>
&lt;p>This is all great, but things get a little harder on a Mac or Windows. MacOS and the Windows OS don&amp;rsquo;t have the same kernel level support for process isolation, control groups and so on, so the Docker Host cannot run on these operating systems. Instead, an extra layer and component is introduced:&lt;/p>
&lt;p>&lt;img src="images/Docker-on-MacOS.png" alt="Docker on OSX">&lt;/p>
&lt;p>What&amp;rsquo;s new?&lt;/p>
&lt;ol>
&lt;li>Oracle VirtualBox has been installed to create and manage virtual machines.&lt;/li>
&lt;li>A virtual machine running Linux (called in this case a &amp;lsquo;docker machine&amp;rsquo;) called &amp;lsquo;default&amp;rsquo; has been created (by convention with the IP 192.168.99.100).&lt;/li>
&lt;li>This virtual machine runs Linux, so can perfectly happily act as the docker host.&lt;/li>
&lt;li>The docker host is still addressable as 127.0.0.1 - &lt;em>from the virtual machine&lt;/em> - from the outside world (i.e. my Mac) I have to use the virtual machine IP.&lt;/li>
&lt;/ol>
&lt;p>So this is how Docker works on a Mac or on Windows. Things are made seemless where possible, for example, all of the required components are installed when you install the &lt;a href="https://www.docker.com/products/docker-toolbox">Docker Toolbox&lt;/a>.&lt;/p>
&lt;h2 id="so-what-1">So What?&lt;/h2>
&lt;p>Well the problem here is that one of the big benefits of using docker is that it allows us to create development environments which are much closer to production environments (at least from a software point of view).&lt;/p>
&lt;p>This kind of breaks down if we are doing development on a Mac or on Windows - because we have introduced an additional component which is simply not going to be present in our production environment. What are the problems?&lt;/p>
&lt;h3 id="1-localhost-vs-docker-machine-ip">1. Localhost vs docker-machine IP&lt;/h3>
&lt;p>Docker helps us be a lot more agnostic to our development box, but if I&amp;rsquo;m writing about how to interact with docker containers there&amp;rsquo;s a problem:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d -p 8080:8080 my-app-server
curl http://localhost:8080/some-api-call
&lt;/code>&lt;/pre>&lt;p>This works on a Linux machine - it does not work on a Mac or Windows. On a Mac I need to run something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d -p 8080:8080 my-app-server
curl http://$(docker-machine ip default)/some-api-call
&lt;/code>&lt;/pre>&lt;p>This will &lt;em>not&lt;/em> work on Linux or Windows.&lt;/p>
&lt;p>Is this a big deal? Actually, kind of. What if I have an integration test which spins up some containers and runs calls against them - the test has to know about the execution environment. That&amp;rsquo;s a pain. An alternative is to run tests in a container and link them with something like docker-compose, but this is not ideal.&lt;/p>
&lt;h3 id="2-terminal-hassle">2. Terminal Hassle&lt;/h3>
&lt;p>If I open a terminal and check to see what containers are running:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker ps
&lt;/code>&lt;/pre>&lt;p>I&amp;rsquo;ll see nothing. If I try to run a container:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -it mongo
&lt;/code>&lt;/pre>&lt;p>I&amp;rsquo;ll get an error - because my docker instance cannot communicate with the host. I need to use a specially set up terminal to tell it to connect to the VM.&lt;/p>
&lt;p>Again, the Docker Toolkit is set up to try and make things easy. If I install the toolkit I can run an app called Docker Quickstart Terminal:&lt;/p>
&lt;p>&lt;img src="images/Quickstart.jpg" alt="Docker Quickstart Terminal">&lt;/p>
&lt;p>And this will open a terminal where I &lt;em>can&lt;/em> use these commands. It will also start the docker machine VM if it has to. It&amp;rsquo;s even smart enough to recognise if I have multiple terminal apps, such as iTerm, and ask which one I want to use.&lt;/p>
&lt;p>This problem is - this doesn&amp;rsquo;t always work smoothly. Sometimes it will seem that the machine has started but will still not accept commands. Typically a restart is needed in this scenario.&lt;/p>
&lt;p>Also, it&amp;rsquo;s an interruption. If you are running a terminal already and want to issue a quick command, it will fail, unless it was a terminal started with the Docker Quickstart app.&lt;/p>
&lt;h3 id="3-inotify---in-container-development">3. inotify - In Container Development&lt;/h3>
&lt;p>If you recognise the term, you probably know the issue. If not, a little explanation is necessary.&lt;/p>
&lt;p>As you get more and more familiar with Docker, you will probably find that you are spending more and more time testing, building then running your image in a container. In fact, you might be changing a single code file and using the container as the dev test server on your machine.&lt;/p>
&lt;p>This fast gets painful - the container image takes time to build and slows down the development cycle. There&amp;rsquo;s a great technique in this scenario: &lt;strong>In Container Development&lt;/strong>.&lt;/p>
&lt;p>In container development is pretty much what it sounds like. Instead of editing your code on your machine, building an image and creating a container to debug, you simply create the container with what you need, &lt;strong>mount your code&lt;/strong> in to the container and run all of your development tooling from inside the container:&lt;/p>
&lt;p>&lt;img src="images/In-Container-Development.png" alt="Docker In Container Development">&lt;/p>
&lt;p>In this diagram, I have my code locally on my machine. I have built a container which runs &lt;code>nodemon&lt;/code>, watching a directory on the container. That directory is actually just a volume containing my code which I have mounted into my container.&lt;/p>
&lt;p>This is a really nice technique - I can still code locally, but as I make changes, &lt;code>nodemon&lt;/code> serves up my new content.&lt;/p>
&lt;p>This specific example applies to Node.js, but can be applied to many scenarios.&lt;/p>
&lt;p>The problem is that many watcher tools like &lt;code>nodemon&lt;/code> use a kernel subsystem called &lt;code>inotify&lt;/code>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> to get notifications when files change. But &lt;code>inotify&lt;/code> doesn&amp;rsquo;t work on virtualbox&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This means that this technique isn&amp;rsquo;t supported for Mac or Windows. There are however some tools which try and work around this with polling.&lt;/p>
&lt;p>So now we have another issue. The develop/test process might be nice on Linux, but for devs on other platforms the process is more clunky.&lt;/p>
&lt;h1 id="docker-for-mac-and-windows-to-the-rescue">Docker for Mac and Windows to the rescue&lt;/h1>
&lt;p>The issues I&amp;rsquo;ve mentioned so far are the big ones which cause me problems personally, I&amp;rsquo;m sure there are others (please comment and let me know!).&lt;/p>
&lt;p>This is why there was rather a lot of interest in the new Docker Beta - one of the big features is that the Docker Machine is going away. In theory, we can use Docker on a Mac or Windows and have the same experience as on Linux.&lt;/p>
&lt;h2 id="so-how">So how?&lt;/h2>
&lt;p>Virtualbox is gone. We still need a VM, but this VM is now a very lightweight Alpine Linux based image which runs on xhyve for MacOS and Hyper-V for Windows. All management of this VM is handled &lt;em>by the docker executable&lt;/em>.&lt;/p>
&lt;p>If these are not familiar terms, &lt;a href="https://en.wikipedia.org/wiki/Alpine_Linux">Alpine Linux&lt;/a> is an &lt;em>extreeeemely&lt;/em> lightweight Linux distro originally design to fit on a floppy disk (I think it clocks at around 5 MB now). &lt;a href="https://github.com/mist64/xhyve">xhyve&lt;/a> is an &lt;em>extremely&lt;/em> lightweight hypervisor which allows FreeBSD and some other distros on OSX. &lt;a href="https://en.wikipedia.org/wiki/Hyper-V">Hyper-V&lt;/a> is a native hypervisor for Windows Server which can run on Windows 8 onwards&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Using tools specifically designed for each platform (and with the help of both Apple and Microsoft), Docker have been able to make the experience much more seamless and smooth.&lt;/p>
&lt;h1 id="trying-it-out">Trying It Out&lt;/h1>
&lt;p>Removing the three pain points discussed and a clean and simple setup process is what I&amp;rsquo;m looking at today, and here&amp;rsquo;s the results.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>Piece of cake. Download the beta, install, run, enter the beta key and pop, there&amp;rsquo;s the new docker:&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-03-at-20-33-20-1.png" alt="Docker Welcome Message">&lt;/p>
&lt;p>The new status bar icon gives me a way to quickly see the status of the machine. Some of the commands hint at features to come, others offer the instructions needed. Settings are fairly basic, but I&amp;rsquo;m not sure what else you&amp;rsquo;d need:&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-04-at-00-09-56.png" alt="Status Bar Screenshot 1">&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-04-at-00-10-07.png" alt="Status Bar Screenshot 2">&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-04-at-00-10-23.png" alt="Status Bar Screenshot 3">&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-04-at-00-44-04.png" alt="Status Bar Screenshot 4">&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-04-at-00-44-12.png" alt="Status Bar Screenshot 5">&lt;/p>
&lt;h3 id="1-localhost-vs-docker-machine-ip-1">1. Localhost vs docker-machine IP&lt;/h3>
&lt;p>Quickly bashing out the commands below shows that the virtual machine IP address issue is gone:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123 mysql
mysql -uroot -p123 -h127.0.0.1
&amp;gt; show databases;
&lt;/code>&lt;/pre>&lt;p>&lt;img src="images/Screen-Shot-2016-06-03-at-23-37-39.png" alt="Localhost Screenshot">&lt;/p>
&lt;p>Great news!&lt;/p>
&lt;p>How this works under the hood is a mystery to me. If anyone knows, I&amp;rsquo;d be interested and would like to update this writeup!&lt;/p>
&lt;h3 id="2-terminal-hassle-1">2. Terminal Hassle&lt;/h3>
&lt;p>Quick and easy to test - running any terminal any way I like lets me access containers using the &lt;code>docker&lt;/code> executable - no magic needed:&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-03-at-23-46-57.png" alt="Shells">&lt;/p>
&lt;p>Here&amp;rsquo;s a screenshot of iTerm3, the Terminal App and the Terminal App running &lt;code>zsh&lt;/code>, all of which are happily communicating with the docker deamon through the &lt;code>docker&lt;/code> app.&lt;/p>
&lt;h3 id="3-in-container-development">3. In Container Development&lt;/h3>
&lt;p>I&amp;rsquo;ve not thrashed this one too hard, but gone for a quick sanity check. Throwing together probably my best ever node.js app&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>:&lt;/p>
&lt;p>&lt;strong>main.js&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>setInterval(function() {
console.log(&amp;#34;Goodbye, cruel world!&amp;#34;);
}, 1000);
&lt;/code>&lt;/pre>&lt;p>and a simple dockerfile:&lt;/p>
&lt;p>&lt;strong>Dockerfile&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>FROM node:6
WORKDIR src/
ADD package.json .
RUN npm install
CMD npm start
&lt;/code>&lt;/pre>&lt;p>is enough to test this. I can build then run the container, mounting the working directory into the &lt;code>src&lt;/code> volume on the container:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker build -t incontainerdev .
docker run -it -v `pwd`:/src incontainerdev]
&lt;/code>&lt;/pre>&lt;p>Immediately, I open a new window and change the source code and save (on my local Mac, not in the container). Voila:&lt;/p>
&lt;p>&lt;img src="images/Screen-Shot-2016-06-04-at-00-03-23.png" alt="Live Reloading">&lt;/p>
&lt;p>Live reloading works without a hitch! &lt;code>nodemon&lt;/code> picks up my changes, using &lt;code>inotify&lt;/code> from the VM (all through a lightweight userspace hypervisor).&lt;/p>
&lt;p>You know what is cool&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>? &lt;strong>I don&amp;rsquo;t even need Node.js installed to build this Node app!&lt;/strong> The runtime is in the container, all of the execution happens in the container.&lt;/p>
&lt;h1 id="thats-a-wrap">That&amp;rsquo;s a Wrap&lt;/h1>
&lt;p>That&amp;rsquo;s it for my initial impressions. From this point onwards I&amp;rsquo;m going to be using Docker for Mac heavily as I&amp;rsquo;ll do all of my work with it installed, so from time to time I may update this article with other observations.&lt;/p>
&lt;p>The key takeaway is: at the moment, Docker for Mac just &lt;em>works&lt;/em>. I&amp;rsquo;m using it in the same way I would on Ubuntu with no messing around. This is great, it seems like a simple thing but I&amp;rsquo;m guessing it was a lot of effort from the guys and girls at Docker, Microsoft and Apple.&lt;/p>
&lt;p>This is still a Beta, there&amp;rsquo;ll be bugs and they&amp;rsquo;ll be fixed. I can&amp;rsquo;t wait for the Beta to go fully into the wild, and see what exciting things people can do with it.&lt;/p>
&lt;p>As usual, any comments or observations are welcome!&lt;/p>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Further Reading&lt;/strong>&lt;/p>
&lt;p>Namespaces: &lt;a href="http://man7.org/linux/man-pages/man7/namespaces.7.html">http://man7.org/linux/man-pages/man7/namespaces.7.html&lt;/a>
cgroups: &lt;a href="http://man7.org/linux/man-pages/man7/cgroups.7.html">http://man7.org/linux/man-pages/man7/cgroups.7.html&lt;/a>
Docker Execution Drivers: &lt;a href="https://blog.docker.com/2014/03/docker-0-9-introducing-execution-drivers-and-libcontainer/">https://blog.docker.com/2014/03/docker-0-9-introducing-execution-drivers-and-libcontainer/&lt;/a>
inotify: &lt;a href="http://man7.org/linux/man-pages/man7/inotify.7.html">http://man7.org/linux/man-pages/man7/inotify.7.html&lt;/a>
The challenges of in-container development on OSX: &lt;a href="http://hharnisc.github.io/2015/09/16/developing-inside-docker-containers-with-osx.html">http://hharnisc.github.io/2015/09/16/developing-inside-docker-containers-with-osx.html&lt;/a>&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Read up on namespaces &lt;a href="http://man7.org/linux/man-pages/man7/namespaces.7.html">here&lt;/a> and cgroups &lt;a href="http://man7.org/linux/man-pages/man7/cgroups.7.html">here&lt;/a>. Docker can also use &lt;a href="https://en.wikipedia.org/wiki/LXC">LXC&lt;/a> but no longer &lt;em>has&lt;/em> to, there&amp;rsquo;s a great write-up &lt;a href="https://blog.docker.com/2014/03/docker-0-9-introducing-execution-drivers-and-libcontainer/">here&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Surprise!&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Read up on inotify &lt;a href="http://man7.org/linux/man-pages/man7/inotify.7.html">here&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>The issue will not be resolved: &lt;a href="https://www.virtualbox.org/ticket/10660">https://www.virtualbox.org/ticket/10660&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>I&amp;rsquo;ve not used the Docker for Windows Beta yet so have not got first hand experience of it. I&amp;rsquo;ve also not looked into compatibility, from memory Hyper-V isn&amp;rsquo;t available on Home versions of Windows, but I might be wrong.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Inspired by my first programming book, the excellent &lt;a href="http://www.amazon.com/C-Dummies-Dan-Gookin/dp/0764570684">C for Dummies&lt;/a> by Dan Gookin.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>For a given definition of cool.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item><item><title>Learn Docker by building a Microservice</title><link>https://dwmkerr.com/learn-docker-by-building-a-microservice/</link><pubDate>Tue, 19 Apr 2016 08:54:39 +0000</pubDate><guid>https://dwmkerr.com/learn-docker-by-building-a-microservice/</guid><description>&lt;p>If you are looking to get your hands dirty and learn all about &lt;a href="https://docker.com">Docker&lt;/a>, then look no further!&lt;/p>
&lt;p>In this article I&amp;rsquo;m going to show you how Docker works, what all the fuss is about, and how Docker can help with a basic development task - building a microservice.&lt;/p>
&lt;p>We&amp;rsquo;ll use a simple Node.js service with a MySQL backend as an example, going from code running locally to containers running a microservice and database.&lt;/p>
&lt;p align="center">
&lt;img src="images/Article.png" />
&lt;/p>
&lt;p>Once you&amp;rsquo;ve read the article, you can find the source code here:&lt;/p>
&lt;p>&lt;a href="https://github.com/dwmkerr/node-docker-microservice">github.com/dwmkerr/node-docker-microservice&lt;/a>&lt;/p>
&lt;h2 id="what-is-docker">What is Docker?&lt;/h2>
&lt;p>At its heart, Docker is software which lets you create an &lt;em>image&lt;/em> (which is a lot like a template for a virtual machine) and then run instances of that image in a &lt;em>container&lt;/em>.&lt;/p>
&lt;p>Docker maintains a vast repository of images, called the &lt;a href="https://hub.docker.com">Docker Hub&lt;/a> which you can use as starting points or as free storage for your own images. You can install Docker, choose an image you&amp;rsquo;d like to use, then run an instance of it in a container.&lt;/p>
&lt;p>We&amp;rsquo;re going to build images, create containers from images and more in this article.&lt;/p>
&lt;h3 id="install-docker">Install Docker&lt;/h3>
&lt;p>To follow along and use this article, you&amp;rsquo;ll need Docker.&lt;/p>
&lt;p>Check the installation guide for your platform, &lt;a href="https://docs.docker.com/engine/installation/">docs.docker.com/engine/installation&lt;/a>.&lt;/p>
&lt;p>If you are on Mac or Windows, consider using a Virtual Machine. I use Parallels on Mac OS X to run an Ubuntu machine for most development activities. Being able to take snapshots, break things and then revert back is very handy when experimenting.&lt;/p>
&lt;h3 id="try-it-out">Try It Out&lt;/h3>
&lt;p>Enter this command:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -it ubuntu
&lt;/code>&lt;/pre>&lt;p>After a bit of spinning, you&amp;rsquo;ll see a prompt like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@719059da250d:/#
&lt;/code>&lt;/pre>&lt;p>Try out a few commands and then exit the container:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@719059da250d:/# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description: Ubuntu 14.04.4 LTS
Release: 14.04
Codename: trusty
root@719059da250d:/# exit
&lt;/code>&lt;/pre>&lt;p>This doesn&amp;rsquo;t look like much, but a lot has happened!&lt;/p>
&lt;p>What you are seeing is the bash shell of an &lt;em>isolated&lt;/em> container running Ubuntu, on your machine. It&amp;rsquo;s yours to place with - you can install things on it, run software, whatever you want.&lt;/p>
&lt;p>Here&amp;rsquo;s a diagram and breakdown of what just happened (the digram is from the &lt;a href="https://docs.docker.com/v1.8/introduction/understanding-docker/">&amp;lsquo;Understanding the Architecture&amp;rsquo; Docker Documentation&lt;/a>, which is great):&lt;/p>
&lt;p>&lt;img src="images/Flow.png" alt="Docker Run Flow">&lt;/p>
&lt;ol>
&lt;li>We issue a docker command:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;code>docker&lt;/code>: run the docker client&lt;/li>
&lt;li>&lt;code>run&lt;/code>: the command to run a new container&lt;/li>
&lt;li>&lt;code>-it&lt;/code>: option to give the container an interactive terminal&lt;/li>
&lt;li>&lt;code>ubuntu&lt;/code>: the image to base the container on&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>The docker service running on the host (our machine) checks to see if we have a copy of the requested image locally- which there isn&amp;rsquo;t.&lt;/li>
&lt;li>The docker service checks the public registry (the docker hub) to see if there&amp;rsquo;s an image named &lt;code>ubuntu&lt;/code> available- which there is.&lt;/li>
&lt;li>The docker service downloads the image and stores it in its local cache of images (ready for next time).&lt;/li>
&lt;li>The docker service creates a new container, based on the &lt;code>ubuntu&lt;/code> image.&lt;/li>
&lt;/ol>
&lt;p>Try any of these:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -it haskell
docker run -it java
docker run -it python
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;re not going to use &lt;a href="https://xkcd.com/1312/">Haskell&lt;/a> today, but you can see, running an environment is very easy.&lt;/p>
&lt;p>It&amp;rsquo;s a snap to build images of our own, with our apps or services on them, databases, whatever we need. We can then run them on any machine with Docker installed - and the image will run in the same, predictable way. We can build our software &lt;em>and the environment it runs on&lt;/em> as code and deploy easily.&lt;/p>
&lt;p>Let&amp;rsquo;s look into a simple microservice as an example.&lt;/p>
&lt;h2 id="the-brief">The Brief&lt;/h2>
&lt;p>We&amp;rsquo;re going to build a microservice which lets us manage a directory of email addresses to phone numbers, using Node.js and MySQL.&lt;/p>
&lt;h2 id="getting-started">Getting Started&lt;/h2>
&lt;p>For doing local development we&amp;rsquo;ll need to install MySQL and create a test database for us to&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;nope.&lt;/p>
&lt;p>Creating a local database and running scripts on it is an easy start, but can get messy. Lots of uncontrolled stuff going on. It might work, we could even control it with some shell scripts checked in to our repo, but what if other developers already have MySQL installed? What if they have a database already with the creative name &amp;lsquo;users&amp;rsquo; which we want to create?&lt;/p>
&lt;h3 id="step-1-creating-a-test-database-server---in-docker">Step 1: Creating a Test Database Server - in Docker&lt;/h3>
&lt;p>This is a great Docker use case. We might not want to run our production database in Docker (perhaps we&amp;rsquo;ll just use Amazon RDS for example), but we can spin up a clean MySQL database in no time as a Docker container for development - leaving our development machine clean and keeping everything we do controlled and repeatable.&lt;/p>
&lt;p>Run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run --name db -d -e MYSQL_ROOT_PASSWORD=123 -p 3306:3306 mysql:latest
&lt;/code>&lt;/pre>&lt;p>This starts a MySQL instance running, allowing access through port 3306 using the root password 123.&lt;/p>
&lt;ol>
&lt;li>&lt;code>docker run&lt;/code> tells the engine we want to run an image (the image comes at the end, &lt;a href="https://hub.docker.com/_/mysql/">mysql:vlatest&lt;/a>&lt;/li>
&lt;li>&lt;code>--name db&lt;/code> names this container &lt;code>db&lt;/code>.&lt;/li>
&lt;li>&lt;code>-d&lt;/code> (or &lt;code>--detach&lt;/code>) detach - i.e. run the container in the background.&lt;/li>
&lt;li>&lt;code>-e MYSQL_ROOT_PASSWORD=123&lt;/code> (or &lt;code>--env&lt;/code>) environment variables - tells docker we want to provide an environment variable. The variable following it is what the MySQL image checks for setting the default root password.&lt;/li>
&lt;li>&lt;code>-p 3306:3306&lt;/code> (or &lt;code>--publish&lt;/code> tells the engine that we want to map the port 3306 from inside the container to out port 3306.&lt;/li>
&lt;/ol>
&lt;p>The last part is really important - even though that&amp;rsquo;s the MySQL default port, if we don&amp;rsquo;t tell docker explicitly we want to map it, it will block access through that port (because containers are isolated until you tell them you want access).&lt;/p>
&lt;p>The return value of this function is the &lt;em>container id&lt;/em>, a reference to the container which you can use to stop it, restart it, issue commands on it and so on. Let&amp;rsquo;s see which containers are running:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ docker ps
CONTAINER ID IMAGE ... NAMES
36e68b966fd0 mysql:latest ... db
&lt;/code>&lt;/pre>&lt;p>The key information is the container ID, image and name. Let&amp;rsquo;s connect to this image and see what&amp;rsquo;s there:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ docker exec -it db /bin/bash
root@36e68b966fd0:/# mysql -uroot -p123
mysql&amp;gt; show databases;
+--------------------+
| Database |
+--------------------+
| information_schema |
+--------------------+
1 rows in set (0.01 sec)
mysql&amp;gt; exit
Bye
root@36e68b966fd0:/# exit
&lt;/code>&lt;/pre>&lt;p>This is pretty clever too:&lt;/p>
&lt;ol>
&lt;li>&lt;code>docker exec -it db&lt;/code> tells docker we want to execute a command on the container named &lt;code>db&lt;/code> (we could also use the id, or just the first few letters of the id). &lt;code>-it&lt;/code> ensures we have an interactive terminal.&lt;/li>
&lt;li>&lt;code>mysql -uroot -p123&lt;/code> the command we actually run as a process in the container, which in this case is just the mysql client.&lt;/li>
&lt;/ol>
&lt;p>We can create databases, tables, users, whatever we need.&lt;/p>
&lt;h3 id="wrapping-up-the-test-database">Wrapping up the Test Database&lt;/h3>
&lt;p>Running MySQL inside a container has already introduced a few Docker tricks, but let&amp;rsquo;s pause here and move onto the service. For now, we&amp;rsquo;ll have create a &lt;code>test-database&lt;/code> folder with a script to start the database, stop the database and setup test data:&lt;/p>
&lt;pre tabindex="0">&lt;code>test-database\setup.sql
test-database\start.sh
test-database\stop.sh
&lt;/code>&lt;/pre>&lt;p>Start is simple:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#!/bin/sh
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run the MySQL container, with a database named &amp;#39;users&amp;#39; and credentials&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># for a users-service user which can access it.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Starting DB...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run --name db -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e MYSQL_ROOT_PASSWORD&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">123&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e MYSQL_DATABASE&lt;span style="color:#f92672">=&lt;/span>users -e MYSQL_USER&lt;span style="color:#f92672">=&lt;/span>users_service -e MYSQL_PASSWORD&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">123&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 3306:3306 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> mysql:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Wait for the database service to start up.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Waiting for DB to start up...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker exec db mysqladmin --silent --wait&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">30&lt;/span> -uusers_service -p123 ping &lt;span style="color:#f92672">||&lt;/span> exit &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run the setup script.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;Setting up initial data...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker exec -i db mysql -uusers_service -p123 users &amp;lt; setup.sql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This script runs the database image in a detached container (i.e. in the background), with a user set up to access a &lt;code>users&lt;/code> database, then waits for the database server to start up, then runs a &lt;code>setup.sql&lt;/code> script to set initial data.&lt;/p>
&lt;p>&lt;code>setup.sql&lt;/code> is:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sql" data-lang="sql">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">create&lt;/span> &lt;span style="color:#66d9ef">table&lt;/span> directory (user_id INT &lt;span style="color:#66d9ef">NOT&lt;/span> &lt;span style="color:#66d9ef">NULL&lt;/span> AUTO_INCREMENT &lt;span style="color:#66d9ef">PRIMARY&lt;/span> &lt;span style="color:#66d9ef">KEY&lt;/span>, email TEXT, phone_number TEXT);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">insert&lt;/span> &lt;span style="color:#66d9ef">into&lt;/span> directory (email, phone_number) &lt;span style="color:#66d9ef">values&lt;/span> (&lt;span style="color:#e6db74">&amp;#39;homer@thesimpsons.com&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;+1 888 123 1111&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">insert&lt;/span> &lt;span style="color:#66d9ef">into&lt;/span> directory (email, phone_number) &lt;span style="color:#66d9ef">values&lt;/span> (&lt;span style="color:#e6db74">&amp;#39;marge@thesimpsons.com&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;+1 888 123 1112&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">insert&lt;/span> &lt;span style="color:#66d9ef">into&lt;/span> directory (email, phone_number) &lt;span style="color:#66d9ef">values&lt;/span> (&lt;span style="color:#e6db74">&amp;#39;maggie@thesimpsons.com&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;+1 888 123 1113&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">insert&lt;/span> &lt;span style="color:#66d9ef">into&lt;/span> directory (email, phone_number) &lt;span style="color:#66d9ef">values&lt;/span> (&lt;span style="color:#e6db74">&amp;#39;lisa@thesimpsons.com&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;+1 888 123 1114&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">insert&lt;/span> &lt;span style="color:#66d9ef">into&lt;/span> directory (email, phone_number) &lt;span style="color:#66d9ef">values&lt;/span> (&lt;span style="color:#e6db74">&amp;#39;bart@thesimpsons.com&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;+1 888 123 1115&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>stop.sh&lt;/code> script will stop the container and remove it (containers are left around by docker by default so that they can be restared quickly, we don&amp;rsquo;t really need that feature for this example):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#!/bin/sh
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Stop the db and remove the container.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker stop db &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> docker rm db
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;re going to make this even more slick later on, simplifying this further. Check the code at this stage by looking at the &lt;a href="https://github.com/dwmkerr/node-docker-microservice/tree/step1">step1&lt;/a> branch of the repo.&lt;/p>
&lt;h3 id="step-2-creating-a-microservice-in-nodejs">Step 2: Creating a Microservice in Node.js&lt;/h3>
&lt;p>This article is really focused on learning Docker, so I&amp;rsquo;m not going to spend ages on the Node.js microservice. Instead, I&amp;rsquo;ll highlight the areas and takeaways.&lt;/p>
&lt;pre tabindex="0">&lt;code>test-database/ # contains the code seen in Step 1
users-service/ # root of our node.js microservice
- package.json # dependencies, metadata
- index.js # main entrypoint of the app
- api/ # our apis and api tests
- config/ # config for the app
- repository/ # abstraction over our db
- server/ # server setup code
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s take this apart bit by bit. The first section to look at is &lt;code>repository&lt;/code>. It can be useful to wrap your database access in some kind of class or abstraction, to allow to mock it for testing purposes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// repository.js
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Exposes a single function - &amp;#39;connect&amp;#39;, which returns
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// a connected repository. Call &amp;#39;disconnect&amp;#39; on this object when you&amp;#39;re done.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#e6db74">&amp;#39;use strict&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">mysql&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;mysql&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Class which holds an open connection to a repository
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// and exposes some simple functions for accessing data.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">Repository&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">constructor&lt;/span>(&lt;span style="color:#a6e22e">connection&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">this&lt;/span>.&lt;span style="color:#a6e22e">connection&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">connection&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">getUsers&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Promise((&lt;span style="color:#a6e22e">resolve&lt;/span>, &lt;span style="color:#a6e22e">reject&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">this&lt;/span>.&lt;span style="color:#a6e22e">connection&lt;/span>.&lt;span style="color:#a6e22e">query&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;SELECT email, phone_number FROM directory&amp;#39;&lt;/span>, (&lt;span style="color:#a6e22e">err&lt;/span>, &lt;span style="color:#a6e22e">results&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#a6e22e">err&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">reject&lt;/span>(&lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;An error occured getting the users: &amp;#34;&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span>));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">resolve&lt;/span>((&lt;span style="color:#a6e22e">results&lt;/span> &lt;span style="color:#f92672">||&lt;/span> []).&lt;span style="color:#a6e22e">map&lt;/span>((&lt;span style="color:#a6e22e">user&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">email&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">user&lt;/span>.&lt;span style="color:#a6e22e">email&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">phone_number&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">user&lt;/span>.&lt;span style="color:#a6e22e">phone_number&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> };
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">getUserByEmail&lt;/span>(&lt;span style="color:#a6e22e">email&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Promise((&lt;span style="color:#a6e22e">resolve&lt;/span>, &lt;span style="color:#a6e22e">reject&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Fetch the customer.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">this&lt;/span>.&lt;span style="color:#a6e22e">connection&lt;/span>.&lt;span style="color:#a6e22e">query&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;SELECT email, phone_number FROM directory WHERE email = ?&amp;#39;&lt;/span>, [&lt;span style="color:#a6e22e">email&lt;/span>], (&lt;span style="color:#a6e22e">err&lt;/span>, &lt;span style="color:#a6e22e">results&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#a6e22e">err&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">reject&lt;/span>(&lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;An error occured getting the user: &amp;#34;&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span>));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#a6e22e">results&lt;/span>.&lt;span style="color:#a6e22e">length&lt;/span> &lt;span style="color:#f92672">===&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">resolve&lt;/span>(&lt;span style="color:#66d9ef">undefined&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#66d9ef">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">resolve&lt;/span>({
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">email&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">results&lt;/span>[&lt;span style="color:#ae81ff">0&lt;/span>].&lt;span style="color:#a6e22e">email&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">phone_number&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">results&lt;/span>[&lt;span style="color:#ae81ff">0&lt;/span>].&lt;span style="color:#a6e22e">phone_number&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">disconnect&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">this&lt;/span>.&lt;span style="color:#a6e22e">connection&lt;/span>.&lt;span style="color:#a6e22e">end&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// One and only exported function, returns a connected repo.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">module&lt;/span>.&lt;span style="color:#a6e22e">exports&lt;/span>.&lt;span style="color:#a6e22e">connect&lt;/span> &lt;span style="color:#f92672">=&lt;/span> (&lt;span style="color:#a6e22e">connectionSettings&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Promise((&lt;span style="color:#a6e22e">resolve&lt;/span>, &lt;span style="color:#a6e22e">reject&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">connectionSettings&lt;/span>.&lt;span style="color:#a6e22e">host&lt;/span>) &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;A host must be specified.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">connectionSettings&lt;/span>.&lt;span style="color:#a6e22e">user&lt;/span>) &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;A user must be specified.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">connectionSettings&lt;/span>.&lt;span style="color:#a6e22e">password&lt;/span>) &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;A password must be specified.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">connectionSettings&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span>) &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;A port must be specified.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">resolve&lt;/span>(&lt;span style="color:#66d9ef">new&lt;/span> &lt;span style="color:#a6e22e">Repository&lt;/span>(&lt;span style="color:#a6e22e">mysql&lt;/span>.&lt;span style="color:#a6e22e">createConnection&lt;/span>(&lt;span style="color:#a6e22e">connectionSettings&lt;/span>)));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There&amp;rsquo;s probably a lot of better ways to do this! But basically we can create a &lt;code>Repository&lt;/code> object like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">repository&lt;/span>.&lt;span style="color:#a6e22e">connect&lt;/span>({
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">host&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;127.0.0.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">database&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;users&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">user&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;users_service&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">password&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;123&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">3306&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}).&lt;span style="color:#a6e22e">then&lt;/span>((&lt;span style="color:#a6e22e">repo&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">repo&lt;/span>.&lt;span style="color:#a6e22e">getUsers&lt;/span>().&lt;span style="color:#a6e22e">then&lt;/span>(&lt;span style="color:#a6e22e">users&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#a6e22e">users&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">repo&lt;/span>.&lt;span style="color:#a6e22e">getUserByEmail&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;homer@thesimpsons.com&amp;#39;&lt;/span>).&lt;span style="color:#a6e22e">then&lt;/span>((&lt;span style="color:#a6e22e">user&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#a6e22e">user&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// ...when you are done...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">repo&lt;/span>.&lt;span style="color:#a6e22e">disconnect&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There&amp;rsquo;s also a set of unit tests in the &lt;code>repository/repository.spec.js&lt;/code> file. Now that we&amp;rsquo;ve got a repo, we can create a server. This is &lt;code>server/server.js&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// server.js
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">express&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;express&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">morgan&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;morgan&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">module&lt;/span>.&lt;span style="color:#a6e22e">exports&lt;/span>.&lt;span style="color:#a6e22e">start&lt;/span> &lt;span style="color:#f92672">=&lt;/span> (&lt;span style="color:#a6e22e">options&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Promise((&lt;span style="color:#a6e22e">resolve&lt;/span>, &lt;span style="color:#a6e22e">reject&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Make sure we have a repository and port provided.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">repository&lt;/span>) &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;A server must be started with a connected repository.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span>) &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;A server must be started with a port.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Create the app, add some logging.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">app&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">express&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">app&lt;/span>.&lt;span style="color:#a6e22e">use&lt;/span>(&lt;span style="color:#a6e22e">morgan&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;dev&amp;#39;&lt;/span>));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Add the APIs to the app.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;../api/users&amp;#39;&lt;/span>)(&lt;span style="color:#a6e22e">app&lt;/span>, &lt;span style="color:#a6e22e">options&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Start the app, creating a running server which we return.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">server&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">app&lt;/span>.&lt;span style="color:#a6e22e">listen&lt;/span>(&lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span>, () =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">resolve&lt;/span>(&lt;span style="color:#a6e22e">server&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This module exposes a &lt;code>start&lt;/code> function, which we can use like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">server&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;./server/server);
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">server.start({port: 8080, repo: repository}).then((svr) =&amp;gt; {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> // we&amp;#39;&lt;/span>&lt;span style="color:#a6e22e">ve&lt;/span> &lt;span style="color:#a6e22e">got&lt;/span> &lt;span style="color:#a6e22e">a&lt;/span> &lt;span style="color:#a6e22e">running&lt;/span> &lt;span style="color:#a6e22e">http&lt;/span> &lt;span style="color:#a6e22e">server&lt;/span> &lt;span style="color:#f92672">:&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that &lt;code>server.js&lt;/code> uses &lt;code>api/users/js&lt;/code>? Here it is:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// users.js
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Defines the users api. Add to a server by calling:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// require(&amp;#39;./users&amp;#39;)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#e6db74">&amp;#39;use strict&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Only export - adds the API to the app with the given options.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">module&lt;/span>.&lt;span style="color:#a6e22e">exports&lt;/span> &lt;span style="color:#f92672">=&lt;/span> (&lt;span style="color:#a6e22e">app&lt;/span>, &lt;span style="color:#a6e22e">options&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">app&lt;/span>.&lt;span style="color:#a6e22e">get&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;/users&amp;#39;&lt;/span>, (&lt;span style="color:#a6e22e">req&lt;/span>, &lt;span style="color:#a6e22e">res&lt;/span>, &lt;span style="color:#a6e22e">next&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">repository&lt;/span>.&lt;span style="color:#a6e22e">getUsers&lt;/span>().&lt;span style="color:#a6e22e">then&lt;/span>((&lt;span style="color:#a6e22e">users&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">res&lt;/span>.&lt;span style="color:#a6e22e">status&lt;/span>(&lt;span style="color:#ae81ff">200&lt;/span>).&lt;span style="color:#a6e22e">send&lt;/span>(&lt;span style="color:#a6e22e">users&lt;/span>.&lt;span style="color:#a6e22e">map&lt;/span>((&lt;span style="color:#a6e22e">user&lt;/span>) =&amp;gt; { &lt;span style="color:#66d9ef">return&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">email&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">user&lt;/span>.&lt;span style="color:#a6e22e">email&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">phoneNumber&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">user&lt;/span>.&lt;span style="color:#a6e22e">phone_number&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> };
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> .&lt;span style="color:#66d9ef">catch&lt;/span>(&lt;span style="color:#a6e22e">next&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">app&lt;/span>.&lt;span style="color:#a6e22e">get&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;/search&amp;#39;&lt;/span>, (&lt;span style="color:#a6e22e">req&lt;/span>, &lt;span style="color:#a6e22e">res&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Get the email.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">email&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">req&lt;/span>.&lt;span style="color:#a6e22e">query&lt;/span>.&lt;span style="color:#a6e22e">email&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">email&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">throw&lt;/span> &lt;span style="color:#66d9ef">new&lt;/span> Error(&lt;span style="color:#e6db74">&amp;#34;When searching for a user, the email must be specified, e.g: &amp;#39;/search?email=homer@thesimpsons.com&amp;#39;.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Get the user from the repo.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">options&lt;/span>.&lt;span style="color:#a6e22e">repository&lt;/span>.&lt;span style="color:#a6e22e">getUserByEmail&lt;/span>(&lt;span style="color:#a6e22e">email&lt;/span>).&lt;span style="color:#a6e22e">then&lt;/span>((&lt;span style="color:#a6e22e">user&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span>(&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">user&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">res&lt;/span>.&lt;span style="color:#a6e22e">status&lt;/span>(&lt;span style="color:#ae81ff">404&lt;/span>).&lt;span style="color:#a6e22e">send&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;User not found.&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#66d9ef">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">res&lt;/span>.&lt;span style="color:#a6e22e">status&lt;/span>(&lt;span style="color:#ae81ff">200&lt;/span>).&lt;span style="color:#a6e22e">send&lt;/span>({
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">email&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">user&lt;/span>.&lt;span style="color:#a6e22e">email&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">phoneNumber&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">user&lt;/span>.&lt;span style="color:#a6e22e">phone_number&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> .&lt;span style="color:#66d9ef">catch&lt;/span>(&lt;span style="color:#a6e22e">next&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Both of these files have unit tests adjacent to the source.&lt;/p>
&lt;p>We&amp;rsquo;ll need config. Rather than using a specialised library, a simple file will do the trick - &lt;code>config/config.js&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// config.js
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Simple application configuration. Extend as needed.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">module&lt;/span>.&lt;span style="color:#a6e22e">exports&lt;/span> &lt;span style="color:#f92672">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">env&lt;/span>.&lt;span style="color:#a6e22e">PORT&lt;/span> &lt;span style="color:#f92672">||&lt;/span> &lt;span style="color:#ae81ff">8123&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">db&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">host&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">env&lt;/span>.&lt;span style="color:#a6e22e">DATABASE_HOST&lt;/span> &lt;span style="color:#f92672">||&lt;/span> &lt;span style="color:#e6db74">&amp;#39;127.0.0.1&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">database&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;users&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">user&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;users_service&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">password&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;123&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">3306&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can &lt;code>require&lt;/code> config as needed. Currently, most config is hard coded, but as you can see from &lt;code>port&lt;/code> it&amp;rsquo;s easy to add environment variables as an option.&lt;/p>
&lt;p>Final step - stringing it together with an &lt;code>index.js&lt;/code> file which composes everything:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// index.js
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Entrypoint to the application. Opens a repository to the MySQL
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// server and starts the server.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">server&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;./server/server&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">repository&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;./repository/repository&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;./config/config&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Lots of verbose logging when we&amp;#39;re starting up...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;--- Customer Service---&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Connecting to customer repository...&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Log unhandled exceptions.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">on&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;uncaughtException&amp;#39;&lt;/span>, &lt;span style="color:#66d9ef">function&lt;/span>(&lt;span style="color:#a6e22e">err&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">error&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;Unhandled Exception&amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">on&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;unhandledRejection&amp;#39;&lt;/span>, &lt;span style="color:#66d9ef">function&lt;/span>(&lt;span style="color:#a6e22e">err&lt;/span>, &lt;span style="color:#a6e22e">promise&lt;/span>){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">error&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;Unhandled Rejection&amp;#39;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">repository&lt;/span>.&lt;span style="color:#a6e22e">connect&lt;/span>({
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">host&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">db&lt;/span>.&lt;span style="color:#a6e22e">host&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">database&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">db&lt;/span>.&lt;span style="color:#a6e22e">database&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">user&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">db&lt;/span>.&lt;span style="color:#a6e22e">user&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">password&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">db&lt;/span>.&lt;span style="color:#a6e22e">password&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">db&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}).&lt;span style="color:#a6e22e">then&lt;/span>((&lt;span style="color:#a6e22e">repo&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Connected. Starting server...&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">server&lt;/span>.&lt;span style="color:#a6e22e">start&lt;/span>({
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">repository&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">repo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}).&lt;span style="color:#a6e22e">then&lt;/span>((&lt;span style="color:#a6e22e">app&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Server started successfully, running on port &amp;#34;&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#e6db74">&amp;#34;.&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">app&lt;/span>.&lt;span style="color:#a6e22e">on&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;close&amp;#39;&lt;/span>, () =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">repository&lt;/span>.&lt;span style="color:#a6e22e">disconnect&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We have a little error handling and beyond that we&amp;rsquo;re just loading config, creating a repo and starting our server.&lt;/p>
&lt;p>That&amp;rsquo;s the microservice. It allows us to get all users, or search a user:&lt;/p>
&lt;pre tabindex="0">&lt;code>HTTP GET /users # gets all users
HTTP GET /search?email=homer@thesimpons.com # searches by email
&lt;/code>&lt;/pre>&lt;p>If you checkout the code, you&amp;rsquo;ll see that there&amp;rsquo;s a few commands available for you:&lt;/p>
&lt;pre tabindex="0">&lt;code>cd ./users-service
npm install # setup everything
npm test # unit test - no need for a test database running
npm start # run the server - you must have a test database running
npm run debug # run the server in debug mode, opens a browser with the inspector
npm run lint # check to see if the code is beautiful
&lt;/code>&lt;/pre>&lt;p>Asides from the code you&amp;rsquo;ve seen we have:&lt;/p>
&lt;ol>
&lt;li>Node Inspector for debugging&lt;/li>
&lt;li>Mocha/shoud/supertest for unit tests&lt;/li>
&lt;li>ESLint for linting&lt;/li>
&lt;/ol>
&lt;p>That&amp;rsquo;s it!&lt;/p>
&lt;p>Run the test database with:&lt;/p>
&lt;pre tabindex="0">&lt;code>cd test-database/
./start.sh
&lt;/code>&lt;/pre>&lt;p>Then the service with:&lt;/p>
&lt;pre tabindex="0">&lt;code>cd ../users-service/
npm start
&lt;/code>&lt;/pre>&lt;p>You can point your browser to &lt;a href="http://localhost:8123/users">localhost:8123/users&lt;/a> and see it in action. If you are using Docker Machine (i.e. you&amp;rsquo;re on Mac or Windows) then &lt;code>localhost&lt;/code> won&amp;rsquo;t work, you need the IP of the docker machine instead. You can use &lt;code>docker-machine ip&lt;/code> to get it.&lt;/p>
&lt;p>We&amp;rsquo;ve whipped through building the service quickly. If you&amp;rsquo;d like to see this code before we continue, check the &lt;a href="https://github.com/dwmkerr/node-docker-microservice/tree/step2">step2&lt;/a> branch.&lt;/p>
&lt;h1 id="step-3-dockerising-our-microservice">Step 3: Dockerising our Microservice&lt;/h1>
&lt;p>OK now it gets fun!&lt;/p>
&lt;p>So we have a microservice which we can run on a dev box, as long as it has a compatible version of Node.js installed. What we&amp;rsquo;d like to do is set up our service so that we can create a &lt;em>Docker Image&lt;/em> from it, allowing us to deploy our service anywhere which supports docker.&lt;/p>
&lt;p>The way we do this is create a &lt;em>Dockerfile&lt;/em>. A Dockerfile is a recipe that tells the Docker engine how to build your image. We&amp;rsquo;ll create a simple Dockerfile in our &lt;code>users-service&lt;/code> directory and start to explore how we can adapt it to our needs.&lt;/p>
&lt;h2 id="creating-the-dockerfile">Creating the Dockerfile&lt;/h2>
&lt;p>Create a new text file called &lt;code>Dockerfile&lt;/code> at &lt;code>users-service/&lt;/code> with the content below:&lt;/p>
&lt;pre tabindex="0">&lt;code># Use Node v4 as the base image.
FROM node:4
# Run node
CMD [&amp;#34;node&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>Now run the commands below to build the image and run the a container from it:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker build -t node4 . # Builds a new image
docker run -it node4 # Run a container with this image, interactive
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s look at the build command first.&lt;/p>
&lt;ol>
&lt;li>&lt;code>docker build&lt;/code> tell the engine we want to create a new image.&lt;/li>
&lt;li>&lt;code>-t node4&lt;/code> tag this image with the tag &lt;code>node4&lt;/code>. We can refer to this image by tag from now on.&lt;/li>
&lt;li>&lt;code>.&lt;/code> use the current directory to find the &lt;code>Dockerfile&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>After some console output you&amp;rsquo;ll see we have a new image created. You can see all images on your system with &lt;code>docker images&lt;/code>. The next command should be fairly familiar from what we&amp;rsquo;ve done so far:&lt;/p>
&lt;ol>
&lt;li>&lt;code>docker run&lt;/code> run a new container from an image.&lt;/li>
&lt;li>&lt;code>-it&lt;/code> use an interactive terminal.&lt;/li>
&lt;li>&lt;code>node4&lt;/code> the tag of the image we want to use in the container.&lt;/li>
&lt;/ol>
&lt;p>When we run this image, we get a node repl, check the current version like so:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">version&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;v4.4.0&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">exit&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is potentially different to the node version on your current machine.&lt;/p>
&lt;h2 id="examining-the-dockerfile">Examining the Dockerfile&lt;/h2>
&lt;p>Looking at the dockerfile we can see quite easily what is going on:&lt;/p>
&lt;ol>
&lt;li>&lt;code>FROM node:4&lt;/code> the first thing we specify in a Dockerfile is the base image. A quick google finds the &lt;a href="https://hub.docker.com/_/node/">node organisation page on the docker hub&lt;/a> showing all of the available images. This is essentially bare bones ubuntu with node installed.&lt;/li>
&lt;li>&lt;code>CMD [&amp;quot;node&amp;quot;]&lt;/code> the &lt;code>CMD&lt;/code> command tells docker that this image should run the node executable. When the executable terminates, the container shuts down.&lt;/li>
&lt;/ol>
&lt;p>With the addition of a few more commands, we can update our Dockerfile so that it runs our service:&lt;/p>
&lt;pre tabindex="0">&lt;code># Use Node v4 as the base image.
FROM node:4
# Add everything in the current directory to our image, in the &amp;#39;app&amp;#39; folder.
ADD . /app
# Install dependencies
RUN cd /app; \
npm install --production
# Expose our server port.
EXPOSE 8123
# Run our app.
CMD [&amp;#34;node&amp;#34;, &amp;#34;/app/index.js&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>The only addition here is that we use the &lt;code>ADD&lt;/code> command to copy everything&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> in the current directory to a folder in the container called &lt;code>app/&lt;/code> . We then use &lt;code>RUN&lt;/code> to run a command in the image, which installs our modules. Finally, we &lt;code>EXPOSE&lt;/code> the server port, telling docker we intend to support inbound connections on &lt;code>8123&lt;/code>, then run our server code.&lt;/p>
&lt;p>Ensure the test-database service is running, then build and run the image again:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker build -t users-service .
docker run -it -p 8123:8123 users-service
&lt;/code>&lt;/pre>&lt;p>If you navigate to &lt;code>localhost:8123/users&lt;/code> in a browser you should see an error, checking the console shows our container is reporting some issues:&lt;/p>
&lt;pre tabindex="0">&lt;code>--- Customer Service---
Connecting to customer repository...
Connected. Starting server...
Server started successfully, running on port 8123.
GET /users 500 23.958 ms - 582
Error: An error occured getting the users: Error: connect ECONNREFUSED 127.0.0.1:3306
at Query._callback (/app/repository/repository.js:21:25)
at Query.Sequence.end (/app/node_modules/mysql/lib/protocol/sequences/Sequence.js:96:24)
at /app/node_modules/mysql/lib/protocol/Protocol.js:399:18
at Array.forEach (native)
at /app/node_modules/mysql/lib/protocol/Protocol.js:398:13
at nextTickCallbackWith0Args (node.js:420:9)
at process._tickCallback (node.js:349:13)
&lt;/code>&lt;/pre>&lt;p>Yikes! So the connection from our &lt;code>users-service&lt;/code> container to the &lt;code>test-database&lt;/code> container is being refused. We might try running &lt;code>docker ps&lt;/code> to see all containers running:&lt;/p>
&lt;pre tabindex="0">&lt;code>CONTAINER ID IMAGE PORTS NAMES
a97958850c66 users-service 0.0.0.0:8123-&amp;gt;8123/tcp kickass_perlman
47f91343db01 mysql:latest 0.0.0.0:3306-&amp;gt;3306/tcp db
&lt;/code>&lt;/pre>&lt;p>They&amp;rsquo;re both there, so what is going on?&lt;/p>
&lt;h2 id="linking-containers">Linking Containers&lt;/h2>
&lt;p>The issue we&amp;rsquo;ve seen is actually to be expected. Docker containers are supposed to be isolated, so it wouldn&amp;rsquo;t make much sense if we could create connections between containers without us explicitly allowing it.&lt;/p>
&lt;p>Yes, we can connect from our machine (the host) to a container, because we&amp;rsquo;ve opened ports for that (using the &lt;code>-p 8123:8123&lt;/code> argument for example). If we allowed containers to talk to each other in the same way, then two containers running on the same machine would be able to communicate, even if the developers didn&amp;rsquo;t intend it, and that&amp;rsquo;s a recipe for disaster, especially when we might have a cluster of machines whos job it is to run containers from different applications.&lt;/p>
&lt;p>If we&amp;rsquo;re going to connect from one container to another, we need to &lt;em>link&lt;/em> them, which tells docker that we explicitly want to allow communication between the two. There are two ways of doing this, the first is the &amp;lsquo;old fasioned&amp;rsquo; but quite simple way, the second we&amp;rsquo;ll see a little later.&lt;/p>
&lt;h3 id="linking-containers-with-the-link-parameter">Linking Containers with the &amp;rsquo;link&amp;rsquo; parameter&lt;/h3>
&lt;p>When we run a container, we can tell docker that we intend to connect to another container using the &lt;code>link&lt;/code> parameter. In our case, we can run our service correctly like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -it -p 8123:8123 --link db:db -e DATABASE_HOST=DB users-service
&lt;/code>&lt;/pre>&lt;ol>
&lt;li>&lt;code>docker run -it&lt;/code> run a docker image in a container, with an interactive terminal.&lt;/li>
&lt;li>&lt;code>-p 8123:8123&lt;/code> map the host port 8123 to the container port 8123.&lt;/li>
&lt;li>&lt;code>link db:db&lt;/code> link to the container named &lt;code>db&lt;/code> and refer to it as &lt;code>db&lt;/code>.&lt;/li>
&lt;li>&lt;code>-e DATABASE_HOST=db&lt;/code> set the &lt;code>DATABASE_HOST&lt;/code> environment variable to &lt;code>db&lt;/code>.&lt;/li>
&lt;li>&lt;code>users-service&lt;/code> the name of the image to run in our container.&lt;/li>
&lt;/ol>
&lt;p>Now when we go to &lt;code>localhost:8123/users&lt;/code> everything works.&lt;/p>
&lt;h4 id="how-it-works">How it works&lt;/h4>
&lt;p>Remember our config file for the service? It allowed us to specify a database host with an environment variable:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// config.js
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Simple application configuration. Extend as needed.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">module&lt;/span>.&lt;span style="color:#a6e22e">exports&lt;/span> &lt;span style="color:#f92672">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">env&lt;/span>.&lt;span style="color:#a6e22e">PORT&lt;/span> &lt;span style="color:#f92672">||&lt;/span> &lt;span style="color:#ae81ff">8123&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">db&lt;/span>&lt;span style="color:#f92672">:&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">host&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#a6e22e">process&lt;/span>.&lt;span style="color:#a6e22e">env&lt;/span>.&lt;span style="color:#a6e22e">DATABASE_HOST&lt;/span> &lt;span style="color:#f92672">||&lt;/span> &lt;span style="color:#e6db74">&amp;#39;127.0.0.1&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">database&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;users&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">user&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;users_service&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">password&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;123&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">3306&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When we run the container, we set this environment variable to &lt;code>DB&lt;/code>, which means we&amp;rsquo;re connecting to a host called &lt;code>DB&lt;/code>. This is &lt;em>automatically&lt;/em> set up for us by the docker engine when we link to a container.&lt;/p>
&lt;p>To see this in action, try running &lt;code>docker ps&lt;/code> to list all running containers. Look up the name of the container running the &lt;code>users-service&lt;/code>, which will be a random name such as &lt;code>trusting_jang&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker ps
CONTAINER ID IMAGE ... NAMES
ac9449d3d552 users-service ... trusting_jang
47f91343db01 mysql:latest ... db
&lt;/code>&lt;/pre>&lt;p>Now we can look at the hosts available on our container:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker exec trusting_jang cat /etc/hosts
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.2 db 47f91343db01 # linking magic!!
172.17.0.3 ac9449d3d552
&lt;/code>&lt;/pre>&lt;p>Remember how &lt;code>docker exec&lt;/code> works? Choose a container name and then whatever follows is the command you&amp;rsquo;ll execute on the container, in our case &lt;code>cat /etc/hosts&lt;/code>.&lt;/p>
&lt;p>OK the hosts file doesn&amp;rsquo;t have the &lt;code># linking magic!!&lt;/code> comment, that&amp;rsquo;s so you can see - docker has added &lt;code>db&lt;/code> to our hosts file so we can refer to the linked container by hostname. This is one consequence of linking. Here&amp;rsquo;s the other:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker exec trusting_jang printenv | grep DB
DB_PORT=tcp://172.17.0.2:3306
DB_PORT_3306_TCP=tcp://172.17.0.2:3306
DB_PORT_3306_TCP_ADDR=172.17.0.2
DB_PORT_3306_TCP_PORT=3306
DB_PORT_3306_TCP_PROTO=tcp
DB_NAME=/trusting_jang/db
&lt;/code>&lt;/pre>&lt;p>From this command we can also see that when docker links a container, it also provides a set of environment variables with some helpful information. We know the host, tcp port and container name.&lt;/p>
&lt;p>That&amp;rsquo;s step 3 complete - we have a MySQL database running happily in a container, we have a node.js microservice which we can run locally or in a container of its own, and we know how to link them together.&lt;/p>
&lt;p>You can check out how the code looks at this stage by going to the &lt;a href="https://github.com/dwmkerr/node-docker-microservice/tree/step3">step3&lt;/a> branch.&lt;/p>
&lt;h1 id="step-4-integration-testing-the-environment">Step 4: Integration Testing the Environment&lt;/h1>
&lt;p>We can now write an integration test which calls the actual server, running as a docker container, calling the containerised test database.&lt;/p>
&lt;p>Writing the integration test can be done in whatever language or on whatever platform you want, within reason, but to keep things simple I&amp;rsquo;m using Node.js as we&amp;rsquo;ve already seen Mocha and Supertest in our project.&lt;/p>
&lt;p>In a new folder, called &lt;code>integration-tests&lt;/code> we&amp;rsquo;ve got a single &lt;code>index.js&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">supertest&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;supertest&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">should&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">require&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;should&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">describe&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;users-service&amp;#39;&lt;/span>, () =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">api&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">supertest&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;http://localhost:8123&amp;#39;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">it&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;returns a 200 for a known user&amp;#39;&lt;/span>, (&lt;span style="color:#a6e22e">done&lt;/span>) =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">api&lt;/span>.&lt;span style="color:#a6e22e">get&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;/search?email=homer@thesimpsons.com&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> .&lt;span style="color:#a6e22e">expect&lt;/span>(&lt;span style="color:#ae81ff">200&lt;/span>, &lt;span style="color:#a6e22e">done&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> });
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>});
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will check an API call and show the results of the test&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>As long as your &lt;code>users-service&lt;/code> and &lt;code>test-database&lt;/code> are running, the tests will pass. However, at this stage the services are getting a little harder to handle:&lt;/p>
&lt;ol>
&lt;li>We have to use a shell script to start and stop the database&lt;/li>
&lt;li>We have to remember a sequence of commands to start the users service against the database&lt;/li>
&lt;li>We have to use node directly to run the integration tests&lt;/li>
&lt;/ol>
&lt;p>Now that we&amp;rsquo;re a little more familiar with Docker we can fix these issues.&lt;/p>
&lt;h3 id="simplifiying-the-test-database">Simplifiying the Test Database&lt;/h3>
&lt;p>Currently we have the following files for the test database:&lt;/p>
&lt;pre tabindex="0">&lt;code>/test-database/start.sh
/test-database/stop.sh
/test-database/setup.sql
&lt;/code>&lt;/pre>&lt;p>Now that we&amp;rsquo;re more familar with Docker, we can improve on this. Looking into the &lt;a href="https://hub.docker.com/_/mysql/">mysql image documentation&lt;/a> on Docker Hub there&amp;rsquo;s a note which tells us any &lt;code>.sql&lt;/code> or &lt;code>.sh&lt;/code> file added to the image&amp;rsquo;s &lt;code>/docker-entrypoint-initdb.d&lt;/code> folder will be executed when setting up the DB.&lt;/p>
&lt;p>This means we can replace our &lt;code>start.sh&lt;/code> and &lt;code>stop.sh&lt;/code> scripts with a &lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>FROM mysql:5
ENV MYSQL_ROOT_PASSWORD 123
ENV MYSQL_DATABASE users
ENV MYSQL_USER users_service
ENV MYSQL_PASSWORD 123
ADD setup.sql /docker-entrypoint-initdb.d
&lt;/code>&lt;/pre>&lt;p>Now to run our test database it is just:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker build -t test-database .
docker run --name db test-database
&lt;/code>&lt;/pre>&lt;h3 id="composing">Composing&lt;/h3>
&lt;p>Building and running each container is still somewhat time consuming. We can take things a step further with the &lt;a href="https://docs.docker.com/compose/">Docker Compose&lt;/a> tool.&lt;/p>
&lt;p>Docker Compose lets you create a file which defines each container in your system, the relationships between them, and build or run them all.&lt;/p>
&lt;p>First, &lt;a href="https://docs.docker.com/compose/install/">install Docker Compose&lt;/a>. Now create a new file in the root of your project called &lt;code>docker-compose.yml&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: &amp;#39;2&amp;#39;
services:
users-service:
build: ./users-service
ports:
- &amp;#34;8123:8123&amp;#34;
depends_on:
- db
environment:
- DATABASE_HOST=db
db:
build: ./test-database
&lt;/code>&lt;/pre>&lt;p>Now check this out:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose build
docker-compose up
&lt;/code>&lt;/pre>&lt;p>Docker Compose has built all of the images needed for our application, created containers fromthem, run them in the correct order and started the whole stack!&lt;/p>
&lt;p>The &lt;code>docker-compose build&lt;/code> command builds each image which is listed in the &lt;code>docker-compose.yml&lt;/code> file:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: &amp;#39;2&amp;#39;
services:
users-service:
build: ./users-service
ports:
- &amp;#34;8123:8123&amp;#34;
depends_on:
- db
environment:
- DATABASE_HOST=db
db:
build: ./test-database
&lt;/code>&lt;/pre>&lt;p>The &lt;code>build&lt;/code> value for each of our services tells docker where to go to find the &lt;code>Dockerfile&lt;/code>. When we run &lt;code>docker-compose up&lt;/code>, docker starts all of our services. Notice from the &lt;code>Dockerfile&lt;/code> we can specify ports and dependencies. Actually, there&amp;rsquo;s a whole bunch of config we can change here.&lt;/p>
&lt;p>In another terminal, run &lt;code>docker compose down&lt;/code> to gracefully shut down the containers.&lt;/p>
&lt;h1 id="winding-up">Winding Up&lt;/h1>
&lt;p>We&amp;rsquo;ve seen a lot of docker in this article, but there&amp;rsquo;s a lot more to it. I hope this has shown some of the interesting and useful things that you can use docker for in your workflow.&lt;/p>
&lt;p>As usual, questions and comments are welcomed! I&amp;rsquo;d also strongly recommend the document &lt;a href="https://docs.docker.com/engine/understanding-docker/">Understanding Docker&lt;/a> to get a deeper understanding of how docker works.&lt;/p>
&lt;p>You can see the final source code for the project built in this article at &lt;a href="https://github.com/dwmkerr/node-docker-microservice">github.com/dwmkerr/node-docker-microservice&lt;/a>&lt;/p>
&lt;h1 id="notes">Notes&lt;/h1>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Copying everything is actually a bad idea, because we will also copy the node_modules folder. Generally it is a better idea explicitly list the files or folders you want to copy, or use a .dockerignore file, which works just like the .gitignore file.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>If the server isn&amp;rsquo;t running, it will actually show a rather annoying exception, due to a bug in supertest, see &lt;a href="https://github.com/visionmedia/supertest/issues/314">github.com/visionmedia/supertest/issues/314&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description><category>CodeProject</category></item></channel></rss>